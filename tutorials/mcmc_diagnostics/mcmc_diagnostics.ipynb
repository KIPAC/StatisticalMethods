{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdT2fcw6CT91"
   },
   "source": [
    "# Tutorial: MCMC Diagnostics\n",
    "\n",
    "For this notebook, you should already have generated and saved Markov chains in the Cluster Redshift Distribution/MCMC Sampling tutorial using two algorithms, conjugate Gibbs and Metropolis. Here, we'll work through the process of diagnosing whether these Markov chains are usefully sampling the posterior distribution, in particular assessing:\n",
    "\n",
    "* their convergence\n",
    "* their autocorrelation\n",
    "* the effective number of independent samples, and how this impacts the usual reported values and credible intervals\n",
    "\n",
    "The diagnostics discussed below include both qualitative and quantitative checks. We don't particularly think it's all that instructive to write the code that does the quantitative calculations - though there is surely room for improvement or expansion if you're interested - so intead we will demonstrate how to use functions provided by the `incredible` and `pandas` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVMdyMVTCT94"
   },
   "outputs": [],
   "source": [
    "# !pip install incredible\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import exists as file_exists\n",
    "from yaml import safe_load\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Xlm0Pjc-CT96",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9500f057c33f39a533f24f18d20e251a",
     "grade": true,
     "grade_id": "datapath",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thisTutorial = 'clredshift' # NB we are reading data from a previous tutorial!\n",
    "if getcwd() == '/content':\n",
    "    # assume we are in Colab, and the user's data directory is linked to their drive/Physics267_data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    datapath = '/content/drive/MyDrive/Physics267_data/' + thisTutorial + '/'\n",
    "else:\n",
    "    # assume we are running locally somewhere and have the data under ./data/\n",
    "    datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3cZ8YRBCT97"
   },
   "source": [
    "## Gibbs samples\n",
    "\n",
    "It's nice to start with the Gibbs sampled chains, since (for this problem) they almost certainly look nicer. First, read them in. The way we did things in the previous tutorial, each entry in the `chains` list will be an $N_\\mathrm{steps} \\times N_\\mathrm{parameters}$ array. The cell below assumes they are present in the location where the Redshift Distribution tutorial writes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHH-5f84CT98"
   },
   "outputs": [],
   "source": [
    "chains = [np.loadtxt(f) for f in glob(datapath+'clredshift_gibbs_*.txt.gz')]\n",
    "\n",
    "param_labels = [r'$z_\\mathrm{cl}$', r'$\\sigma^2$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-6FXQUqCT99"
   },
   "source": [
    "### Visual inspection\n",
    "\n",
    "You've already used the most important method of vetting chains: visual inspection. The key questions are:\n",
    "1. Do multiple, independent chains appear to be sampling the same distribution (have they _converged_ to the same distribution)?\n",
    "2. Is there a clear \"burn-in\" period before convergence that should be eliminated?\n",
    "3. Are the chains highly autocorrelated (taking small steps compared with the width of the posterior)? This is not an issue per se, if the chains are long enough, although it means the sampler is not moving as efficiently as one might hope.\n",
    "\n",
    "Plot the parameter traces below, and answer these questions (qualitatively) for the conjugate Gibbs sampling chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGmfJmMCCT9-"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "cr.plot_traces(chains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7arvIQyXCT9_"
   },
   "source": [
    "> Space to answer the 3 key questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "uIk7yMd_CT-A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d9167eb2929962e117beb81a19ffce6",
     "grade": false,
     "grade_id": "checkpoint1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_have_answered = False # change to True when true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2ovaxWMNCT-A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7083af3ed7fcfde5e6c95045633c8634",
     "grade": true,
     "grade_id": "check1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert I_have_answered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Y6tbDSCT-B"
   },
   "source": [
    "Is there a burn-in that should clearly be removed? If so, do so here by setting the value of `burn` to a number of steps $>0$. (You shouldn't need to remove very much - if `burn=100`, for e.g., doesn't seem incredibly generous in this case, then something went wrong in the previous tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "13hyCrWUCT-B",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c03b2dfc8fae59046a16944d88268250",
     "grade": false,
     "grade_id": "gibbs_burn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# burn =\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0B2L11hCT-C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(chains)):\n",
    "    chains[i] = chains[i][burn:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVESviUYCT-D"
   },
   "source": [
    "### Gelman-Rubin statistic\n",
    "\n",
    "Recall from the MCMC Diagnostics notes that the Gelman-Rubin convergence statistic, $R$, quantitatively tests the similarlity of independent chains intended to sample the same PDF. To be meaningful, they should start from different locations and burn-in should be removed.\n",
    "\n",
    "For a given parameter, $\\theta$, the $R$ statistic compares the variance across chains with the variance within a chain. Intuitively, if the chains are random-walking in very different places, i.e. not sampling the same distribution, $R$ will be large.\n",
    "\n",
    "We'd like to see $R\\approx 1$; for example, $R<1.1$ is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D9O9ACvCT-D"
   },
   "outputs": [],
   "source": [
    "cr.GelmanRubinR(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xdL2fS1CT-E"
   },
   "source": [
    "If your Gibbs sampler works properly, $R$ for the chains we ran should be _very_ close to 1 (we have differences of order $10^{-5}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sujSl1tQCT-E",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c49f4f9b8f428f1d67c2c865e9910416",
     "grade": false,
     "grade_id": "gibbs_R",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(cr.GelmanRubinR(chains), [1.0]*len(param_labels), atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI26bplDCT-F"
   },
   "source": [
    "### Autocorrelation\n",
    "\n",
    "Similarly, the autocorrelation of a sequence, as a function of lag, $k$, can be quantified:\n",
    "\n",
    "$\\rho_k = \\frac{\\mathrm{Cov}_i\\left(\\theta_i,\\theta_{i+k}\\right)}{\\mathrm{Var}(\\theta)}$\n",
    "\n",
    "The larger lag one needs to get a small autocorrelation, the less informative individual samples are.\n",
    "\n",
    "The `pandas` function `plotting.autocorrelation_plot()` is useful for this. Note that seemingly random oscillations basically tell us the level of noise due to the finite chain length. A coherent drop as a function of lag indicates a genuine autocorrelation, and the lag at which it drops to within the noise (roughly indicated by the horizontal, gray lines) is an approximate autocorrelation length. If we needed to thin the chains to conserve disk space, this would be a reasonable factor to thin by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7cOmXlACT-F"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "for j,lab in enumerate(param_labels):\n",
    "    pd.plotting.autocorrelation_plot(chains[0][:,j], ax=ax[j]);\n",
    "    ax[j].set_ylabel(lab+' autocorrelation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgCLf3ElCT-G"
   },
   "source": [
    "Again, for this problem, the Gibbs chains should be very well behaved. Our autocorrelation plots look like noise at almost all lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Y-ZhsHcjCT-G",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1de2d22100e8b792ee34589c6e55a8b5",
     "grade": false,
     "grade_id": "checkpoint2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Yup_that_checks_out = False # change to True if, yup, that checks out\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DOoBJxv8CT-H",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8230405ca2c2f1c6c642c4c21d9b0b30",
     "grade": true,
     "grade_id": "check2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert Yup_that_checks_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO0C-IA9CT-H"
   },
   "source": [
    "### Effective number of independent samples\n",
    "\n",
    "From $m$ chains of length $n$, we can also estimate the \"effective number of independent samples\" as\n",
    "\n",
    "$n_\\mathrm{eff} = \\frac{mn}{1+2\\sum_{t=1}^\\infty \\hat{\\rho}_t}$, with\n",
    "\n",
    "$\\hat{\\rho}_t = 1 - \\frac{V_t}{2V}$ ($V$ as in the Gelman-Rubin calculation), and\n",
    "\n",
    "$V_t = \\frac{1}{m(n-t)} \\sum_{j=0}^m \\sum_{i=t+1}^n (\\theta_{i,j} - \\theta_{i-t,j})^2$.\n",
    "\n",
    "In practice, the sum in $n_\\mathrm{eff}$ is cut off when the estimates $\\hat{\\rho}_t$ become \"too noisy\", e.g. when the sum of two successive values $\\hat{\\rho}_t$ and $\\hat{\\rho}_{t+1}$ is negative. Roughly speaking, this should occur when the lag is of the order of the autocorrelation length.\n",
    "\n",
    "The `effective_samples` below function allows you to pass a guess at this maximum lag, since doing the calculation to arbitrarily long lags becomes very expensive. It will issue a warning if it thinks this maximum lag is too small, according to the criterion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuBfPoUNCT-I"
   },
   "outputs": [],
   "source": [
    "maxlag = 500\n",
    "cr.effective_samples(chains, maxlag=maxlag) # `maxlag' might be something you need to play with, in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygP8mzL-CT-I"
   },
   "source": [
    "Since we started with 4 chains of length 10,000 (possibly minus a little burn-in), $n_\\mathrm{eff}$ values close to 40,000 are telling us that the autocorrelation is indeed quite small, in agreement with the plots above. Let's remember that this need not be true for every problem; Gibbs sampling is not independence sampling and produces a Markov chain that in principle could be highly correlated, depending on the model and data involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gKGbu2vbCT-J",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "754087e6aa21631481d41f505d451e40",
     "grade": false,
     "grade_id": "gibbs_neff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.all(cr.effective_samples(chains, maxlag=maxlag) > 30e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DB4HeMgCT-J"
   },
   "source": [
    "Note that, as with the Gelman-Rubin statistic, this is a case where one might be interested in seeing the effective number of samples for the most degenerate linear combinations of parameters, rather than the parameters themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzHEvbPRCT-K"
   },
   "source": [
    "### Something to do\n",
    "\n",
    "By now you are probably bored. Don't worry. Here is some work for you to do.\n",
    "\n",
    "Let's get a sense of how many samples are _really_ needed to, e.g., determine 1D credible intervals (as opposed to making the whole posterior look nice). Remember that the effective number of samples is less than the total.\n",
    "\n",
    "At this point, we're done comparing the individual chains, so we can lump them all together into one massive list of MCMC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYC4aEbgCT-K"
   },
   "outputs": [],
   "source": [
    "chain = np.concatenate(chains, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChJTkY_RCT-K"
   },
   "source": [
    "For reference the total number of samples is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ywyo4kNCT-L"
   },
   "outputs": [],
   "source": [
    "chain.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAFITcQ_CT-L"
   },
   "source": [
    "We'll name the results using this full chain `40k` in the expectation that the length is still close to 40,000 steps, even after removing burn-in.\n",
    "\n",
    "Let's have a look at the credible interval calculation for $z_{\\mathrm{cl}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92GFqTq3CT-N"
   },
   "outputs": [],
   "source": [
    "print(chain.shape[0], 'samples')\n",
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "h40k = cr.whist(chain[:,0], plot=ax[0])\n",
    "ci40k = cr.whist_ci(h40k, plot=ax[1]);\n",
    "ax[0].set_xlabel(param_labels[0]);\n",
    "ax[1].set_xlabel(param_labels[0]);\n",
    "ci40k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iehujoJCT-O"
   },
   "source": [
    "The PDF estimate should look pretty reliable with so many samples. The question is, if we're going to reduce this to a statement like $z_{\\mathrm{cl}}=X^{+Y}_{-Z}$, keeping only up to the leading significant figure of $Y$ and $Z$, how many did we actually need to keep?\n",
    "\n",
    "Thin the chain by factors of 4, 40, and 400 (to produce chains of length about 10000, 1000 and 100), and see how the _endpoints_ of the 68.3% credible intervals compare. We're looking at the endpoints rather than the values of $Y$ and $Z$ above because the latter are more volatile (depending also on the estimate of $X$).\n",
    "\n",
    "Remember that thinning by a factor of 4 means that we keep only every 4th entry in the chain, **not** that we simply select the first 25% of samples. So we're not answering how long we needed to bother _running_ the chain to begin with - that's a slightly different question. We're finding out how redundant our samples are, not just in the \"effective independence\" sense, but for the specific purpose of quantifying this credible interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "NAPKxGX_CT-O",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f41605182c603292ef20b473e84eca19",
     "grade": false,
     "grade_id": "thin_gibbs",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No clues here, but it's pretty much cut and paste.\n",
    "# Analogous to the cell above, save the output of `whist` in h10k, h1k, h100, and the output of\n",
    "# whist_ci in ci10k, ci1k and ci100. This is so we can plot them all togther later.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjTslAXGCT-P"
   },
   "outputs": [],
   "source": [
    "print('40k:', ci40k['min'][0], ci40k['max'][0])\n",
    "print('10k:', ci10k['min'][0], ci10k['max'][0])\n",
    "print(' 1k:', ci1k['min'][0], ci1k['max'][0])\n",
    "print('100:', ci100['min'][0], ci100['max'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UptcQa_CT-P"
   },
   "source": [
    "Your mileage may vary slightly, of course, but we see a differences of order $10^{-4}$, several times smaller than the width of the interval.\n",
    "\n",
    "... which is a little surprising, honestly, even though we knew the autocorrelation was quite low in this case. But here's a slightly different question: which of the possible results would you be confident enough to put in a paper, having looked at the marginalized PDFs? (The cell below compares them on the same plot, along with the 1 and 2$\\sigma$ CIs.) And, relatedly, which one would you actually want to show in a figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZ0-T17_CT-S"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14.0, 5.0)\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "ax[0].plot(h40k['x'], h40k['density'], '-', label='40k');\n",
    "ax[0].plot(h10k['x'], h10k['density'], '-', label='10k');\n",
    "ax[0].plot(h1k['x'], h1k['density'], '-', label='1k');\n",
    "ax[0].plot(h100['x'], h100['density'], '-', label='100');\n",
    "ax[0].legend();\n",
    "ax[0].set_xlabel(param_labels[0]);\n",
    "ax[1].plot(0.0, ci40k['mode'], 'o', color='C0', label='40k');\n",
    "ax[1].plot([0.0]*2, [ci40k['min'][0],ci40k['max'][0]], '-', color='C0', linewidth=3);\n",
    "ax[1].plot([0.0]*2, [ci40k['min'][1],ci40k['max'][1]], '--', color='C0');\n",
    "ax[1].plot(1.0, ci10k['mode'], 'o', color='C1', label='10k');\n",
    "ax[1].plot([1.0]*2, [ci10k['min'][0],ci10k['max'][0]], '-', color='C1', linewidth=3);\n",
    "ax[1].plot([1.0]*2, [ci10k['min'][1],ci10k['max'][1]], '--', color='C1');\n",
    "ax[1].plot(2.0, ci1k['mode'], 'o', color='C2', label='1k');\n",
    "ax[1].plot([2.0]*2, [ci1k['min'][0],ci1k['max'][0]], '-', color='C2', linewidth=3);\n",
    "ax[1].plot([2.0]*2, [ci1k['min'][1],ci1k['max'][1]], '--', color='C2');\n",
    "ax[1].plot(3.0, ci100['mode'], 'o', color='C3', label='100');\n",
    "ax[1].plot([3.0]*2, [ci100['min'][0],ci100['max'][0]], '-', color='C3', linewidth=3);\n",
    "ax[1].plot([3.0]*2, [ci100['min'][1],ci100['max'][1]], '--', color='C3');\n",
    "ax[1].legend();\n",
    "ax[1].set_ylabel(param_labels[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaSJTAbvCT-S"
   },
   "source": [
    "Just to belabor the point, here are the `mode` and `-low +high` CIs for the 4 cases, at the precision we would actually report (NB because everyone's data set is somewhat random, you may need to change the number of digits to round to to be appropriate). In our solutions, the only non-trivial difference is in the 100-sample case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bb2LMQ2CT-T"
   },
   "outputs": [],
   "source": [
    "roundto = 4\n",
    "print('~40k samples:', (np.round(ci40k['mode'], roundto)), (np.round(ci40k['low'][0], roundto)), '+'+str((np.round(ci40k['high'][0], roundto))))\n",
    "print('~10k samples:', (np.round(ci10k['mode'], roundto)), (np.round(ci10k['low'][0], roundto)), '+'+str((np.round(ci10k['high'][0], roundto))))\n",
    "print(' ~1k samples:', (np.round(ci1k['mode'], roundto)), (np.round(ci1k['low'][0], roundto)), '+'+str((np.round(ci1k['high'][0], roundto))))\n",
    "print('~100 samples:', (np.round(ci100['mode'], roundto)), (np.round(ci100['low'][0], roundto)), '+'+str((np.round(ci100['high'][0], roundto))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMuSinCvCT-U"
   },
   "source": [
    "The bottom line is that summarizing a posterior with 1D best values and CIs is not, in fact, all that demanding in terms of the number of samples... provided that those samples are close to independent. Making a visual of the PDF in 1D or 2D that looks like it isn't noise dominated is often significantly more demanding. The upshot is that, if we have enough samples for the posterior plots to look not-noise-dominated, we can usually be pretty confident about numbers we report from them. Kepp in mind also that we used the \"easier\" of the two parameters in this problem for the example; typically the posterior for $\\sigma^2$ is less symmetric, and the samples more correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW8Du5KNCT-U"
   },
   "source": [
    "## Metropolis samples\n",
    "\n",
    "Now, read in the Metropolis chains and perform the same checks. Again, use the chains in `solutions/` if you didn't work the Metropolis notebook yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzer93jYCT-V"
   },
   "outputs": [],
   "source": [
    "chains = [np.loadtxt(f) for f in glob(datapath+'clredshift_metro_*.txt.gz')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov8xVGMbCT-V"
   },
   "source": [
    "Below we plot the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq80OqRFCT-W"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "cr.plot_traces(chains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3G26wa8CT-W"
   },
   "source": [
    "On the basis of the traces above, choose a burn-in length to remove from the beginning of each chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "rNysdwrhCT-X",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "435c77af50a6ff240b52f2bf7e071163",
     "grade": false,
     "grade_id": "burn_metro",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mburn =\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1DsbhLZCT-X"
   },
   "outputs": [],
   "source": [
    "for i in range(len(chains)):\n",
    "    chains[i] = chains[i][mburn:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD9UrRJUCT-Y"
   },
   "source": [
    "Depending on how the burn-in period looked, the remaining traces might be clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An3V4Hp0CT-Y"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "cr.plot_traces(chains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDSBDnwECT-a"
   },
   "source": [
    "Now that you hopefully have had a good look at the these chains, answer the same key questions from the first \"Visual Inspection\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JcQmgJYCT-b"
   },
   "source": [
    "> Space to answer the 3 key questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IELqbZJCT-b"
   },
   "source": [
    "Compare the chains from two methods in these terms (again, 3 parts to this answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yacIZdhOCT-c"
   },
   "source": [
    "> Space to answer the 3 key questions, comparing the MCMC algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "XMrjoBnxCT-c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a37a42a5d02fc7e87ce3144a155636c5",
     "grade": false,
     "grade_id": "checkpoint3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_have_answered_everything = False # change True when it is true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D08oT2YQCT-d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a71e988888aef630aa1e386df6a6a415",
     "grade": true,
     "grade_id": "check3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert I_have_answered_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLV5GkN8CT-d"
   },
   "source": [
    "Here we compute the G-R criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvTb_huYCT-e"
   },
   "outputs": [],
   "source": [
    "cr.GelmanRubinR(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIExFGqtCT-e"
   },
   "source": [
    "Your results will depend on the proposal distribution you used, but in this problem it's possible to get $R$ values very close to 1 again, though possibly a bit larger than in the Gibbs case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nNeNGn53CT-f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bc6e4cafc368de74f9a6e7d02d76784",
     "grade": false,
     "grade_id": "metro_R",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(cr.GelmanRubinR(chains), [1.0]*len(param_labels), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Tdg2SyCT-f"
   },
   "source": [
    "Next, we'll look at the autocorrelation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEfpl9bOCT-i"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "for j,lab in enumerate(param_labels):\n",
    "    pd.plotting.autocorrelation_plot(chains[0][:,j], ax=ax[j]);\n",
    "    ax[j].set_ylabel(lab+' autocorrelation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj9VElDICT-i"
   },
   "source": [
    "This will likely look qualitatively different from the Gibbs case, in that there is a clear sign of correlation at small lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyetsdHpCT-j"
   },
   "source": [
    "Next, the effective number of samples, increasing `maxlag` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1VNXIbHCT-l"
   },
   "outputs": [],
   "source": [
    "maxlag = 500\n",
    "cr.effective_samples(chains, maxlag=maxlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgkE-Y48CT-n"
   },
   "source": [
    "This will likely be **much** smaller than it was for the Gibbs chains, due to higher autocorrelation, but we should still aim to have $n_\\mathrm{eff}>1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sYKehlwPCT-o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c8c04979c80101e9d163fc72381b83b",
     "grade": false,
     "grade_id": "cell-bb33b7f6d02800e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.all(cr.effective_samples(chains, maxlag=maxlag) > 1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws0G95mBCT-r"
   },
   "source": [
    "That being the case, we won't bother repeating the thinning exercise above. However, it's worth comparing the marginalized constraints we infer from this method to the other. Here they are for $z_\\mathrm{cl}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nw2qclElCT-s"
   },
   "outputs": [],
   "source": [
    "chain = np.concatenate(chains, axis=0)\n",
    "print('Total samples:', chain.shape[0])\n",
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "hmetro = cr.whist(chain[:,0], plot=ax[0])\n",
    "cimetro = cr.whist_ci(hmetro, plot=ax[1]);\n",
    "ax[0].set_xlabel(param_labels[0]);\n",
    "ax[1].set_xlabel(param_labels[0]);\n",
    "cimetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7mY0zPhCT-s"
   },
   "source": [
    "You can/should compare them in detail (for both parameters). For our solutions, the 68.3% CI for $z_\\mathrm{cl}$ is essentially identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2REjWE1dCT-t"
   },
   "outputs": [],
   "source": [
    "print('Gibbs:', round(ci40k['min'][0], roundto), round(ci40k['max'][0], roundto))\n",
    "print('Metro:', round(cimetro['min'][0], roundto), round(cimetro['max'][0], roundto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh7wVbG3CT-u"
   },
   "source": [
    "In other words, all that extra correlation doesn't really matter, as long as we have enough effectively independent samples at the end of the day. Provided, of course, that the extra time spent producing them is not a barrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xk2HE2nCT-u"
   },
   "source": [
    "## Parting thoughts\n",
    "\n",
    "Hopefully this notebook has given you some practice with evaluating MCMC results and deciding how trustworthy they are. We do want to emphasize again that any conclusions about the relative performance of the two samplers involved are not generalizable. Conjugate Gibbs is often, but not always, a very nice option when the model permits it. On the other hand, we're not seeing Metropolis at its best; it can be enhanced in ways that improve its performance beyond what we've seen here (we'll cover some in the More Samplers notes). Combined with the fact that it can be used with any likelihood and prior, Metropolis sampling is an essential tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PA3nJntCT-u"
   },
   "source": [
    "### Parting technical note\n",
    "\n",
    "Should you be inclined to use `incredible` beyond provided code in this course (any why wouldn't you?) note that all of its functions that take multiple chains as arguments require them to be in one of the following formats:\n",
    "* a **list** of `Nsamples` $\\times$ `Nparams` **arrays**, as in this notebook\n",
    "* a 3D `Nchains` $\\times$ `Nsamples` $\\times$ `Nparams` **array**\n",
    "\n",
    "In particular, passing a 3D array with some other axis order will probably not crash anything, but will produce nonsense."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
