{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Generative Models\n",
    "\n",
    "This exercise is mostly to practice going from a real-world(ish) problem described in words to an actionable model. By model, we mean\n",
    "1. a list of quantities comprising your data and parameters from which predicted data can be produced;\n",
    "2. a PGM representing the conditional dependences of the parameters and data;\n",
    "3. a list of expressions containing the same information as the PGM, with the added specification of what probability distributions are involved.\n",
    "\n",
    "\"Expressions\" in this context are of the form you saw in the reading, and need not be fully spelled-out equations, for example:\n",
    "* $a \\Leftarrow b,c$ \n",
    "* $x \\sim \\mathrm{Normal}(\\mu, \\sigma)$\n",
    "* $\\mu \\sim \\mathrm{some~prior}$\n",
    "\n",
    "Translates to\n",
    "* $a$ is a deterministic function of $b$ and $c$\n",
    "* $x$ is normally distributed with mean $\\mu$ and standard deviation $\\sigma$\n",
    "* $\\mu$ is distributed according to some prior that I would need to specify in practice, but don't necessarily have to bother with for this exercise. Here \"prior\" implies no dependence on other listed parameters.\n",
    "\n",
    "_Every_ parameter and datum in the model must have such a rule for how it depends on other quantities (or priors). The result is a recipe for generating mock data, and also contains all the information needed to do inference given real data that we've collected.\n",
    "\n",
    "There is no set rule saying that it's better to draw the PGM first and write the expressions second, or vice versa; different people find each approach more or less natural.\n",
    "\n",
    "To turn in a PGM, you could for e.g.\n",
    "* draw on paper and include a photo in the notebook\n",
    "* do the digital equivalent with a tablet and stylus\n",
    "* use some other simple drawing tool and include that graphic in the notebook\n",
    "* use the `daft` package to produce a PGM graphic directly in Python\n",
    "\n",
    "Personally, I find the current version of `daft` extremely ugly and have taken to using Google Drawings or old-fashioned scribbling, but whatever works.\n",
    "\n",
    "Finally, note that **some of the situations described below are intentionally ambiguous**. Expect to have to make some assumptions in order to fully specify the model, and note what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Relaxed cluster fraction\n",
    "\n",
    "X-ray imaging data for 361 galaxy clusters were analyzed, and 57 of them were found to be morphologically \"relaxed\" according to some metric. We want to constrain the fraction of relaxed clusters in the Universe (the probability that a randomly chosen cluster is relaxed), $f$, assuming that the data set is representative.\n",
    "\n",
    "### 1a. Model specification\n",
    "\n",
    "Enumerate the model parameters, draw a PGM and write down the corresponding probability expressions for this problem. Be explicit about the form of the sampling distribution (see [Essential Probability](../notes/essential_probability.ipynb)), and arbitrarily choose some prior distribution for $f$ for the second part, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Generate data\n",
    "\n",
    "Go through the process of generating mock data from the model. Produce a visualization that compares an ensemble of mock data sets (say 1000) for\n",
    "1. model parameters fixed at some fiducial value(s)\n",
    "2. model parameters varying according to the PGM/expressions you write down above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression\n",
    "\n",
    "Your data is a list of $\\{x_k,y_k,\\sigma_k\\}$ triplets, where $\\sigma_k$ is some estimate of the \"error\" on $y_k$. You think a linear model, $y(x)=a+bx$, might explain these data.\n",
    "\n",
    "In the absence of any better information, assume that $\\vec{x}$ and $\\vec{\\sigma}$ are (somehow) known precisely, and that the \"error\" on $y_k$ is Gaussian (mean of $a+bx_k$ and standard deviation $\\sigma_k$).\n",
    "\n",
    "Enumerate the model parameters, draw a PGM and write down the corresponding probability expressions. Optionally, generate and visualize mock data for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Exoplanet transit photometry\n",
    "\n",
    "You've taken several images of a particular field, in order to record the transit of an exoplanet in front of a star (resulting in a temporary decrease in its brightness). Some kind of model, parametrized by $\\theta$, describes the time series of the resulting flux. Before we get to measure a number of counts, however, each image is affected by time-specific variables, e.g. related to changing weather. To account for these, you've also measured 10 other stars in the same field in every exposure. The assumption is that the average intrinsic flux of these stars should be constant in time, so that they can be used to correct for photometric variations, putting the multiple measurements of the target star on the same scale.\n",
    "\n",
    "Enumerate the model parameters, draw a PGM and write down the corresponding probability expressions. Optionally, generate and visualize mock data for this problem.\n",
    "\n",
    "Thanks to Anja von der Linden for inspiring (and then correcting) the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Galaxy cluster center offsets\n",
    "\n",
    "You've measured the centers of a sample of galaxy clusters in two ways: by choosing a brightest cluster galaxy (BCG) and by finding the centroid of each cluster's X-ray emission. The difference between the two should say something about the fidelity of the BCG selection method, among other things. The BCG positions are determined essentially perfectly, but the X-ray centroids come with a Gaussian statistical uncertainty of typically $\\sim30$ kpc (standard deviation) in both the $x$ and $y$ directions.\n",
    "\n",
    "The underlying model is assumed to be that the BCG and true X-ray centroid coincide perfectly in a fraction $f$ of clusters. In the remaining clusters, the true X-ray centroid and BCG are displaced according to a 2D Gaussian whose width in either direction is $\\sigma$.\n",
    "\n",
    "Enumerate the model parameters, draw a PGM and write down the corresponding probability expressions. Optionally, generate and visualize mock data for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
