{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder!\n",
    "\n",
    "After pulling down the tutorial notebook, immediately make a copy. Then do not modify the original. Do your work in the copy. This will prevent the possibility of git conflicts should the version-controlled file change at any point in the future. (The same exhortation applies to homeworks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Tutorial\n",
    "\n",
    "## Fitting a cluster model to XMM image data\n",
    "\n",
    "Alas, we return (to the X-ray imaging data from Week 2)! Refer to that tutorial for a basic introduction to the nature of the data products.\n",
    "\n",
    "In case you don't still have the data sitting around, you can download them again from here:\n",
    "* https://heasarc.gsfc.nasa.gov/FTP/xmm/data/rev0/0098010101/PPS/P0098010101M2U009IMAGE_3000.FTZ\n",
    "* https://heasarc.gsfc.nasa.gov/FTP/xmm/data/rev0/0098010101/PPS/P0098010101M2U009EXPMAP3000.FTZ\n",
    "\n",
    "(Remember that these are not to be committed to the repository.)\n",
    "\n",
    "Previously, we ignored the big, honkin' cluster of galaxies in the center of the field, and concentrated on building a generative model for the point-like sources. This time, we'll simply mask out those sources, and instead try to fit a model to the extended emission from the cluster (Abell 1835).\n",
    "\n",
    "#### Context\n",
    "\n",
    "The basic problem is one of parameter inference, which you're familar with by now. What distinguishes this assignment from previous ones is\n",
    "1. the number of free parameters\n",
    "2. the (relatively) larger computational expense of evaluating the likelihood\n",
    "\n",
    "Consequently, this is a good excuse for you (in the homework) to figure out how to work with one of the pre-packaged MCMC algorithms that exist. We have a non-exhaustive list [here](../../doc/MCMC_packages.md), but anything is fair game apart from the following restrictions:\n",
    "1. You must be able to articulate how the method works (at the superficial level of our expositions in lecture).\n",
    "2. `emcee` is off limits, since you previously saw it in homework, and we use it again in this tutorial.\n",
    "\n",
    "We'll give you a lot of code below. It isn't intentionally buggy, but do look through it and make sure you're happy with what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the imports I think we might need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astropy.io.fits as pyfits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from astropy.visualization import LogStretch\n",
    "logstretch = LogStretch()\n",
    "import scipy.stats as stats\n",
    "from io import StringIO\n",
    "import daft\n",
    "from matplotlib import rc\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "class SolutionMissingError(Exception):\n",
    "    def __init__(self):\n",
    "        Exception.__init__(self,\"You need to complete the solution for this code to work!\")\n",
    "def REPLACE_WITH_YOUR_SOLUTION():\n",
    "    raise SolutionMissingError\n",
    "REMOVE_THIS_LINE = REPLACE_WITH_YOUR_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a class to deal with reading in and handling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XrayData:\n",
    "    def __init__(self, imagefile, expmapfile):\n",
    "        \"\"\"\n",
    "        Read in the data from `imagefile' and `expmapfile'.\n",
    "        Store the actual arrays from these in `im' and `ex'.\n",
    "        Create helper arrays of the x and y IMAGE coordinates of each pixel, stored in `xs' and `ys'.\n",
    "         - Do remember that IMAGE coordinates are not the same thing as array indices!\n",
    "        Create a `mask' array indicating whether a given image pixel is actually illuminated (exptime>0).\n",
    "         - More generally, this can be used to indicate whether we include a given pixel in the analysis.\n",
    "         - This is real valued with 0's and 1's indicating exclusion and inclusion. There were once reasons for this.\n",
    "        \"\"\"\n",
    "        ### read in the data\n",
    "        self.imfits = pyfits.open(imagefile)\n",
    "        self.im = self.imfits[0].data\n",
    "        self.exfits = pyfits.open(expmapfile)\n",
    "        self.ex = self.exfits[0].data\n",
    "        ### make helper arrays of x and y coordinates\n",
    "        self.xs = np.array([np.arange(self.im.shape[0]) for j in np.arange(self.im.shape[1])])\n",
    "        self.ys = np.array([[j for i in np.arange(self.im.shape[1])] for j in np.arange(self.im.shape[0])])\n",
    "        ### create a mask\n",
    "        self.mask = self.ex * 0.0\n",
    "        self.mask[self.ex > 0.0] = 1.0\n",
    "    def mask_circle(self, x, y, r):\n",
    "        \"\"\"\n",
    "        Change the `mask' array such that a circular region with radius `r' (in pixels) centered on `x' and `y'\n",
    "        (in IMAGE coordinates) will be flagged to be ignored. For good measure, set `ex' to 0.0 in the same region.\n",
    "        \"\"\"\n",
    "        there = ( (self.xs-x)**2 + (self.ys-y)**2 <= r**2 )\n",
    "        self.mask[there] = 0.0\n",
    "        self.ex[there] = 0.0\n",
    "    def show(self):\n",
    "        \"\"\"Plot all the arrays we're holding onto, applying the current mask.\"\"\"\n",
    "        plt.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "        fig, axs = plt.subplots(2, 3)\n",
    "        axs[0,0].imshow(logstretch(self.im*self.mask), cmap='gray', origin='lower');\n",
    "        axs[0,0].set_title(\"Masked image\");\n",
    "        axs[0,1].imshow(self.ex*self.mask, cmap='gray', origin='lower');\n",
    "        axs[0,1].set_title(\"Masked exposure map\");\n",
    "        axs[0,2].imshow(self.mask, cmap='gray', origin='lower');\n",
    "        axs[0,2].set_title(\"Mask\");\n",
    "        axs[1,0].imshow(self.xs*self.mask, cmap='gray', origin='lower');\n",
    "        axs[1,0].set_title(\"Masked X ramp\");\n",
    "        axs[1,1].imshow(self.ys*self.mask, cmap='gray', origin='lower');\n",
    "        axs[1,1].set_title(\"Masked Y ramp\");\n",
    "        axs[1,2].set_title(\"Nothing\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reading in the data and displaying it as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(open('Solution/where_the_data_at.py').read())\n",
    "except IOError:\n",
    "    data_path = REPLACE_WITH_YOUR_SOLUTION() # relative or absolute path to the folder where the data files are\n",
    "                                             # e.g., \"./\"\n",
    "data = XrayData(data_path+\"P0098010101M2U009IMAGE_3000.FTZ\", data_path+\"P0098010101M2U009EXPMAP3000.FTZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the point source masks, using the list of sources we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regions_string = \\\n",
    "u\"\"\"232 399 9.6640149\n",
    "188 418 6.6794089\n",
    "362 474 7.8936824\n",
    "336 417 5.8364482\n",
    "381 359 3.8626665\n",
    "391 418 5.5885783\n",
    "398 294 3.538914\n",
    "417 209 5.2474726\n",
    "271 216 5.3269609\n",
    "300 212 6.0974003\n",
    "286 162 3.7078355\n",
    "345 153 4.8141418\n",
    "168 361 5.6344116\n",
    "197 248 4.6760734\n",
    "277 234 5.0308471\n",
    "241 212 4.1267598\n",
    "251 379 4.4363759\n",
    "310 413 2.5081459\n",
    "460 287 5.9048854\n",
    "442 353 4.6259039\n",
    "288 268 4.4204645\n",
    "148 317 4.7704631\n",
    "151 286 7.9281045\n",
    "223 239 5.561259\n",
    "490 406 4.0450217\n",
    "481 318 4.7402437\n",
    "\"\"\"\n",
    "regions = np.loadtxt(StringIO(regions_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply each region as a mask\n",
    "for reg in regions:\n",
    "    data.mask_circle(reg[0], reg[1], reg[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that it worked as expected (compare with above)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We will use a common parametric model for the surface brightness of galaxy clusters: the azimuthally symmetric beta model.\n",
    "\n",
    "$S_\\beta(r) = S_0 \\left[1.0 + \\left(\\frac{r}{r_c}\\right)^2\\right]^{-3\\beta + 1/2}$,\n",
    "\n",
    "where $r$ is projected distance from the cluster center.  Note that this model describes a 2D surface brightness distribution, with $r^2 = (x-x_0)^2 + (y-y_0)^2$ and $(x_0,y_0)$ being the cluster center. Also note that pixels are perfectly respectable units for coordinates and projected distance for our purposes.\n",
    "\n",
    "The model should also include bacgkround, which for simplicity we will take to be uniform. So the total model surface brightness is\n",
    "\n",
    "$S(r) = S_0 \\left[1.0 + \\left(\\frac{r}{r_c}\\right)^2\\right]^{-3\\beta + 1/2} + b$,\n",
    "\n",
    "with model parameters $x_0$, $y_0$, $S_0$, $r_c$, $\\beta$ and $b$. From experience, we know that the beta model parameters are degenerate such that $\\ln(S_0)$ is much easier to sample than $S_0$, so we'll do that below.\n",
    "\n",
    "Finally, we'll work with $S$ in units of counts/second/pixel, such that\n",
    "\n",
    "$S(x_i,y_i)$ $\\times$ `ex`$_i$ is the expected number of counts in the $i$th pixel, i.e. the mean of a Poisson likelihood function for the data `im`$_i$.\n",
    "\n",
    "When working with this data set previously, we worried about the point spread function (PSF) blurring the image before we recorded it. Since we're now looking at an object which is significantly more extended than the PSF, we'll simplify by neglecting the blurring in this problem (not a great idea in real life, but it will suit us fine).\n",
    "\n",
    "Useful functions follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaModelFun(r, S0, rc, beta):\n",
    "    '''\n",
    "    The beta model function itself\n",
    "    '''\n",
    "    return S0 * (1.0 + (r/rc)**2)**(-3.0*beta + 0.5)\n",
    "\n",
    "def betaModelImage(xs, ys, x0, y0, S0, rc, beta):\n",
    "    '''\n",
    "    Returns an image with values given by the (azimuthally symmetric) beta model parameters.\n",
    "    The inputs `xs' and `ys' should be arrays of x and y coordinate values (in the same coordinate\n",
    "      system as x0 and y0), and the rest are beta model parameters.\n",
    "    The shape of the output is the same as that of `xs' and `ys' (which had better be the same).\n",
    "    '''\n",
    "    return betaModelFun(np.sqrt( (xs-x0)**2 + (ys-y0)**2 ), S0, rc, beta)\n",
    "\n",
    "def modelImage(data, x0, y0, S0, rc, beta, bg):\n",
    "    '''\n",
    "    Returns an image of the whole model (beta model plus background).\n",
    "    `data' should be of XrayData type.\n",
    "    Note that the output is *not* masked according to `data.mask'.\n",
    "    '''\n",
    "    return (betaModelImage(data.xs, data.ys, x0, y0, S0, rc, beta) + bg) * data.ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even a PGM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 14, 8\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "pgm = daft.PGM([8,5], observed_style=\"inner\", label_params={'fontsize':15})\n",
    "pgm.add_node(daft.Node(\"b\", r\"$b$\", 0.5, 0.5))\n",
    "pgm.add_node(daft.Node(\"beta\", r\"$\\beta$\", 0.5, 1.25))\n",
    "pgm.add_node(daft.Node(\"rc\", r\"$r_c$\", 0.5, 2.0))\n",
    "pgm.add_node(daft.Node(\"s0\", r\"$S_0$\", 0.5, 2.75))\n",
    "pgm.add_node(daft.Node(\"y0\", r\"$y_0$\", 0.5, 3.5))\n",
    "pgm.add_node(daft.Node(\"x0\", r\"$x_0$\", 0.5, 4.35))\n",
    "pgm.add_plate(daft.Plate([1.0, 1.0, 5.0, 3.0], position=\"bottom right\", label=r\"unmasked pixels $i$\"))\n",
    "pgm.add_node(daft.Node(\"Sbeta\", r\"$S_\\beta$\", 2.0, 3.0, fixed=True, offset=[5.0, 0.0]))\n",
    "pgm.add_node(daft.Node(\"xi\", r\"$x_i$\", 1.5, 2.0, fixed=True, offset=[0.0, -20.0]))\n",
    "pgm.add_node(daft.Node(\"yi\", r\"$y_i$\", 2.25, 2.0, fixed=True, offset=[0.0, -20.0]))\n",
    "pgm.add_node(daft.Node(\"Stot\", r\"$S$\", 4.0, 2.5, fixed=True))\n",
    "pgm.add_node(daft.Node(\"ex\", r\"ex$_i$\", 5.0, 3.25, fixed=True))\n",
    "pgm.add_node(daft.Node(\"im\", r\"im$_i$\", 5.0, 2.25, observed=True))\n",
    "pgm.add_edge(\"x0\", \"Sbeta\")\n",
    "pgm.add_edge(\"y0\", \"Sbeta\")\n",
    "pgm.add_edge(\"s0\", \"Sbeta\")\n",
    "pgm.add_edge(\"rc\", \"Sbeta\")\n",
    "pgm.add_edge(\"beta\", \"Sbeta\")\n",
    "pgm.add_edge(\"xi\", \"Sbeta\")\n",
    "pgm.add_edge(\"yi\", \"Sbeta\")\n",
    "pgm.add_edge(\"Sbeta\", \"Stot\")\n",
    "pgm.add_edge(\"b\", \"Stot\")\n",
    "pgm.add_edge(\"Stot\", \"im\")\n",
    "pgm.add_edge(\"ex\", \"im\")\n",
    "pgm.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose priors\n",
    "\n",
    "As we have emphasized in class, the prior encodes what information we bring to the analysis before gathering the data. As such, this is the appropriate time to decide what priors we'd like to use. Even though you may not have any relevant domain knowledge (and we don't expect you to gain any just for this problem), the model itself motivates certain prior constraints. For example, $r_c$ is a scale for the radius, and therefore can't be negative or zero. A (weak) statement similar to this can be made about most of the model parameters.\n",
    "\n",
    "Beyond such physically/mathematically motivated bounds, next the task is to decide on a specific prior distribution for each parameter. Please don't kill yourselves trying to come up with the most sophisticated or information theoretically motivated thing. Imagine trying to justify your choice to a thesis advisor who, we'll say hypothetically, has no appreciation for such subtleties and just wants to hear something that sounds uninformative.\n",
    "\n",
    "Note that one of the tasks in this week's HW will be to justify the choice of priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come up with a set of parameter values that are broadly good\n",
    "\n",
    "Because the likelihood in this problem (or at least my naive implementation of it) is relatively slow to evaluate, it is well worth spending some time coming up with a decent ballpark guess at the solution. You do not want to be sitting around for an hour only to find that your chain was started in such a poor position that it still hasn't made it anywhere useful.\n",
    "\n",
    "Broadly sensible values of the other parameters can be estimated by looking at the data in various ways. The least obvious in this context is probably $\\beta$, so we'll mention that the value $\\beta \\sim 2/3$ is a canonical one that comes from the simple spherical collapse model of cluster formation.\n",
    "\n",
    "The cell below provides code to inspect the residual image (data-model) for a given set of parameters. This isn't the only, nor probably the best, way to inspect the data for this purpose, but it's one that we have to hand already. If you can find model parameters that remove most of the structure from the residual image (it doesn't have to be perfect), then you'll be broadly in the right neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(open('Solution/starting_point.py').read())\n",
    "except IOError:\n",
    "    start = REPLACE_WITH_YOUR_SOLUTION() # a list, NOT a numpy array\n",
    "\n",
    "x0, y0, lnS0, rc, beta, bg = start\n",
    "model_image = modelImage(data, x0, y0, np.exp(lnS0), rc, beta, bg)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16.0, 5.0)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(logstretch(data.im*data.mask), cmap='gray', origin='lower');\n",
    "axs[0].set_title(\"Data\");\n",
    "axs[1].imshow(logstretch(model_image*data.mask), cmap='gray', origin='lower');\n",
    "axs[1].set_title(\"Model\");\n",
    "axs[2].imshow(logstretch((data.im-model_image)*data.mask), cmap='gray', origin='lower');\n",
    "axs[2].set_title(\"Residual\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the posterior\n",
    "\n",
    "Write functions to evaluate the prior, likelihood and posterior as a function of `params` = ($x_0$, $y_0$, $\\ln S_0$, $r_c$, $\\beta$, $b$), taking note of the use of $\\ln S_0$ rather than $S_0$ (including/especially in your prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(open('Solution/prior.py').read())\n",
    "except IOError:\n",
    "    REMOVE_THIS_LINE()\n",
    "    def ln_prior(params):\n",
    "        x0, y0, lnS0, rc, beta, bg = params\n",
    "        S0 = np.exp(lnS0)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the likelihood function. Note we need to eventually take a sum that counts only *some* pixels, according to `data.mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(open('Solution/likelihood.py').read())\n",
    "except IOError:\n",
    "    REMOVE_THIS_LINE()\n",
    "    def ln_likelihood(params, data): # `data' of XrayData type\n",
    "        x0, y0, lnS0, rc, beta, bg = params\n",
    "        S0 = np.exp(lnS0)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    exec(open('Solution/posterior.py').read())\n",
    "except IOError:\n",
    "    REMOVE_THIS_LINE()\n",
    "    def ln_posterior(params, data): # `data' of XrayData type\n",
    "        pass # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test these functions out** with some reasonable parameter values before moving on. Also, make sure that _unreasonable_ values (in terms of numerical evaluation) will be caught and dealt with somehow in your implementations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from the posterior using `emcee`\n",
    "\n",
    "Below, we set up `emcee` and run it to produce a small number of samples. Do run for longer if you want something to compare your homework results to, but be aware that it may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package authors advise us to start all walkers in a clump around a single starting point, so let's do that, using the \"reasonable fit\" parameters you found above. Maybe it would make sense to expand the clump if we're not so sure about how good the starting position is? (I haven't tried it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwalkers = 20 # an arbitrary number I chose\n",
    "ndim = len(start) # dimension of the parameter space\n",
    "\n",
    "# Starting points for all walkers, the 1% Gaussian offsets being another arbitrary choice\n",
    "p0 = np.array([start*(1.0 + 0.01*np.random.rand(ndim)) for j in range(nwalkers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More setup. Set `nthreads` appropriately for your machine to take advantage of parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nthreads = 1 # Set for your system\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior, args=[data], threads=nthreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the sampler. Use a small `Nsteps` at first, just to get an idea of how long a real run will take. (On my laptop, it's ~1s per step, for my particular likelihood implementation and `nwalkers`=20.) Then do a longer run to see what happens as far as convergence. Eventually, run long enough to get real results.\n",
    "\n",
    "Note that subsequent calls to the same `sampler` will continue an existing run, which is convenient. If you want to start over with a new starting point, for e.g., it makes sense to re-instantiate `sampler` with the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsteps = 10 # Adjust this\n",
    "\n",
    "%time sampler.run_mcmc(p0, Nsteps);\n",
    "ens = sampler.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn = 0 # initial inspection with burn=0, then make it larger\n",
    "N2show = 8 # number of walkers to plot\n",
    "param_names = [r'$x_0$', r'$y_0$', r'$\\ln S_0$', r'$r_c$', r'$\\beta$', r'$b$']\n",
    "symbol = \"\"\n",
    "if Nsteps < 100: symbol = 'o'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 20.0)\n",
    "for i in range(ndim):\n",
    "    plt.subplot(ndim, 1, i+1)\n",
    "    for j in range(N2show):\n",
    "        plt.plot(ens[j,burn:,i], symbol);\n",
    "    plt.ylabel(param_names[i], fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a corner plot, because why not. (If your chain is extremely long, you might want to do some thinning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corner.corner(ens[:,burn:,:].reshape((-1, ndim)), labels=param_names,\n",
    "              color='blue', show_titles=True, title_fmt='.2g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the end of the tutorial! Hopefully you've got good results from a long, converged chain. If so, it would be good to compare them with what you get in the homework portion, e.g. with contours from both in the same corner plot.\n",
    "\n",
    "You might for example, want to save the flattened ensemble (\"chain\"), the fancy subscripted and reshaped `ens` that's passed to `corner` above, with `np.savetxt` or similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
