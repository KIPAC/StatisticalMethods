{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Comparison of Galaxy Cluster Centering Models\n",
    "\n",
    "In the [previous tutorial](model_evaluation.ipynb), we evaluated the \"goodness of fit\" of two different models on a data set representing the distribution of centering offsets in a galaxy cluster sample. Here we will continue by doing two styles of quantitative test to compare the two models, with the goal of deciding whether an improved goodness of fit actually justifies the additional complexity of the second model. Specifically, you will calculate\n",
    "* the Deviance Information Criterion, one of several possible information criteria, which has the advantages of being relatively simple and having a straightforward Bayesian interpretation;\n",
    "* the Bayesian evidence, a more principled but more complex approach fully in the Bayesian framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TutorialName = 'model_comparison'\n",
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up\n",
    "\n",
    "First, we'll want to take advantage of your work in the previous tutorial. As irritating as it may be, paste the (completed) definitions of `Model`, `ExponentialModel` and your alternative model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # cut/paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll read in the chains for each model that you produced. (You'll need to fill in the name of your model class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1 = ExponentialModel(samples=np.loadtxt('saved/centering_model1_chain.txt', ndmin=2))\n",
    "TBC() # Model2 = YourModel(samples=np.loadtxt('saved/centering_model2_chain.txt', ndmin=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're as lazy as me, your implementation above relied on the data `y` being at global scope, so here they are again. If there were any other global variable dependences I haven't anticipated, you'll need to reproduce their definitions here also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([39.30917,35.13419,5.417072,59.75137,30.69077,14.45971,27.07368,27.48429,80.60219,483.1432,24.65057,\n",
    "              22.36524,43.39081,39.89816,30.67409,6.905061,53.69709,9.504133,41.07874,10.9369,48.29861,61.34125,\n",
    "              68.37279,30.51124,26.74462,13.7165,6.043301,976.1495,27.20097,7.818419,5.589193,3.310114,271.8901,\n",
    "              126.0384,99.51247,249.1279,403.0484,3.071718,0.9434036,54.94336,1.529382,8.441071,19.59434,59.43049,\n",
    "              77.21293,29.6533,286.7116,11.2386,9.511912,29.04711,33.77766,151.4803,223.3557,12.33816,25.22682,\n",
    "              26.86597,339.7084,405.6737,3.809868,221.6523,307.2994,73.36697,42.15523,36.74785,5.415392,69.4721,\n",
    "              136.8073,17.3534,4.135966,20.19435,79.06968,8.095599,4.474533,44.90669,85.891,1.636425,75.39335,\n",
    "              15.94149,2.828709,20.5636,41.52905,42.51133,104.3908,67.41335,13.80204,394.9841,33.90415,84.78714,\n",
    "              36.77924,14.48424,66.01276,2.910331,92.79938,29.74337,42.40971,1.692674,1.039994,120.5902,154.7106,\n",
    "              14.38967,147.8399,166.5054,87.53685,22.63141,638.1976,273.6167,593.4997,45.57279,87.30421,75.03385,\n",
    "              18.33932,36.05779,3.659462,263.9074,0.2432062,8.499095,1.160031,38.16615,41.65371,361.5,148.9294,\n",
    "              10.25777,71.29159,10.02279,16.36062,601.1667,4.960311,12.22526,87.54137,48.48371,78.56777,212.8153,\n",
    "              77.0353,62.7624,81.26739,34.36881,42.63432,264.4551,15.24863,25.94133,35.88882,34.94669,222.5425,\n",
    "              304.9676,19.68377,7.216153,17.61534,32.25887,14.08842,773.5914])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate the DIC for each model\n",
    "\n",
    "Recall that the Deviance Information Criterion is given by:\n",
    "\n",
    "$\\mathrm{DIC} = \\langle D(\\theta) \\rangle + p_D; \\quad p_D = \\langle D(\\theta) \\rangle - D(\\langle\\theta\\rangle)$\n",
    "\n",
    "where $\\theta$ are the parameters of a model, the deviance $D(\\theta)=-2\\log P(\\mathrm{data}|\\theta)$, and averages $\\langle\\rangle$ are over the posterior distribution of $\\theta$.\n",
    "\n",
    "Write a function to compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIC(Model):\n",
    "    \"\"\"\n",
    "    Compute the Deviance Information Criterion for the given model.\n",
    "    (In a less pedagogical world, this would logically be a method of the base Model class.)\n",
    "    \"\"\"\n",
    "    # Compute the deviance D for each sample\n",
    "    D = -2.0*np.array([ Model.log_likelihood(*params) for params in Model.samples ])\n",
    "    #pD = \n",
    "    #DIC = \n",
    "    #return DIC, pD\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the DIC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC1, pD1 = DIC(Model1)\n",
    "print(Model1.name+':')\n",
    "print(\"Effective number of fitted parameters =\", pD1)\n",
    "print(\"DIC =\", DIC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** For Model 1 (the exponential), I get $p_D \\approx 1.0$ and DIC $\\approx 1668$. As with anything else computed from chains, there will be some stochasticity to the values you compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC2, pD2 = DIC(Model2)\n",
    "print(Model2.name+':')\n",
    "print(\"Effective number of fitted parameters =\", pD2)\n",
    "print(\"DIC =\", DIC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your values of $p_D$ make intuitive sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to interpret this, we can compare the reduction (hopefully) in the DIC of Model 2 compared with Model 1 to the Jeffreys scale (see the [notes](../notes/model_evaluation.ipynb)). By this metric, is your second model better at explaining the data than the exponential model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC1 - DIC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute the evidence by Monte Carlo integration\n",
    "\n",
    "To do this, note that\n",
    "\n",
    "$p(\\mathrm{data}|H)=\\int d\\theta \\, p(\\mathrm{data}|\\theta,H) \\, p(\\theta|H)$\n",
    "\n",
    "can be approximated by averaging the likelihood over samples from the prior:\n",
    "\n",
    "$p(\\mathrm{data}|H) \\approx \\frac{1}{m}\\sum_{k=1}^m p(\\mathrm{data}|\\theta_k,H)$, with $\\theta_k\\sim p(\\theta|H)$.\n",
    "\n",
    "This estimate is much more straightforward than trying to use samples from the posterior to calculate the evidence (which would require us to be able to normalize the posterior, which would require an estimate of the evidence, ...). But in general, and especially for large-dimensional parameter spaces, it is very inefficient (because the likelihood typically is large in only a small fraction of the prior volume). Still, let's give it a try.\n",
    "\n",
    "Write a function to draw a large number of samples from the prior and use them to calculate the evidence. To avoid numerical over/underflows, use the special `scipy` function `logsumexp` (which we imported directly, way at the top of the notebook) to do the sum. As the name implies, this function is equivalent to `log(sum(exp(...)))`, but is more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_evidence(Model, N=1000):\n",
    "    \"\"\"\n",
    "    Compute the log evidence for the model using N samples from the prior\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick test to check for NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_evidence(Model1, N=2), log_evidence(Model2, N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly how precisely do we need to know the log Evidence, to be able to compare models? Run `log_evidence` with different values of `N` (the number of prior samples in the average) to until you're satisfied that you're getting a usefully accurate result for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Model 1:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]:\n",
    "    %time logE1 = log_evidence(Model1, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Model 2:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]:\n",
    "    %time logE2 = log_evidence(Model2, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # SPEEDBUMP: Do the two evidence calculations look converged as a function of N?\n",
    "#                  If not, increase the number of samples in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have log evidences computed for each model. Now what? We just compare their difference to the Jeffreys scale again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE2 - logE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we might end up with a different conclusion as to the strength of any preference of Model 2, compared with the DIC! The reason is that the evidence explicitly accounts for the information in the prior (which, recall, counts as part of the model definition), while the DIC does this much less directly.\n",
    "\n",
    "We could also be good Bayesians and admit that there should be a prior distribution in model space. For example, maybe I have a very compelling theoretical reason why the offset distribution should be exponential (I don't, but just for example). Then, I might need some extra convincing that an alternative model is required.\n",
    "\n",
    "We would then compute the ratio of the posterior probabilities of the models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_H1 = 0.9 # or your choice\n",
    "prior_H2 = 1.0 - prior_H1 # assuming only these two options\n",
    "\n",
    "log_post_H1 = np.log(prior_H1) + logE1\n",
    "log_post_H2 = np.log(prior_H2) + logE2\n",
    "\n",
    "print('Difference of log posteriors (H2-H1):', log_post_H2-log_post_H1)\n",
    "print('Ratio of posteriors (H2/H1):', np.exp(log_post_H2-log_post_H1)) # NB this one might over/underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the fitness of the alternative model you chose, you may find that only an extremely lopsided prior in model space would influence your conclusion.\n",
    "\n",
    "Comment on what you find from the evidence, compared with the DIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the evidence with `dynesty`\n",
    "\n",
    "To get some experience with a package that uses nested sampling to compute the evidence, let's repeat Section 2 using `dynesty`.\n",
    "\n",
    "Looking at [the docs](https://dynesty.readthedocs.io/en/latest/crashcourse.html), we first need a function that maps the unit cube onto our prior. That is, the code is doing a substitution like\n",
    "\n",
    "$\\int d\\theta\\,p(\\theta|H)\\,p(\\mathrm{data}|\\theta,H) = \\int_0^1 dF \\,p\\left[\\mathrm{data}|\\theta(F),H\\right]$,\n",
    "\n",
    "where $F=\\int_{-\\infty}^\\theta d\\theta'\\,p(\\theta'|H)$ is the cumulative distribution function of the prior (hence the identity $dF/d\\theta = p(\\theta|H)$ makes the equation above work out).\n",
    "\n",
    "If our priors were uniform, the translation from $F$ to $\\theta$ is a simple translation and rescaling, but for other priors it involves going through the prior distribution's quantile function. Fortunately, we assumed uniform priors for Model 1! However, if you followed the advice in the previous notebook to keep a dictionary of univariate priors in the `Model` object, and those priors are `scipy.stats` distributions, it's relatively easy to handle this slightly more general case using functions provided by `scipy`. Regrdless of how you choose to do it, implement a function that performs this transformation for the 1-parameter Exponential model (and priors) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Do a sanity check - the cell below should return the quartiles of the uniform prior you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptform([0.25]), ptform([0.5]), ptform([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a log-likelihood function that takes a vector of parameters as input, in contrast to how it's written in the Model class. This could be solved by using the ridiculous construction in the `Model.log_posterior` method, or we could just define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like(params):\n",
    "    return Model1.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just go ahead and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like, ptform, len(Model1.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at some stuff (see the Dynesty documentation for an explanation of what all this is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a summary of the run.\n",
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evidence panel appears to be utterly unhelpful in this case (at least when I ran it), so below is a plot of the log-evidence as a function of iteration, discarding the beginning when the accumulated evidence is truly tiny. It should show us converging to a value similar to what we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=12);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts the log-evidence at the last iteration, which may or not be the approved method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with what we got from simple monte carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE1 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was so easy, let's go ahead and do the same for Model 2. Implement the transformation from the unit cube to your model's parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform2(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this elegant definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like2(params):\n",
    "    return Model2.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like2, ptform2, len(Model2.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and produce the same set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=12);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, extract the final evidence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and compare it to the SMC result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE2 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the work you'd already done, that was (I hope) staggeringly easy to set up, even if it did take longer to compute the same answer (I find) in this case. In larger-dimensional parameter spaces, it's easy to imagine this method being far preferable to blindly sampling from the prior. Note that we run `dynesty` out of the box here - there are many options (and some potential failure modes) that you should read about before using it in the real world."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
