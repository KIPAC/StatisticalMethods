{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Toy Photometry with Metropolis Sampling\n",
    "\n",
    "In this notebook, we will use Metropolis sampling to solve the inference problem that we previously struggled to carry out [on a grid](toy_photometry_grid.ipynb). Have a look back at that tutorial if you need a refresher on the general setup. You will:\n",
    "\n",
    "* implement the Metropolis sampling algorithm\n",
    "* obtain samples from the 2D posterior of the simplified version of the problem\n",
    "* obtain samples from the full 4D posterior\n",
    "* marvel at how much more efficient that was than working on a grid\n",
    "* visualize credible intervals and regions\n",
    "* use `pygtc` to make a triangle plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TutorialName = 'toy_metro'\n",
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "%matplotlib inline\n",
    "from pygtc import plotGTC\n",
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll begin by reading in the mock data set you worked with the grid-based tutorial, and storing it in an `Image` object, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_image = np.loadtxt('data/toy_photometry.dat')\n",
    "class Image:\n",
    "    def __init__(self, image):\n",
    "        self.im = image\n",
    "        self.imx,self.imy = np.meshgrid(range(image.shape[1]), range(image.shape[0]))\n",
    "data = Image(mock_image)\n",
    "plt.imshow(data.im, origin='lower');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll still want to refer to the truth model used to generate the above data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramnames = ['x0', 'y0', 'mu0', 'sigma'] # the canonical parameter order, because I said so\n",
    "truth = {'x0':17.1, 'y0':12.75, 'mu0':50.0, 'sigma':3.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation\n",
    "\n",
    "This time we're going to do a fit using the Metropolis algorithm. Unlike the case of Gibbs sampling, this doesn't require (conversely: doesn't get to directly exploit) any nice mathematical properties of the posterior, so you can recycle the `log_prior` and `log_likelihood` functions from your [grid-based analysis](toy_photometry_grid.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(x0, y0, mu0, sigma):\n",
    "    TBC()\n",
    "\n",
    "def mean_img(x0, y0, mu0, sigma):\n",
    "    '''\n",
    "    Return an array with the expected mean counts in each pixel for these model parameters\n",
    "    '''\n",
    "    TBC()\n",
    "\n",
    "def log_likelihood(data, **params):\n",
    "    '''\n",
    "    `data` is an Image object, as defined above\n",
    "    '''\n",
    "    mu = mean_img(**params)\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the free log-posterior function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(data, **params):\n",
    "    lnp = log_prior(**params)\n",
    "    if np.isfinite(lnp):\n",
    "        lnp += log_likelihood(data, **params)\n",
    "    return lnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's check that they return finite values without obvious bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( log_prior(**truth) )\n",
    "print( log_likelihood(data, **truth) )\n",
    "print( log_posterior(data, **truth) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler implementation\n",
    "\n",
    "Next, we need a proposal distribution. I'll use a multivariate Gaussian centered on the current position. This is translationally invariant, so later on we can use the simple Metropolis acceptance rule instead of the slightly more complex Metropolis-Hastings rule. A Gaussian isn't necessarily the best choice in general, since the most likely proposals are very small steps, but it will do for the moment.\n",
    "\n",
    "To further keep it simple, let's make the proposal independent in each parameter (a diagonal covariance matrix for the 4-dimensional Gaussian). Similarly to the grid method, you'll want to guess the appropriate order of magnitude for steps in each parameter, which is the same order as the width of the posterior, and you may need to return to this point to adjust them after seeing the performance.\n",
    "\n",
    "Since we're assuming a diagonal covariance, let's go ahead and just represent the proposal distribution as 4 univariate Gaussians, as in the dictionary below.\n",
    "\n",
    "Aside: you may not have seen it before in this class, but calling `scipy.norm()` as below produces what they call a \"frozen\" probability distribution object, with fixed parameters. The standard deviation is whatever you specify via the `scale` argument, and the (unspecified) mean would remain at the default value of 0.0. This means that each entry should be interpreted as a distribution for the **displacement** of a proposal from the current position of a chain. In other words, the proposal distributon density for a change to $x_0$ could be computed with `proposal_distribution['x0'].pdf(x0_proposed - x0_current)`. Other methods of `scipy` probability distributions, in particular random number generation, are also available; the main difference from how we've used them before is that we can't/don't need to specify parameter values in the call because they've already been frozen in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# proposal_distribution = {'x0':st.norm(scale=...) ,\n",
    "#                          'y0':st.norm(scale=...),\n",
    "#                          'mu0':st.norm(scale=...),\n",
    "#                          'sigma':st.norm(scale=...)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that returns a proposed point in parameter space, given the current location and the above dictionary of proposal distributions.\n",
    "\n",
    "_Technical note:_ You might be tempted to begin this function with a command like `proposal = current_params`. If so, remember that, in Python, `b = a` does not make a copy of `a` if `a` is a dictionary (or a `numpy` array for that matter). Both `b` and `a` would point to the same data in memory. The safest/quickest way to get a new dictionary with the same structure as `a` whose values can then be safely overwritten is with `b = a.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose(current_params, dists):\n",
    "    \"\"\"\n",
    "    current_params: dictionary holding current position in parameter space\n",
    "    dists: dictionary of proposal distributions\n",
    "    \n",
    "    Return value: a new dictionary holding the proposed destination in parameter space\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test starting position:', truth)\n",
    "print('Test proposal:', propose(truth, proposal_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the sampler itself. Write a function that takes the current parameter values and log-posterior value as input (along with the data and proposal distributions), and returns the next set of parameters values and corresponding log-posterior as a tuple. These can be identical to the inputs, if the proposal is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(data, current_params, current_lnP, proposal_dists):\n",
    "    \"\"\"\n",
    "    data: Image object\n",
    "    current_params: dictionary of parameter values\n",
    "    current_lnP: log-posterior density corresponding to current_params\n",
    "    proposal_dists: dictionary of proposal distributions\n",
    "    \n",
    "    Return value: a tuple holding the next parameter dictionary and corresponding log-posterior density\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    # trial_params = ...\n",
    "    # trial_lnP = ...\n",
    "    # if [accept/reject condition]:\n",
    "    #    return (trial_params, trial_lnP)\n",
    "    # else:\n",
    "    #    return (current_params, current_lnP)\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, make sure it works without crashing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try is several times to get a mixture of acceptances and rejections, hopefully\n",
    "for i in range(10):\n",
    "    print(step(data, truth, log_posterior(data, **truth), proposal_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting for 2 parameters\n",
    "\n",
    "There's fundamentally no functional difference between fitting the 2-parameter or 4-parameter models with this method. But, for symmetry with the other notebooks, let's start with the 2-parameter fit (with $x_0$ and $y_0$ fixed to the truth) anyway.\n",
    "\n",
    "Given the work you've done above, a quick and dirty way to accomplish this is to use a delta-function proposal distribution for $x_0$ and $y_0$, ensuring that they never change in value. That way we don't need to change any of the existing code to explicitly account for some parameters being fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_distribution2 = proposal_distribution.copy()\n",
    "proposal_distribution2['x0'] = st.norm(scale=0.0)\n",
    "proposal_distribution2['y0'] = st.norm(scale=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a chain, starting the free parameters, $\\mu_0$ and $\\sigma$, from random but broadly reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = truth.copy() # to get x0 and y0\n",
    "params ['mu0'] = st.uniform.rvs()*100.0\n",
    "params['sigma'] = st.uniform.rvs()*9.9 + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand how the cell below is making use of the functions you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nsamples = 10000\n",
    "samples2 = np.zeros((nsamples, 2))\n",
    "\n",
    "current_lnP = log_posterior(data, **params)\n",
    "for i in range(samples2.shape[0]):\n",
    "    params,current_lnP = step(data, params, current_lnP, proposal_distribution2)\n",
    "    samples2[i,:] = [params[k] for k in ['mu0', 'sigma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the most basic (yet still extremely important) visual check to see how our sampler performed, looking at traces of the Markov chain for each parameter. (It's ok if you haven't read the notes on [MCMC Diagnostics](../notes/mcmc_diagnostics.ipynb) yet; we will go more in-depth later.) These trace plots show the value of each parameter as a function of iteration, and we'll add a line showing the value that was used to create the mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = [r'$x_0$', r'$y_0$', r'$\\mu_0$', r'$\\sigma$']\n",
    "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n",
    "fig, ax = plt.subplots(2,1);\n",
    "cr.plot_traces(samples2, ax, labels=[r'$\\mu_0$', r'$\\sigma$'], truths=[truth['mu0'], truth['sigma']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what you see here (in particular, how correlated each sequence appears and how many repeated values there are) will depend on the width of your proposal distributions. But, hopefully, you can see the sequence finding its way to a part of parameter space that it likes, and then continuing to jump around there. If you want to, you can go back and adjust the proposal scales to be smaller (if there are many rejected proposals) or larger (if the accepted steps are tiny compared with the long-term variation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plotGTC` to quickly visualize the posterior. This package shows us all the 1D marginalized posteriors and every pair of 2D marginalized posteriors (as a contour plot), after some smoothing, in a triangular grid. However, before doing so, we want to remove the \"burn-in\" period during which the chain is finding its way to the region of high posterior probability. I set this to be the initial 1000 steps below, but you might need to adjust it for your results (though if your burn-in phase is longer than this, it might be worth re-running with a smaller proposal scale).\n",
    "\n",
    "Dashed lines show the truth values the data were simulated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000 # adjust if needed\n",
    "samples2 = samples2[burnin:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(samples2, paramNames=[r'$\\mu_0$', r'$\\sigma$'], truths=[truth['mu0'], truth['sigma']],\n",
    "        figureSize=5, customLabelFont={'size':12}, customTickFont={'size':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The cell below will compare your samples with some we have generated. They won't be identical, but should be extremely close if you've used the priors and data specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = np.loadtxt('solutions/metro.dat.gz')\n",
    "plotGTC([samples2, ours], paramNames=[r'$\\mu_0$', r'$\\sigma$'], chainLabels=['yours', 'ours'],\n",
    "        figureSize=5, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare to the corresponding grid analysis, let's compute credible intervals with the same code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "marg1d = [cr.whist(samples2[:,i]) for i in range(samples2.shape[1])]\n",
    "ci1d = [cr.whist_ci(marg, plot=axes) for marg,axes in zip(marg1d,ax)]\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(param_labels[i+2])\n",
    "    ax[i].axvline(truth[paramnames[i+2]], color='C1', label='truth')\n",
    "ax[0].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the `center` and `width` summaries of each parameter with what you got from the grid analysis. Do you have the same results to the precision that one would normally report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting for 4 parameters\n",
    "\n",
    "Let's now fit for the source position also ($x_0$ and $y_0$). All we really need to change compared with the last section is to use the original proposal distribution dictionary, which contained non-delta-function proposals for all the parameters. We'll again start at a random but broadly reasonable point in parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'mu0':st.uniform.rvs()*100.0,\n",
    "          'sigma':st.uniform.rvs()*9.9 + 0.1,\n",
    "          'x0':st.uniform.rvs()*32.0,\n",
    "          'y0':st.uniform.rvs()*32.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nsamples = 10000\n",
    "samples4 = np.zeros((nsamples, 4))\n",
    "\n",
    "current_lnP = log_posterior(data, **params)\n",
    "for i in range(samples4.shape[0]):\n",
    "    params,current_lnP = step(data, params, current_lnP, proposal_distribution)\n",
    "    samples4[i,:] = [params[k] for k in paramnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "fig, ax = plt.subplots(4,1);\n",
    "cr.plot_traces(samples4, ax, labels=param_labels, truths=[truth[k] for k in paramnames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remove the clear burn-in phase and plot the triangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000 # adjust if needed\n",
    "samples4 = samples4[burnin:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(samples4, paramNames=param_labels, truths=[truth[k] for k in paramnames],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Once again, you can compare these to our own solutions; the posteriors should look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = np.loadtxt('solutions/metro4.dat.gz')\n",
    "plotGTC([samples4, ours], paramNames=param_labels, chainLabels=['yours', 'ours'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the credible intervals, for completeness. We won't bother with the goodness of fit, since the grid analysis did address that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 4.0)\n",
    "fig, ax = plt.subplots(1,len(paramnames))\n",
    "marg1d = [cr.whist(samples4[:,i]) for i in range(samples4.shape[1])]\n",
    "ci1d = [cr.whist_ci(marg, plot=axes) for marg,axes in zip(marg1d,ax)]\n",
    "for i in range(len(paramnames)):\n",
    "    ax[i].set_xlabel(param_labels[i])\n",
    "    ax[i].axvline(truth[paramnames[i]], color='C1', label='truth')\n",
    "ax[0].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any qualitative observations on how the marginalized posteriors compare to their counterparts from the course grid analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci1d # for posterity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later notebooks, we'll want to see how multiple, independent chains with different starting points behave when using this method. The cell below will take care of running 4 chains, started at different positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chains = [np.zeros((10000,4)) for j in range(4)]\n",
    "\n",
    "for samples in chains:\n",
    "    params = {'mu0':st.uniform.rvs()*100.0,\n",
    "              'sigma':st.uniform.rvs()*9.9 + 0.1,\n",
    "              'x0':st.uniform.rvs()*32.0,\n",
    "              'y0':st.uniform.rvs()*32.0}\n",
    "    current_lnP = log_posterior(data, **params)\n",
    "    for i in range(samples.shape[0]):\n",
    "        params,current_lnP = step(data, params, current_lnP, proposal_distribution)\n",
    "        samples[i,:] = [params[k] for k in paramnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a more colorful version of the trace plots, showing all of the chains simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "cr.plot_traces(chains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0},\n",
    "           truths=[truth[k] for k in paramnames], truth_kwargs={'color':'k', 'linestyle':'--'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save them for later, without removing burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,samples in enumerate(chains):\n",
    "    np.savetxt('saved/toy_metro_chain_'+str(i)+'.txt.gz', samples, header=' '.join(paramnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting thoughts\n",
    "\n",
    "There you have it - at the cost of some random walking, you have now fit a model with enough free parameters to make a grid-based solution uncomfortably slow. Metropolis and Metropolis-Hastings are extremely widely applicable and therefore powerful, though one needs to think about how to provide an efficient proposal distribution. We'll see the consequences of not doing so in [MCMC Diagnostics](../notes/mcmc_diagnostics.ipynb) and some intelligent proposal strategies in [More Sampling Methods](../notes/more_samplers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "If you have already done the [Gibbs Sampling](toy_photometry_gibbs.ipynb) tutorial, comment on the differences you see between the chains that the two methods produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer in Markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
