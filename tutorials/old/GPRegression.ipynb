{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 8 Tutorial\n",
    "\n",
    "## Gaussian Process Regression\n",
    "\n",
    "In this example, we return to the \"straight line\" problem, generate some mock data, and investigate a \"model-free model\", a Gaussian Process, for them. The idea is to find a flexible model that can _interpolate_ between the data we have, in order to predict future data lying in the gaps, or beyond the observed domain.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to `pip install scikit-learn` and check that you have v0.18 or higher as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "\n",
    "class SolutionMissingError(Exception):\n",
    "    def __init__(self):\n",
    "        Exception.__init__(self,\"You need to complete the solution for this code to work!\")\n",
    "def REPLACE_WITH_YOUR_SOLUTION():\n",
    "    raise SolutionMissingError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "Let's generate a simple Cepheids-like dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "xlimits = [0,350]\n",
    "ylimits = [0,250]\n",
    "\n",
    "def generate_data(seed=None):\n",
    "    \"\"\"\n",
    "    Generate a 30-point data set, with x and sigma_y as standard, but with\n",
    "    y values given by\n",
    "\n",
    "    y = a_0 + a_1 * x + a_2 * x**2 + a_3 * x**3 + noise\n",
    "    \"\"\"\n",
    "    Ndata = 30\n",
    "\n",
    "    xbar = 0.5*(xlimits[0] + xlimits[1])\n",
    "    xstd = 0.25*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "    x = xbar + xstd * np.random.randn(Ndata)\n",
    "\n",
    "    meanerr = 0.025*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    sigmay = meanerr + 0.3 * meanerr * np.abs(np.random.randn(Ndata))\n",
    "\n",
    "    a = np.array([37.2,0.93,-0.002,0.0])\n",
    "    y = a[0] + a[1] * x + a[2] * x**2 + a[3] * x**3 + sigmay*np.random.randn(len(x))\n",
    "\n",
    "    return x,y,sigmay\n",
    "\n",
    "def plot_yerr(x, y, sigmay):\n",
    "    \"\"\"\n",
    "    Plot an (x,y,sigma) dataset as a set of points with error bars \n",
    "    \"\"\"\n",
    "    plt.errorbar(x, y, yerr=sigmay, fmt='.', ms=7, lw=1, color='k')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$y$', fontsize=16)\n",
    "    plt.xlim(*xlimits)\n",
    "    plt.ylim(*ylimits)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y, sigmay) = generate_data(seed=13)\n",
    "\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Gaussian Process\n",
    "\n",
    "Let's follow [Jake VanderPlas' example](http://www.astroml.org/book_figures/chapter8/fig_gp_example.html#book-fig-chapter8-fig-gp-example), to see how to work with the [`scikit-learn` v0.18](http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#gaussian-processes-regression-basic-introductory-example) Gaussian Process regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF as SquaredExponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining a GP\n",
    "\n",
    "First we define a kernel function, for populating the covariance matrix of our GP. To avoid confusion, a Gaussian kernel is referred to as a \"squared exponential\" (or a \"radial basis function\", RBF). The squared exponential kernel has one hyper-parameter, the length scale that is the Gaussian width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10.0\n",
    "\n",
    "kernel = SquaredExponential(length_scale=h, length_scale_bounds=(0.01, 1000.0))\n",
    "gp0 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's draw some samples from the unconstrained process, o equivalently, the prior. Each sample is a function $y(x)$, which we evaluate on a grid. We'll need to assert a value for the kernel hyperparameter $h$, which dictates the correlation length between the datapoints. That will allow us to compute a mean function (which for simplicity we'll set to the mean observed $y$ value), and a covariance matrix that captures the correlations between datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xgrid = np.atleast_2d(np.linspace(0, 399, 100)).T\n",
    "print(\"y(x) will be predicted on a grid of length\", len(xgrid))\n",
    "\n",
    "# Draw three sample y(x) functions:\n",
    "draws = gp0.sample_y(xgrid, n_samples=3)\n",
    "\n",
    "print(\"Drew 3 samples, stored in an array with shape \", draws.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot these, to see what our prior looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a 4-panel figure:\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# Plot our three prior draws:\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Rescaled prior sample $y(x)$')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y(x)$')\n",
    "ax.legend(fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each predicted $y(x)$ is drawn from a Gaussian of unit variance, and with off-diagonal elements determined by the covariance function. \n",
    "\n",
    "Try changing `h` to see what happens to the smoothness of the predictions. \n",
    "\n",
    "> Go back up to the cell where `h` is assigned, and re-run that cell and the subsequent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our data to be well interpolated by this Gaussian Process, it will need to be rescaled such that it has zero mean and unit variance. There are [standard methods for doing this](http://scikit-learn.org/stable/modules/preprocessing.html), but we'll do this rescaling here for transparency - and so we know what to add back in later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale():\n",
    "    def __init__(self, y, err):\n",
    "        self.original_data = y\n",
    "        self.original_err = err\n",
    "        self.mean = np.mean(y)\n",
    "        self.std = np.std(y)\n",
    "        self.transform()\n",
    "        return\n",
    "    def transform(self):\n",
    "        self.y = (self.original_data - self.mean) / self.std\n",
    "        self.err = self.original_err / self.std\n",
    "        return()\n",
    "    def invert(self, scaled_y, scaled_err):\n",
    "        return (scaled_y * self.std + self.mean, scaled_err * self.std)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rescaled = Rescale(y, sigmay)\n",
    "print('Mean, variance of original data: ',np.round(np.mean(y)), np.round(np.var(y)))\n",
    "print('Mean, variance of rescaled data: ',np.round(np.mean(rescaled.y)), np.round(np.var(rescaled.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we can undo the scaling, for any `y` and `sigmay`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, sigmay2 = rescaled.invert(rescaled.y, rescaled.err)\n",
    "print('Mean, variance of inverted, rescaled data: ',np.round(np.mean(y2)), np.round(np.var(y2)))\n",
    "print('Maximum differences in y, sigmay, after round trip: ',np.max(np.abs(y2 - y)), np.max(np.abs(sigmay2 - sigmay)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constraining the GP\n",
    "\n",
    "Now, using the same covariance function, lets \"fit\" the GP by constraining each draw from the GP to go through our data points, and _optimizing_ the length scale hyperparameter `h`. \n",
    "\n",
    "Let's first look at how this would work for two data points with no uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our (rescaled) datapoints:\n",
    "x1 = np.array([x[10], x[12]])\n",
    "rescaled_y1 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay1 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, with initial length_scale h=10:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp1 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "# Fit it to our two noiseless datapoints:\n",
    "gp1.fit(x1[:, None], rescaled_y1)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp1.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid1, rescaled_ygrid1_err = gp1.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling, of both y(x) on our grid, and our two constraining data points:\n",
    "ygrid1, ygrid1_err = rescaled.invert(rescaled_ygrid1, rescaled_ygrid1_err)\n",
    "y1, sigmay1 = rescaled.invert(rescaled_y1, rescaled_sigmay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(222)\n",
    "ax.plot(xgrid, ygrid1, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill(np.concatenate([xgrid, xgrid[::-1]]),\n",
    "        np.concatenate([(ygrid1 - ygrid1_err), (ygrid1 + ygrid1_err)[::-1]]),\n",
    "        alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "ax.plot(x1, y1, '.k', ms=6, label='Noiseless constraints')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(0, 399)\n",
    "ax.set_xlabel('$x$')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the absence of information, the GP tends to produce $y(x)$ that fluctuate around the prior mean function, which we chose to be a constant. Let's draw some samples from the posterior PDF, and overlay them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "draws = gp1.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))\n",
    "\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See how the posterior sample $y(x)$ functions all pass through the constrained points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Including Observational Uncertainties\n",
    "\n",
    "The mechanism for including uncertainties is a little esoteric: `scikit-learn` wants to be given a \"nugget,\" called `alpha`, to multiply the diagonal elements of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x2 = np.array([x[10], x[12]])\n",
    "rescaled_y2 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay2 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp2 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay2 / rescaled_y2) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our two noisy datapoints:\n",
    "gp2.fit(x2[:, None], rescaled_y2)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp2.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid2, rescaled_ygrid2_err = gp2.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid2, ygrid2_err = rescaled.invert(rescaled_ygrid2, rescaled_ygrid2_err)\n",
    "y2, sigmay2 = rescaled.invert(rescaled_y2, rescaled_sigmay2)\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp2.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(223)\n",
    "\n",
    "def gp_plot(ax, xx, yy, ee, datax, datay, datae, samples, legend=True):\n",
    "    ax.cla()\n",
    "    ax.plot(xx, yy, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "    ax.fill(np.concatenate([xx, xx[::-1]]),\n",
    "            np.concatenate([(yy - ee), (yy + ee)[::-1]]),\n",
    "            alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "    ax.errorbar(datax, datay, datae, fmt='.k', ms=6, label='Noisy constraints')\n",
    "    ax.set_xlim(0, 399)\n",
    "    ax.set_ylim(0, 399)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y(x)$')\n",
    "    ax.plot(xgrid, samples[:,0], '-r')\n",
    "    ax.plot(xgrid, samples[:,1], '-g')\n",
    "    ax.plot(xgrid, samples[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "    if legend: ax.legend(fontsize=8)\n",
    "    return\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid2, ygrid2_err, x2, y2, sigmay2, draws, legend=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, the posterior sample $y(x)$ functions pass through the constraints _within the errors_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using all the Data\n",
    "\n",
    "Now let's extend the above example to use all of our datapoints. This additional information should pull the predictions further away from the initial mean function. We'll also compute the marginal log likelihood of the best fit hyperparameter, in case we want to compare this choice of kernel with another one (in the exercises, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use all of our datapoints:\n",
    "x3 = x\n",
    "rescaled_y3 = rescaled.y\n",
    "rescaled_sigmay3 = rescaled.err\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "# Could comment this out, and then import and use an \n",
    "# alternative kernel here. \n",
    "\n",
    "gp3 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay3 / rescaled_y3) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our noisy datapoints:\n",
    "gp3.fit(x3[:, None], rescaled_y3)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid3, rescaled_ygrid3_err = gp3.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid3, ygrid3_err = rescaled.invert(rescaled_ygrid3, rescaled_ygrid3_err)\n",
    "y3, sigmay3 = rescaled.invert(rescaled_y3, rescaled_sigmay3)\n",
    "\n",
    "# We have fitted the length scale parameter - print the result here:\n",
    "params = gp3.kernel_.get_params()\n",
    "print('Kernel: {}'.format(gp3.kernel_))\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "print('Marginal log-Likelihood: {:.3f}'.format(gp3.log_marginal_likelihood(gp3.kernel_.theta)))\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp3.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(224)\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid3, ygrid3_err, x3, y3, sigmay3, draws, legend=True)\n",
    "fig\n",
    "\n",
    "# fig.savefig('../../lessons/graphics/mfm_gp_example_pjm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now see the Gaussian Process model providing a smooth interpolation between the points. The posterior samples show fluctuations, but all are plausible under our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try a different kernel function, from the list given in the [scikit-learn docs here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.gaussian_process). \"Matern\" could be a good choice. Do you get a higher value of the marginal log likelihood when you fit this model? Under what circumstances would this marginal log likelihood approximate the Bayesian Evidence well?\n",
    "\n",
    "----\n",
    "\n",
    "2. Extend the analysis above to do a _posterior predictive model check_ of your GP inference. You'll need to generate new replica datasets from posterior draws from the fitted GP. Use the discrepancy measure $T(\\theta,d) = -2 \\log L(\\theta;d)$. Does your GP provide an adequate fit to the data? Could it be over-fitting? What could you do to prevent this? There's some starter code for you below.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Alternative kernel\n",
    "\n",
    "Go back to the gp3 cell, and try something new..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Kernel: RBF\n",
    "Best-fit kernel length scale = \n",
    "Marginal log-Likelihood: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Kernel: ???\n",
    "Best-fit kernel length scale = \n",
    "Marginal log-Likelihood: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Posterior Predictive Model Check\n",
    "\n",
    "For this we need to draw models from our GP, and then generate a dataset from each one. We'll do this in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_replica_data(xgrid, ygrid, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a 30-point data set, with x and sigma_y as standard, but with\n",
    "    y values given by the \"lookup tables\" (gridded function) provided.\n",
    "    \"\"\"\n",
    "    Ndata = 30\n",
    "\n",
    "    xbar = 0.5*(xlimits[0] + xlimits[1])\n",
    "    xstd = 0.25*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "    x = xbar + xstd * np.random.randn(Ndata)\n",
    "\n",
    "    meanerr = 0.025*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    sigmay = meanerr + 0.3 * meanerr * np.abs(np.random.randn(Ndata))\n",
    "\n",
    "    # Look up values of y given input lookup grid\n",
    "    y = np.zeros(Ndata)\n",
    "    for k in range(Ndata):\n",
    "        y[k] = np.interp(x[k], np.ravel(xgrid), ygrid)\n",
    "    # Add noise:\n",
    "    y += sigmay*np.random.randn(len(x))\n",
    "\n",
    "    return x,y,sigmay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrepancy(y_model, y_obs, s_obs):\n",
    "    \"\"\"\n",
    "    Compute discrepancy measure comparing model y and \n",
    "    observed/replica y (with its uncertainty). \n",
    "    \n",
    "    T = -2 log L\n",
    "    \"\"\"\n",
    "    T = REPLACE_WITH_YOUR_SOLUTION()\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 1000 sample models:\n",
    "Nsamples = 1000\n",
    "draws = gp3.sample_y(xgrid, n_samples=Nsamples)\n",
    "x_rep, y_rep, sigmay_rep = np.zeros([30,Nsamples]), np.zeros([30,Nsamples]), np.zeros([30,Nsamples])\n",
    "# Difference in discrepancy measure, for plotting\n",
    "dT = np.zeros(Nsamples)\n",
    "\n",
    "# For each sample model, draw a replica dataset and accumulate test statistics:\n",
    "y_model = np.zeros(30)\n",
    "for k in range(Nsamples):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))\n",
    "    ygrid = draws[:,k]\n",
    "    x_rep[:,k], y_rep[:,k], sigmay_rep[:,k] = generate_replica_data(xgrid, ygrid, seed=None)\n",
    "    dT[k] = REPLACE_WITH_YOUR_SOLUTION()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot P(T[y_rep]-T[y_obs]|y_obs) as a histogram:\n",
    "\n",
    "plt.hist(dT, density=True)\n",
    "plt.xlabel(\"$T(y_{rep})-T(y_{obs})$\")\n",
    "plt.ylabel(\"Posterior predictive probability density\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
