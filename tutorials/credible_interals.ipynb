{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Credible Intervals/Regions\n",
    "\n",
    "Here we'll compare different conventions for defining \"best values\" and credible intervals (CI's) to summarize PDFs:\n",
    "1. Mode and highest posterior density\n",
    "2. Median and quantiles\n",
    "\n",
    "We'll compute these quantities from *samples* of a PDF. In addition, you will compare to the mean and standard deviation, though note that in general, unless your PDF is a Gaussian, those intervals don't encompass the probability they claim to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some concrete test cases to work with. We'll find 1D values/CI's for these, and then turn to the 2D case where `x=test1D_1` and `y=test1D_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1D_1 = st.beta.rvs(1.5, 5.0, size=100000)\n",
    "test1D_2 = np.concatenate((st.norm.rvs(loc=-2.0, size=60000), st.norm.rvs(loc=2.0, size=40000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find intervals corresponding to \"$1\\sigma$\" and \"$2\\sigma$\" equivalent probability. Recall that these are $\\approx$0.683 and 0.954, or more precisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chi2.cdf([1.0, 4.0], df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D intervals\n",
    "\n",
    "Let's start with the 1D best value and CI for `test_1D_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test1D_1, bins=100, density=True, histtype='step');\n",
    "plt.xlabel('test1D_1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll find the mode and highest posterior density CI's using the only package we know of that does the latter correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this package, there are actually two steps. The first converts the samples into a histogram, optionally doing some smoothing. After that, there is a second call to find the mode and CI's. This is so that the second function can also be used in cases where we have, e.g., direct evaluations of the posterior on a grid (we would just have to coerce those into the same format as the histogram that gets made from samples).\n",
    "\n",
    "A simple call of the first function, `whist`, produces the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = cr.whist(test1D_1, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, by default, this does some smoothing, specifically using a kernel density estimate method. This is why the blue curve (which is what is actually returned), is smoother than the raw histogram shown in the background. Note that the mode of the blue curve is not identical to the $x$ value where the the raw histogram is highest. This is certainly correct in this case, and it's often true that the mode of an unsmoothed histogram will be thrown off by monte carlo noise. In general, one should smooth until noisey features that you're pretty sure shouldn't be there are gone, and no more - assuming you're confident enough to make that call.\n",
    "\n",
    "There may be times when you want to not smooth, or to have finer control over the smoothing (e.g., if you believe the identified mode is still off due to remaining noise) - see the docstring for more and/or play around yourself.\n",
    "\n",
    "\n",
    "For reference, take a look at the returned object, a dictionary that straightforwardly holds the abscissae and estimated density of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, assuming this looks reasonable, we feed the output of `whist` into `whist_ci`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci1 = cr.whist_ci(h1, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look familiar from the notes. The plot shows the mode and the 1 and 2 sigma intervals, whose bounding probabilities are indicated by the horizontal lines. Here you should look at the intersection of the blue/solid and green/dashed lines with the black/solid PDF - if they aren't all coming together in the same place, then the underlying histogram needs to be binned more finely.\n",
    "\n",
    "The return value contains a number of useful (and some redundant) values. Of most interest are the `mode` and interval `min` and `max` values. `low` and `high` are just the distance of `min` and `max` from the mode. Finally, `center` is the average of `min` and `max`, and `width` is half the difference between `min` and `max`. These are convenient for cases where the CI is symmetric about the mode (at the precision we would be reporting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text, we would normally report the constraint as $\\mathrm{mode}^{+\\mathrm{high}}_{-\\mathrm{low}}$, or $\\mathrm{mode}\\pm\\mathrm{width}$ if the CI is symmetric.\n",
    "\n",
    "Next, compute the alternative \"best fit\" and CI's using the median/quantiles method from the notes. Store the median as a scalar and the intervals as shape (2,) arrays holding the endpoints of each interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# med1 = \n",
    "# quant1_1 = \n",
    "# quant1_2 = \n",
    "\n",
    "print(med1, quant1_1, quant1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though their use is not recommended, we'll also compare to the mean/standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea1 = np.mean(test1D_1)\n",
    "std1 = np.std(test1D_1)\n",
    "print(mea1, std1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll show the median/quantile summary (green) and the mean/standard deviation summary (red) over the plot from earlier. You can see that none of them quite agree, and the mean/standard deviation $2\\sigma$ interval includes negative values where the PDF is zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci1 = cr.whist_ci(h1, plot=plt, plot_levels=False)\n",
    "plt.plot(quant1_2, [2.,2.], 'g')\n",
    "plt.plot(quant1_1, [2.05,2.05], 'g')\n",
    "plt.plot(med1, 2.05, 'go')\n",
    "plt.plot(mea1, 2.55, 'ro')\n",
    "plt.plot([mea1-std1, mea1+std1], [2.55,2.55], 'r')\n",
    "plt.plot([mea1-2*std1, mea1+2*std1], [2.5,2.5], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the same procedure for `test1D_2` next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = cr.whist(test1D_2, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# med2 = \n",
    "# quant2_1 = \n",
    "# quant2_2 = \n",
    "\n",
    "print(med2, quant2_1, quant2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea2 = np.mean(test1D_2)\n",
    "std2 = np.std(test1D_2)\n",
    "print(mea2, std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci2 = cr.whist_ci(h2, plot=plt, plot_levels=False)\n",
    "plt.plot(quant2_2, [0.2,0.2], 'g')\n",
    "plt.plot(quant2_1, [0.205,0.205], 'g')\n",
    "plt.plot(med2, 0.205, 'go')\n",
    "plt.plot(mea2, 0.155, 'ro')\n",
    "plt.plot([mea2-std2, mea2+std2], [0.155,0.155], 'r')\n",
    "plt.plot([mea2-2*std2, mea2+2*std2], [0.15,0.15], 'r')\n",
    "ci2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... this is an example of a multimodal PDF where the $1\\sigma$ highest-density CI is multiply connected (i.e. not contiguous). Note that `ci2` contains _two_ entries for `level` 0.683 to reflect this. The corresponding values in `prob` show the integrated probability in each of them, if that's useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D regions\n",
    "\n",
    "Next we have the case of 2D credible regions (CR's), where we'll take `test1D_1` and `test1D_2` to be samples of two parameters from the same model.\n",
    "\n",
    "To find the HPD regions, we have analogous functions to the ones used above. However, `whist2d` does not have a KDE smoothing option, so in general you will want to manually adjust the number of bins (in each parameter) and the size of the smoothing kernel (in units of bins). For example, compare the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = cr.whist2d(test1D_1, test1D_2, bins=25, smooth=0, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = cr.whist2d(test1D_1, test1D_2, bins=50, smooth=1, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value of this function is a dictionary with `x`, `y` and `z` entries, corresponding to the two parameter values at the bin centers, and the density in each bin. These have dimensionality 1, 1, and 2, respectively.\n",
    "\n",
    "To find the CR's, we call `whist2d_ci`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci3 = cr.whist2d_ci(h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the mode and the list of points making up the contours in the above plot in a slightly complicated structure that you can examine if you're interested.\n",
    "\n",
    "One should test whether the amount of smoothing used is too much. (This is important in the 1D case also, but the KDE smoothing rarely has this issue.) Overlaying smoothed and less smoothed contours is a useful way of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = cr.whist2d(test1D_1, test1D_2, bins=100, smooth=0);\n",
    "ci3 = cr.whist2d_ci(h3, mode_fmt=None, contour_color='r');\n",
    "h3 = cr.whist2d(test1D_1, test1D_2, bins=50, smooth=1);\n",
    "ci3 = cr.whist2d_ci(h3, mode_fmt=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comparison, the more smoothed contours should look like... a smoothed version of the squiggly ones, as opposed to being much broader.\n",
    "\n",
    "Note that the function that extracts the contours in `whist2d_ci` (distinct from the one that _plots_ them) is not foolproof - it seems especially likely to screw up if there are too many complicated squiggles. So it is (a) extra-important to do this check, and (b) best to use `whist2d` for the check rather than the function (below) that simply plots contours once you've found them. Once you know what the contours are supposed to look like, you can make sure nothing pathalogical has happened when they get re-plotted (as below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no 2D analog of the median/quantile convention for CI's, but we can compare with the equivalent (not recommended) regions defined via the parameter means and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cr.ci2D_plot(ci3['contours'][1], plt) # this is how you plot contours from whist2d_ci without having to\n",
    "cr.ci2D_plot(ci3['contours'][0], plt) # repeat all the computations\n",
    "\n",
    "cv = np.cov(test1D_1, test1D_2)\n",
    "cr.cov_ellipse(cv, center=[mea1,mea2], level=0.68268949, plot=plt, fmt='r--');\n",
    "cr.cov_ellipse(cv, center=[mea1,mea2], level=0.95449974, plot=plt, fmt='r--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stand-alone contour plotting function adds a single contour level at a time, so it was called twice.\n",
    "\n",
    "Options to the various lower-level `matplotlib` functions that are called can be used to customize things, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.ci2D_plot(ci3['contours'][1], plt, fill=True, fill_kwargs={'color':str(0.954)})\n",
    "cr.ci2D_plot(ci3['contours'][0], plt, fill=True, fill_kwargs={'color':str(0.683)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increasingly common way to visualize multiple 1D and 2D marginalized PDFs from a high-dimensional posterior distribution is the so-called \"triangle plot\" that has the 1D PDFs on its diagonal and each corresponding 2D PDF off the diagonal, in a triangular matrix of plots. For example, this call finds all of the CI's and CR's (note that it has to pop up a contour plot due to the way `whist2d_ci` works internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = cr.whist_triangle(np.array([test1D_1, test1D_2]).T, bins=50, smooth2D=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the triangle thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cr.whist_triangle_plot(tri, paramNames=[r'$x$', r'$y$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lots of options can be customized, and other things can be added to any of the panels via the returned array of `matplotlib` axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, a = cr.whist_triangle_plot(tri, paramNames=[r'$x$', r'$y$'], linecolor1D='b',\n",
    "                              fillcolors2D=[(0.026, 0.818, 1.000), (0.714, 0.936, 1.000)]);\n",
    "cr.cov_ellipse(cv, center=[mea1,mea2], level=0.68268949, plot=a[1][0], fmt='r--');\n",
    "cr.cov_ellipse(cv, center=[mea1,mea2], level=0.95449974, plot=a[1][0], fmt='r--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are other packages that can be better for getting a quick look at plots like this, in particular `corner` and `pygtc`. We'll use both of these in later tutorials. They have the advantage that you get the whole triangle plot from a single function call. The downside is that all the calculations are redone each time you call the function, and it isn't so straightforward to do the sanity checks mentioned above. So we recommend using one of those packages only for quickly visualizing samples from a PDF. In contrast, with `incredible` it's straightforward to do the hard calculations once, and then tweak the display to your heart's content, so it might be more suitable for publication graphics (in addition to providing the CI calculations). To summarize the options as of this writing:\n",
    "* `corner`: probably the quickest way to create a triangle plot showing the key information (note that the default 2D probability levels are not the conventional ones)\n",
    "* `pygtc`: looks nicer than `corner`; can put multiple posteriors on the same plot\n",
    "* `incredible`: provides fine-tuned control over CR calculations, and ability to save results rather than just a plot, but needs some hand holding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
