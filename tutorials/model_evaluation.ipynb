{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Model Evaluation\n",
    "\n",
    "We've previously done simple, visual comparisons of fitted models with data, usually on the form \"does this model curve go through the data points' error bars?\" While visual checking is a powerful tool, that particular formulation only really makes sense when the sampling distribution is Gaussian (so that the \"error bar\" can equally well be associated with the data as with the model).\n",
    "\n",
    "More generally, we can say that a model (including specific parameter values) fits the observed data well if it could plausibly have generated the data. That is, in data space, does the observed data lie in a region where the sampling distribution is relatively large? Because the dimensionality of the data space tends to be very large, this is usually not a simple thing to visualize. However, we can simplify by asking the same question about a low-dimensional summary statistic of the data - some quantity or quantities that can be computed from the data.\n",
    "\n",
    "In classical statistics, where we do assume a Gaussian sampling distribution, the usual summary to look at is the $\\chi^2$ statistic. We ask: is the $\\chi^2$ (sum of squared residuals) after fitting our model to the real data consistent with the distribution of $\\chi^2$ values we would expect from many data sets randomly generated from the best fit model?\n",
    "\n",
    "In Bayesian analysis, we can follow a similar chain of logic, with two differences:\n",
    "1. We are free to use any summary statistic we want. In particular, we could choose something that reflects the feature(s) of the data we most care about the model describing well. Alternatively, a common choice is to use $-2$ times the maximum log posterior (or log likelihood) for a given (mock) data set.\n",
    "2. It makes sense to marginalize over the posterior distribution of the model parameters, rather than looking only at predictions from the best-fitting model.\n",
    "\n",
    "Below, we'll introduce a simple, simulated (though realistic) data set and work through this procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "Our data is just a list of numbers. Each one represents a measured distance, $y$, between two different estimates of the center of a galaxy cluster: the location of the presumed central galaxy and a centroid of the diffuse, emissive gas. The context here is that automated algorithms sometimes fail to chose the central galaxy correctly (because of image artifacts or other problems), whereas the gas centroid is more reliable but also more expensive to measure. Therefore, we'd like to use this data set to _characterize the distribution of mis-centerings_ so that the galaxy-based centers can be used for large sample, with the resulting errors propagated forward through future processing, e.g., weak lensing estimates.\n",
    "\n",
    "Let's load up the data and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import emcee\n",
    "import dill\n",
    "import incredible as cr\n",
    "from pygtc import plotGTC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into global variable `y`. Each entry is an offset in units of kpc. This particular data set is fictional, but intentionally similar to a real one that has actually been used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([39.30917,35.13419,5.417072,59.75137,30.69077,14.45971,27.07368,27.48429,80.60219,483.1432,24.65057,\n",
    "              22.36524,43.39081,39.89816,30.67409,6.905061,53.69709,9.504133,41.07874,10.9369,48.29861,61.34125,\n",
    "              68.37279,30.51124,26.74462,13.7165,6.043301,976.1495,27.20097,7.818419,5.589193,3.310114,271.8901,\n",
    "              126.0384,99.51247,249.1279,403.0484,3.071718,0.9434036,54.94336,1.529382,8.441071,19.59434,59.43049,\n",
    "              77.21293,29.6533,286.7116,11.2386,9.511912,29.04711,33.77766,151.4803,223.3557,12.33816,25.22682,\n",
    "              26.86597,339.7084,405.6737,3.809868,221.6523,307.2994,73.36697,42.15523,36.74785,5.415392,69.4721,\n",
    "              136.8073,17.3534,4.135966,20.19435,79.06968,8.095599,4.474533,44.90669,85.891,1.636425,75.39335,\n",
    "              15.94149,2.828709,20.5636,41.52905,42.51133,104.3908,67.41335,13.80204,394.9841,33.90415,84.78714,\n",
    "              36.77924,14.48424,66.01276,2.910331,92.79938,29.74337,42.40971,1.692674,1.039994,120.5902,154.7106,\n",
    "              14.38967,147.8399,166.5054,87.53685,22.63141,638.1976,273.6167,593.4997,45.57279,87.30421,75.03385,\n",
    "              18.33932,36.05779,3.659462,263.9074,0.2432062,8.499095,1.160031,38.16615,41.65371,361.5,148.9294,\n",
    "              10.25777,71.29159,10.02279,16.36062,601.1667,4.960311,12.22526,87.54137,48.48371,78.56777,212.8153,\n",
    "              77.0353,62.7624,81.26739,34.36881,42.63432,264.4551,15.24863,25.94133,35.88882,34.94669,222.5425,\n",
    "              304.9676,19.68377,7.216153,17.61534,32.25887,14.08842,773.5914])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a quick histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "bins = np.linspace(0,1000,20)\n",
    "plt.hist(y, bins=bins, color=\"skyblue\");\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14);\n",
    "plt.ylabel(\"Frequency\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choosing a Test Statistic\n",
    "\n",
    "The hypothesis we will test in this tutorial is the model outlined in the next section - but how well that model fits the data is a question we will answer in part using a _test statistic._ \n",
    "\n",
    "In this case, a feature we especially want to capture with our model is the fraction of clusters where $y$ is small (in the context of a galaxy cluster) vs not small. For concreteness, I suggest making $T$ the number of clusters where $y<100$ kpc; the provided solutions that you can check your work against will use this statistic. However, you are encouraged to play around with other possibilities to see how the simple model below succeeds or fails to reproduce the corresponding features of the data.\n",
    "\n",
    "Note that the test statistic I chose is a function of the data only, although in general it's also possible to use statistics that are functions of both the data and model (the best-fitting log posterior being an example of this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(yy):\n",
    "    \"\"\"\n",
    "    Argument: a data vector (either the real data or a simulated data set)\n",
    "    Returns: a scalar test statistic computed from the argument\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the test statistic of the real data to verify that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_true = T(y)\n",
    "print(\"The test statistic of the real data is T =\", T_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check:** The recommended test statistic evaluates to 118 on the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Computational Framework\n",
    "\n",
    "Once we define a model to work with (below), we'll want to fit that model to the data, and then evaluate it using the methods we saw in the [notes](../notes/model_evaluation.ipynb). These include:\n",
    "\n",
    "* a visual check using replica datasets drawn from the posterior predictive distribution\n",
    "* a quantitative posterior predictive model check using a suitable test statistic $T(y)$\n",
    "\n",
    "In the next notebook, we'll choose and fit a second, alternative model, and compare the two in terms of\n",
    "\n",
    "* the Deviance Information Criterion (DIC), to assess the models' (relative) generalized predictive accuracy\n",
    "* the Bayesian Evidence, to provide insight on the (relative) probability of each model given the data\n",
    "\n",
    "Notice that each of these bulleted operations can be coded as a _function of the model_, $H$ (e.g. a visual check of the model, the evidence of the model, and so on). We will therefore be fancy and write a _class_ that completely describes the model, and also contains a set of _functions that act on model._ Since we anticipate looking at multiple models, we'll use _inheritance_. While this level of object oriented programming may not be familiar, it's a good thing to learn, and most of the details are filled in for you below.\n",
    "\n",
    "We start by defining a _base class_, which contains the functionality common to any model we care to define later. To make it clear what functionality we expect _derived classes_ to provide, we'll include defintions of non-functional methods that the derived classes will need to _override_. (This is completely unnecessary in Python, but has the benefit of providing a complete list of functions we'll eventually want to fill in.)\n",
    "\n",
    "You should make sure you understand the workings of even the functions that are completely given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is something we can throw to discourage direct instantiation of the base class\n",
    "class VirtualClassError(Exception):\n",
    "    def __init__(self):\n",
    "        Exception.__init__(self,\"Do not directly instantiate the base Model class!\")\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Base class for inference and model evaluation in a simple cluster mis-centering analysis.\n",
    "    In all these functions, `params' is a dictionary of model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Note: derived classes should have their own __init__ function which ends by calling this one\n",
    "        \"\"\"\n",
    "        # Storage for MCMC samples from fitting the model\n",
    "        self.samples = None\n",
    "        self.Nsamples = 0\n",
    "        # Lines below are meant as a template for derived classes, do not uncomment for the base class\n",
    "        #\n",
    "        # A name for the model!\n",
    "        #self.name = '...'\n",
    "        #\n",
    "        # As part of the model definition, store an ordered list of parameter names (for dictionary keys)\n",
    "        # Named arguments of other functions that correspond to parameters should appear in this order\n",
    "        #self.param_names = [...]\n",
    "        #\n",
    "        # parameter names as display text (for plot labels),\n",
    "        #self.param_labels = [...]\n",
    "        #\n",
    "        # and a dictionary of priors (keys must match param_names)\n",
    "        #self.priors = {...}\n",
    "  \n",
    "    def log_prior(self, **params):\n",
    "        \"\"\"\n",
    "        Return the log prior PDF p(params|H)\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "\n",
    "    def draw_samples_from_prior(self, N):\n",
    "        \"\"\"\n",
    "        Return N samples from the prior PDF p(params|H) as a list of dictionaries\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "\n",
    "    def log_likelihood(self, **params):\n",
    "        \"\"\"\n",
    "        Return the log of the likelihood function L(params) = p(y|params,H)\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "    \n",
    "    def sampling_distribution(self, yy, **params):\n",
    "        \"\"\"\n",
    "        Return the sampling distribution p(yy|params,H) at a point in data space yy given parameter(s) args\n",
    "        We expect a vector input yy, and return the corresponding probabilities.\n",
    "            \n",
    "        Note: This is useful for making plots of \"the model\" overlaid on the histogram of the data\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes \n",
    "        \n",
    "    def generate_replica_dataset(self, N, **params):\n",
    "        \"\"\"\n",
    "        Return a replica dataset y_rep from the sampling distribution p(y_rep|args,H).\n",
    "        (Length N should match the real data)\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "        \n",
    "    def log_posterior(self, parameterlist=None, **params):\n",
    "        \"\"\"\n",
    "        Return the log of the (unnormalized) posterior PDF p(params|y,H)\n",
    "        \n",
    "        The parameterlist argument is there for compatibility with emcee.\n",
    "        This will be an infinite loop if you named one of your parameters \"parameterlist\", but in that case\n",
    "        you deserve it.\n",
    "        This also means that for even a simple call, we would need to specify the parameter name(s),\n",
    "         i.e. not log_posterior(pval), but log_posterior(param=pval). This doesn't seem unreasonable.\n",
    "        \"\"\"\n",
    "        if parameterlist is not None:\n",
    "            pdict = {k:parameterlist[i] for i,k in enumerate(self.param_names)}\n",
    "            return self.log_posterior(**pdict)\n",
    "        lnp = self.log_prior(**params)\n",
    "        if lnp != -np.inf:\n",
    "            lnp += self.log_likelihood(**params)\n",
    "        return lnp\n",
    "\n",
    "    def draw_samples_from_posterior(self, starting_params, nsteps, nwalkers=8, threads=1):\n",
    "        \"\"\"\n",
    "        Use emcee to draw samples from P(params|y,H). Hopefully it Just Works.\n",
    "        You could try using e.g. threads=4 to speed things up with multiprocessing.\n",
    "        \"\"\"\n",
    "      \n",
    "        # The density to sample is this model's own posterior PDF\n",
    "        npars = len(starting_params)        \n",
    "        self.sampler = emcee.EnsembleSampler(nwalkers, npars, self.log_posterior, threads=threads) \n",
    "        \n",
    "        # Generate an ensemble of walkers within +/-1% of the guess:\n",
    "        theta_0 = np.array([starting_params*(1.0 + 0.01*np.random.randn(npars)) for j in range(nwalkers)])\n",
    "        # Note that the initial parameter array theta_0 should have dimensions nwalkers × npars\n",
    "        \n",
    "        # Evolve the ensemble:\n",
    "        self.sampler.run_mcmc(theta_0, nsteps)\n",
    "        \n",
    "        # Plot the raw samples:\n",
    "        plt.rcParams['figure.figsize'] = (12.0, 4.0*npars)\n",
    "        fig, ax = plt.subplots(npars, 1);\n",
    "        cr.plot_traces(self.sampler.chain[:min(8,nwalkers),:,:], ax, labels=self.param_labels);\n",
    "\n",
    "    def check_chains(self, burn, maxlag):\n",
    "        '''\n",
    "        Ignoring `burn` samples from the front of each chain, compute convergence criteria and\n",
    "        effective number of samples.\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        tmp_samples = [self.sampler.chain[i,burn:,:] for i in range(nwalk)]\n",
    "        print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "        print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))\n",
    "        print('NB: Since walkers are not independent, these will be optimistic!')\n",
    "\n",
    "    def remove_burnin(self, burn):\n",
    "        '''\n",
    "        Remove `burn` samples from the front of each chain, and concatenate\n",
    "        Plot, and store the result in self.samples\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        self.samples = self.sampler.chain[:,burn:,:].reshape(nwalk*(nsteps-burn), npars)\n",
    "        self.Nsamples = self.samples.shape[0]\n",
    "        plt.rcParams['figure.figsize'] = (12.0, 4.0*npars)\n",
    "        fig, ax = plt.subplots(npars, 1);\n",
    "        cr.plot_traces(self.samples, ax, labels=self.param_labels);\n",
    "        \n",
    "    def posterior_mean(self):\n",
    "        '''\n",
    "        Helper function for computing the posterior mean of each parameter (from MCMC samples)\n",
    "        '''\n",
    "        m = np.mean(self.samples, axis=0)\n",
    "        return {k:m[i] for i,k in enumerate(self.param_names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating a Simple Model\n",
    "\n",
    "First, let's assume the simple model $H_1$, that the sampling distribution is an exponential:\n",
    "\n",
    "$p(\\{y_i\\}|a, H_1) = \\prod_i \\frac{1}{a}e^{-y_i/a}$; $y_i\\geq0$\n",
    "\n",
    "Our single parameter is $a$, the mean of the exponential distribution.\n",
    "\n",
    "Note that this is the entirety of the sampling distribution. Don't be scared of the simplicity! Our model is that the data are realizations of some distribution, and we want (initially) to find the parameter(s) of that distribution. There's nothing tricky (but also nothing Gaussian) about this.\n",
    "\n",
    "### 2a. Implementation in code\n",
    "\n",
    "Complete the implementation of this model as a derived class of `Model`, below. Note than an `ExponentialModel` object still has all the methods defined for `Model`, in particular the ones that we _don't_ need to redefine here.\n",
    "\n",
    "Note that this includes choosing a reasonable prior for $a$. It should be a proper (normalizable) distribution. We don't want to deal with improper distributions when calculating the evidence later on. _To compare with the self-check below, use a wide, uniform (but bounded!) prior_.\n",
    "\n",
    "Again, even when a function is completely give, you should make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialModel(Model):\n",
    "    \"\"\"\n",
    "    Simple exponential model for mis-centering.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # As part of the model definition, store an ordered list of parameter names (for dictionary keys)\n",
    "        # A name for the model!\n",
    "        self.name = 'Exponential Model'\n",
    "        # Named arguments of other functions that correspond to parameters should appear in this order\n",
    "        self.param_names = ['a']\n",
    "        # parameter names as display text (for plot labels),\n",
    "        self.param_labels = [r'$a$']\n",
    "        # and a dictionary of priors (keys must match param_names)\n",
    "        self.priors = TBC() # {'a':\n",
    "        # The next line finishes initialization by calling the parent class' __init__\n",
    "        Model.__init__(self)\n",
    "  \n",
    "    def log_prior(self, a):\n",
    "            \"\"\"\n",
    "            Return the log prior PDF P(a1|H)\n",
    "            \"\"\"\n",
    "            TBC()\n",
    "\n",
    "    def draw_samples_from_prior(self, N):\n",
    "        \"\"\"\n",
    "        Return N samples of a from the prior PDF p(a|H) as a 2D array\n",
    "        (one column per parameter)\n",
    "        \"\"\"\n",
    "        TBC()\n",
    "\n",
    "    def log_likelihood(self, a):\n",
    "        \"\"\"\n",
    "        Evaluate the log of the likelihood function L(a) = p(y|a,H)\n",
    "        \"\"\"\n",
    "        return np.sum(st.expon.logpdf(y, scale=a))\n",
    "    \n",
    "    def sampling_distribution(self, yy, a):\n",
    "        \"\"\"\n",
    "        Evaluate the sampling distribution PDF p(yy|a,H) at a point in data space yy given parameter value a\n",
    "        We expect a vector input yy, and return the corresponding probabilities.\n",
    "            \n",
    "        Note: This is useful for making plots of \"the model\" overlaid on the histogram of the data\n",
    "        \"\"\"\n",
    "        return st.expon.pdf(yy, scale=a)\n",
    "        \n",
    "    def generate_replica_dataset(self, N, a):\n",
    "        \"\"\"\n",
    "        Return a replica data set y_rep from the sampling distribution p(y_rep|a,H).\n",
    "        (Length N should match the real data)\n",
    "        \"\"\"\n",
    "        TBC()\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try instantiating a model and drawing a dozen samples from its prior as a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1 = ExponentialModel()\n",
    "print(\"12 sample values drawn from the prior of Model1: \", Model1.draw_samples_from_prior(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the log-posterior function to make sure it's not obviously buggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [1.0, 10.0, 100.0, -3.14]:\n",
    "    print('Log-posterior for a=', a, ' = ', Model1.log_posterior(a=a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the mock-data producing function (with an arbitrary $a$ value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.hist(Model1.generate_replica_dataset(len(y), 500.), bins=bins, color=\"lightgray\");\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14);\n",
    "plt.ylabel(\"Frequency (replica)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test the sampling distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(bins, Model1.sampling_distribution(bins, 500.));\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14);\n",
    "plt.ylabel(\"$p(y|a)$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Fit the model to the data\n",
    "\n",
    "The `draw_samples_from_posterior` method carries out a parameter inference with `emcee` and displays its Markov chains. `remove_burnin` removes burn-in and concatenates the chains. Since these step aren't really the point of this problem, the code is given to you, but you'll still need to experiment with the keyword argument inputs (and read the code to see what they do) in order to get good results. The suggestions in the cell below are pretty terrible.\n",
    "\n",
    "The concatenated MCMC samples are stored in the `Model.samples` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "TBC()\n",
    "# This line will execute out of the box, but will not work well. The arguments should be fiddled with.\n",
    "# Model1.draw_samples_from_posterior(starting_params=[1000.0], nsteps=10, nwalkers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will compute the usual convergence criteria and effective number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # Model1.check_chains( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that looks ok, remove the burn-in for good, and plot the concatenation of what's left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # Model1.remove_burnin( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful for later to know the mean of the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Posterior mean value of a = \", Model1.posterior_mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check:** With wide, uniform priors, you should find a posterior mean $a$ of around 96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Visually compare the posterior predictions with the data.\n",
    "\n",
    "First, let's just plot the posterior-mean model over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "\n",
    "# First the histogram of observed data, as backdrop:\n",
    "plt.hist(y, bins=bins, color=\"skyblue\", density=True, label=\"Observed\")\n",
    "\n",
    "# Now overlay a curve following the sampling distribution conditioned on the posterior mean value of a1: \n",
    "pp = Model1.sampling_distribution(bins, **Model1.posterior_mean())\n",
    "plt.plot(bins, pp, linestyle=\"dashed\", color=\"red\", label=\"Posterior mean model\")\n",
    "\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14)\n",
    "plt.ylabel(\"Normalized Frequency\", fontsize=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of plot should be familiar: it's often a good idea to evaluate model adequacy in data space. You should already be able to see telling differences between the a well-fitting model's sampling distribution, and the data histogram.\n",
    "\n",
    "Now, let's compare a random predicted (\"replica\") data set, drawn from the _posterior predictive distribution,_ with the data. To do this we first draw a parameter value from the posterior PDF, and then generate a dataset from the sampling distribution conditioned on that value. The result is a sample from $p(y_\\mathrm{rep}|y,H_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "\n",
    "# First the histogram of observed data, as backdrop:\n",
    "plt.hist(y, bins=bins, color=\"skyblue\", density=True, label=\"Observed\");\n",
    "\n",
    "# Choose a posterior sample at random and generate a replica dataset, and show its histogram\n",
    "j = np.random.randint(0, Model1.Nsamples)\n",
    "mock = Model1.generate_replica_dataset(len(y), *Model1.samples[j,:])\n",
    "plt.hist(mock, bins=bins, alpha=1.0, histtype=\"step\", color=\"red\", density=True, label=\"Sample posterior prediction\")\n",
    "\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14)\n",
    "plt.ylabel(\"Normalized Frequency\", fontsize=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is nice because it is comparing apples with apples: do mock datasets drawn from our model sampling distribution with any plausible parameter value \"look like\" the real data?\n",
    "\n",
    "To best evaluate this, we want to visualize the posterior predictive distribution of replica datasets. We can do this by plotting many replica datasets drawn from the posterior predictive PDF, e.g. _one for each of our posterior samples_. Let's put this plot in a function, so we can re-use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_check(axes, Model, Nreps=None):\n",
    "    # First the histogram of observed data, as backdrop:\n",
    "    axes.hist(y, bins=bins, color=\"skyblue\", density=True, label=\"Observed\");\n",
    "\n",
    "    # Compute the posterior mean parameters\n",
    "    pm = Model.posterior_mean()\n",
    "\n",
    "    # Make a large number of replica datasets, and overlay histograms of them all \n",
    "    if Nreps is None: Nreps = len(Model.samples)\n",
    "    alpha = 5.0 / Nreps\n",
    "    for jj in np.round(np.linspace(0, len(Model.samples), num=Nreps, endpoint=False)): \n",
    "        j = int(jj)\n",
    "        if j==0:  \n",
    "            # Plot a dataset drawn from the posterior mean, to give a good legend\n",
    "            mock = Model.generate_replica_dataset(len(y), **pm)\n",
    "            axes.hist(mock, bins=bins, histtype=\"step\", alpha=1.0, color=\"red\", density=True,\n",
    "                     label=\"Sample posterior predictions\");\n",
    "        else:     \n",
    "            # Take the next posterior sample a and generate a replica dataset\n",
    "            mock = Model.generate_replica_dataset(len(y), *Model.samples[j,:])\n",
    "            axes.hist(mock, bins=bins, histtype=\"step\", alpha=alpha, color=\"red\", density=True);\n",
    "\n",
    "    # Include the posterior mean model for comparison\n",
    "    pp = Model.sampling_distribution(bins, **pm)\n",
    "    axes.plot(bins, pp, linestyle=\"dashed\", color=\"red\", label=\"Posterior mean model\");\n",
    "\n",
    "    axes.set_title(Model.name, fontsize=14)\n",
    "    axes.set_xlabel(\"Measured distance $y$\", fontsize=14)\n",
    "    axes.set_ylabel(\"Normalized Frequency\", fontsize=14)\n",
    "    axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "fig, ax = plt.subplots(1)\n",
    "visual_check(ax, Model1, Nreps=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these visual checks, would you say the model does a good job of predicting the observed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: Your thoughts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Quantitative posterior predictive model check\n",
    "\n",
    "Now let's quantify the (in)adequacy of the fit with a quantitative _posterior predictive model check_, based on the `test_statistic` function you've already defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample the posterior predictive distribution of test statistics $p[T(y_\\mathrm{rep})|y,H]$, we need to generate replica datasets from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_of_T(Model):\n",
    "    \"\"\"\n",
    "    Compute T(yrep) for each yrep drawn from the posterior predictive distribution,\n",
    "    using parameter samples stored in Model.\n",
    "    \"\"\"\n",
    "    return np.array([T(Model.generate_replica_dataset(len(y), *p)) for p in Model.samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do the following:\n",
    "* plot a histogram of $T(\\mathrm{mock~data})$\n",
    "* compare that distribution with $T(\\mathrm{real~data})$\n",
    "* compute and report the p-value for $T(\\mathrm{real~data})$\n",
    "\n",
    "And we want all of that in packaged in functions of the model, so that we can re-use it later (on different models!). \n",
    "\n",
    "First write a function to estimate the p-value, $P(T > T(y)|y,H)$, based on posterior predictions from the chains you ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvalue(Model):\n",
    "    \"\"\"\n",
    "    Compute the posterior predictive p-value, P(T > T(y)|y,H):\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that plots the distribution of T, and reports the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive_check(Model, nbins=25):\n",
    "    \"\"\"\n",
    "    Compute the posterior predictive distribution of the test statistic T(y_rep), and compare with T(y_obs) \n",
    "    \"\"\"\n",
    "    # Compute distribution of T(yrep):\n",
    "    TT = distribution_of_T(Model)\n",
    "    \n",
    "    # Plot:\n",
    "    plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "    plt.hist(TT, bins=nbins, histtype=\"step\", color=\"red\", label=\"$P(T(y_{\\\\rm rep})|y)$\")\n",
    "    \n",
    "    # Overlay T(y_obs):\n",
    "    plt.axvline(x=T(y), color=\"gray\", linestyle=\"dashed\", label=\"$T(y_{\\\\rm observed})$\")\n",
    "    plt.xlabel(\"Test statistic T(y)\", fontsize=14)\n",
    "    plt.ylabel(\"Posterior predictive probability density\", fontsize=14)\n",
    "    plt.legend();\n",
    "    \n",
    "    # Compute p-value:\n",
    "    p = pvalue(Model)\n",
    "    print(\"p-value =\", p)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = posterior_predictive_check(Model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this result agree with your visual evaluation of the model fitness from the last section? If not, perhaps the test statistic you chose doesn't reflect the agreement you're looking for when inspecting the posterior predictions. If you'd like to re-define your test statistic, do so now and repeat this check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: Your thoughts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check:** If you implemented the test statistic and priors recommended above, your p-value should be about 0.001. The precise value will vary because of all the randomization involved in estimating it, and the fact that we are out in the tail of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating a more complex model\n",
    "\n",
    "Next, we'll repeat all of that (it should be much easier this time!) with a second model. The functional form is up to you, but to make things interesting, choose a model with at least two parameters. Since this is a PDF over distances, it should be defined over the non-negative real line. The test statistic will, of course, have to be the same as before.\n",
    "\n",
    "Put some thought into the priors for this model. They will be important when we come to comparing the two models quantitatively in the next notebook.\n",
    "\n",
    "Copy/paste your implementation of `ExponentialModel` and edit it to implement some other model (give the class a different name, naturally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# class someOtherModel(Model): ...\n",
    "\n",
    "## Instantiate it:\n",
    "# Model2 = someOtherModel() # replace with the actual name of your new class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the prior sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pars in Model2.draw_samples_from_prior(12):\n",
    "    print(pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the posterior function on a couple of these prior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pars in Model2.draw_samples_from_prior(5):\n",
    "    print('Log-posterior for', pars, '=', Model2.log_posterior(**pars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the mock data generator. Here and in the following test, you can replace the call to `draw_samples_from_prior` with specific parameter values if your prior is so wide that you're unlikely to see anything on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.hist(Model2.generate_replica_dataset(len(y), **Model2.draw_samples_from_prior(1)[0]), bins=bins, color=\"lightgray\");\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14);\n",
    "plt.ylabel(\"Frequency (replica)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(bins, Model2.sampling_distribution(bins, **Model2.draw_samples_from_prior(1)[0]));\n",
    "plt.xlabel(\"Measured distance $y$\", fontsize=14);\n",
    "plt.ylabel(\"$p(y|a)$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "TBC() # Model2.draw_samples_from_posterior( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # Model2.check_chains( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # Model2.remove_burnin( ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple parameters, look at their covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(Model2.samples, paramNames=Model2.param_labels, figureSize=6,\n",
    "        customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the posterior mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Posterior mean parameters = \", Model2.posterior_mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually compare posterior predictions with the data, alongside those of the exponential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14.0, 5.0)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "visual_check(ax[0], Model1, Nreps=100);\n",
    "visual_check(ax[1], Model2, Nreps=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = posterior_predictive_check(Model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your model appear to reproduce the data better than the exponential one? Does it have a more compatible p-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: Your thoughts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So is it a better model, in the sense of being justified by the data? To answer that, we need to keep in mind that a more flexible model (i.e. with more free parameters) will usually be able to fit the data more closely, even if it's only reproducing the particular realization of noise present in our specific data set. In other words, directly comparing the maximum likelihood achieved, or even the p-values, is not enough to say that the more complex model is statistically better at reproducing the features of the observed data. For that, we will need to turn to the model selection methods covered in the next notebook.\n",
    "\n",
    "To save ourselves having to re-run the chains in that notebook, let's save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # dill.dump_session('../ignore/model_evaluation.db') # change path if you want"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
