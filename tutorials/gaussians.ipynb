{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Gaussians and Least Squares\n",
    "\n",
    "So far in the notes and problems, we've mostly avoided one of the most commonly used probability distributions, the Gaussian or normal distribution:\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu,\\sigma) \\equiv p_\\mathrm{Normal}(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp \\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]$. [Endnote 1]\n",
    "\n",
    "There are two reasons for this:\n",
    "1. The symmetry between $x$ and $\\mu$ makes it easy to miss the distinction between the sampling distribution and the likelihood function, and to conflate the model parameter $\\sigma$ with an \"error bar\" associated strictly with the data (which it may or may not be).\n",
    "2. The assumption of Gaussian PDFs is baked into various classical statistics methods to the extent that it isn't always obvious to the user. As always, it's important to think about whether an assumption or approximation is justified, and thus to see examples of when it is not.\n",
    "\n",
    "That said, it is certainly common to use Gaussian distributions in practice, particularly in cases where\n",
    "1. the approximation is well justified, as in the large-count limit of the Poisson distribution (typical of optical astronomy and longer wavelengths); or\n",
    "2. we are effectively handed a table of data with \"error bars\" and have no better alternative than to assume a Gaussian sampling distribution.\n",
    "\n",
    "Gaussians have lots of nice mathematical features that make them convenient to work with when we can. For example, see a list of identities for the multivariate Gaussian [here](https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf) or [here](https://cs.nyu.edu/~roweis/notes/gaussid.pdf).\n",
    "\n",
    "There are a couple of cases that it's useful to work through if you haven't before, to build intuition. We'll do that here, with:\n",
    "\n",
    "* the product of two Gaussians\n",
    "* showing conjugacy\n",
    "* linear transformations\n",
    "* extending classical weighted least squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiplication\n",
    "\n",
    "The product of Gaussians comes up, for example, when the sampling distributions for different data points are independent Gaussians, or when the sampling distribution and prior are both Gaussian (this is a conjugate pair).\n",
    "\n",
    "So, consider\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu_1,\\sigma_1) \\, \\mathrm{Normal}(x|\\mu_2,\\sigma_2)$.\n",
    "\n",
    "This can be manipulated into a different product of two Gaussians, with $x$ appearing in only one of them. Do so. (Note that this is a proportionality, not an equality - the coefficient in front will not perfectly normalize things when you're done.)\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu_1,\\sigma_1) \\, \\mathrm{Normal}(x|\\mu_2,\\sigma_2) \\propto \\mathrm{Normal}(x|\\mu_a,\\sigma_a) \\, \\mathrm{Normal}(0|\\mu_b,\\sigma_b)$.\n",
    "\n",
    "If $x$ were a model parameter, and $\\mu_i$ and $\\sigma_i$ were independent measurements of $x$ with error bars, how do you interpret each term of this factorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> math, math, math, math,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your solution by plugging in some values for $x$, $\\mu_i$ and $\\sigma_i$. The function below returns the $\\frac{(x-\\mu)^2}{\\sigma^2}$ part of the PDF, which is what we care about here (since it's where $x$ appears)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# pick some values (where m is mu, s sigma)\n",
    "# x = \n",
    "# m1 = \n",
    "# s1 = \n",
    "# m2 = \n",
    "# s2 = \n",
    "\n",
    "# compute things\n",
    "# sa = \n",
    "# ma = \n",
    "# mb = \n",
    "# sb = \n",
    "\n",
    "def exp_part(y, m, s):\n",
    "    return ((y - m) / s)**2\n",
    "\n",
    "print('This should be a pretty small number:', \n",
    "      exp_part(x,m1,s1) + exp_part(x,m2,s2) - ( exp_part(x,ma,sa) + exp_part(0,mb,sb) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conjugacy\n",
    "\n",
    "When the sampling distribution is normal with a fixed variance, the conjugate prior for the mean is also normal. Show this for the case of a single data point, $y$; that is,\n",
    "\n",
    "$p(\\mu|y,\\sigma) \\propto \\mathrm{Normal}(y|\\mu,\\sigma)\\,\\mathrm{Normal}(\\mu|m_0,s_0) \\propto \\mathrm{Normal}(\\mu|m_1,s_1)$\n",
    "\n",
    "and find $m_1$ and $s_1$ in terms of $y$, $\\sigma$, $m_0$ and $s_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> math, math, math, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check your work by choosing some fiducial values and \n",
    "looking at the ratio $\\mathrm{Normal}(y|\\mu,\\sigma)\\,\\mathrm{Normal}(\\mu|m_0,s_0) / \\mathrm{Normal}(\\mu|m_1,s_1)$ over a range of $\\mu$. It should be constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# pick some values\n",
    "# y = \n",
    "# sigma = \n",
    "# m0 = \n",
    "# s0 = \n",
    "\n",
    "# compute things\n",
    "# s1 = \n",
    "# m1 = \n",
    "\n",
    "# plot\n",
    "mugrid = np.arange(-1.0, 2.0, 0.01)\n",
    "# we'll compare the log-probabilities, since that's a good habit to be in\n",
    "diff = st.norm.logpdf(y, loc=mugrid, scale=sigma)+st.norm.logpdf(mugrid, loc=m0, scale=s0) - st.norm.logpdf(mugrid, loc=m1, scale=s1)\n",
    "\n",
    "print('This should be a pretty small number, and constant:')\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(mugrid, diff, 'b-');\n",
    "plt.xlabel(r'$\\mu$');\n",
    "plt.ylabel('log-posterior difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear transformation\n",
    "\n",
    "Consider the distribution\n",
    "\n",
    "$\\mathrm{Normal}\\left[y\\,\\big|\\,\\mu_y(x;a,b),\\sigma_y\\right]$,\n",
    "\n",
    "where $\\mu_y(x;a,b)=a+bx$. Re-express this in terms of a distribution over $x$, i.e.\n",
    "\n",
    "$\\mathrm{Normal}\\left[x|\\mu_x(y;a,b),\\sigma_x(y;a,b)\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> math, math, math, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classical weighted least squares\n",
    "\n",
    "Classical WLS is a simple method for fitting a line to data that you've almost certainly seen before. Consider data consisting of $n$ triplets $(x_i,y_i,\\sigma_i)$, where $x_i$ are assumed to be known perfectly and $\\sigma_i$ is interpreted as a \"measurement error\" for $y_i$. WLS maximizes the likelihood function\n",
    "\n",
    "$\\mathcal{L}(a,b;x,y,\\sigma) = \\prod_{i=1}^n \\mathrm{Normal}(y_i|a+bx_i,\\sigma_i)$.\n",
    "\n",
    "In fact, we can get away with being more general and allowing for the possibility that the different measurements are not independent, with their measurement errors jointly characterized by a known covariance matrix, $\\Sigma$, rather than the individual $\\sigma_i$:\n",
    "\n",
    "$\\mathcal{L}(a,b;x,y,\\Sigma) = \\mathrm{Normal}(y|X\\beta,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp \\left[-\\frac{1}{2}(y-X\\beta)^\\mathrm{T}\\Sigma^{-1}(y-X\\beta)\\right]$,\n",
    "\n",
    "where $X$ is called the _design matrix_, with each row equal to $(1, x_i)$, and $\\beta = \\left(\\begin{array}{c}a\\\\b\\end{array}\\right)$.\n",
    "\n",
    "With a certain amount of algebra, it can be shown that $\\mathcal{L}$ is proportional to a bivariate Gaussian over $\\beta$,\n",
    "\n",
    "$\\mathcal{L} \\propto \\mathrm{Normal}(\\beta | \\mu_\\beta, \\Sigma_\\beta)$,\n",
    "\n",
    "with\n",
    "\n",
    "$\\Sigma_\\beta = (X^\\mathrm{T}\\Sigma^{-1}X)^{-1}$;\n",
    "\n",
    "$\\mu_\\beta = \\Sigma_\\beta X^\\mathrm{T}\\Sigma^{-1} y$.\n",
    "\n",
    "In classical WLS, $\\mu_\\beta$ is the \"best fit\" estimate of $a$ and $b$, and $\\Sigma_\\beta$ is the covariance of the standard errors on those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative simplicity of the computations above, not to mention the fact that they are efficiently implemented in numerous packages, can be useful even in situations beyond the assumption-heavy scenario where WLS is derived. As a simple example, consider a case where the sampling distribution corresponds to the likelihood function above, but we wish to use an informative prior on $a$ and $b$.\n",
    "\n",
    "Taking advantage of the results you derived above (all of which have straightforward multivariate analogs), \n",
    "1. What is the form of prior, $p(a,b|\\alpha)$, that makes this problem conjugate? (Here $\\alpha$ is a stand-in for whatever parameters determine the prior.)\n",
    "2. What are the form and parameters of the posterior, $p(a,b|x,y,\\Sigma,\\alpha)$?\n",
    "3. Verify that you recover the WLS solution in the limit of the prior being uniform over the $(a,b)$ plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will explicitly show the correspondance in (3) for a WLS fit of some mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data\n",
    "a = 0.0\n",
    "b = 1.0\n",
    "n = 10\n",
    "x = st.norm.rvs(size=n)\n",
    "sigma = st.uniform.rvs(1.0, 2.0, size=n)\n",
    "y = st.norm.rvs(loc=a+b*x, scale=sigma, size=n)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='bo');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses the `statsmodels` package to perform the WLS calculations. You are encouraged to implement the matrix algebra above to verify the results. What we get at the end are $\\mu_\\beta$ and $\\Sigma_\\beta$, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.WLS(y, sm.add_constant(x), weights=sigma**-2)\n",
    "wls = model.fit()\n",
    "mu_beta = np.matrix(wls.params).T # cast as a column vector\n",
    "Sigma_beta = np.asmatrix(wls.normalized_cov_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the parameters of the posterior for $\\beta$ based on $\\mu_\\beta$ and $\\Sigma_\\beta$ (parameters that appear in the sampling distribution) and the parameters of the conjugate prior. Set the prior parameters to be equivalent to the uniform distribution for the check below (you can put in something different to see how it looks later).\n",
    "\n",
    "Transform `post_mean` to a shape (3,) numpy array for convenience (as opposed to, say, a 3x1 matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# define prior parameters\n",
    "\n",
    "# do some calculations, possibly\n",
    "\n",
    "# parameters of the posterior:\n",
    "# post_cov = ...\n",
    "# post_mean = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the WLS and posterior parameters (they should be identical for a uniform prior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WLS mean and covariance:')\n",
    "print(mu_beta)\n",
    "print(Sigma_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Posterior mean and covariance:')\n",
    "print(post_mean)\n",
    "print(post_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can compare your analytic solution to a brute-force calculation of the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post_brute(a, b):\n",
    "    like = np.sum( st.norm.logpdf(y, loc=a+b*x, scale=sigma) )\n",
    "    prior = st.multivariate_normal.logpdf([a,b], mean=np.asarray(prior_mean)[:,0], cov=prior_cov)\n",
    "    return prior + like\n",
    "\n",
    "print('Difference between elegant and brute-force log posteriors for some random parameter values:')\n",
    "print('(The third column should be basically constant, though non-zero.)\\n')\n",
    "for i in range(10):\n",
    "    a = np.random.rand() * 10.0 - 5.0\n",
    "    b = np.random.rand() * 10.0 - 5.0\n",
    "    diff = st.multivariate_normal.logpdf([a,b], mean=post_mean, cov=post_cov) - log_post_brute(a,b)\n",
    "    print([a, b, diff])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endnotes\n",
    "\n",
    "1. In statistics literature, a more common convention is $p(x|\\mu,\\sigma^2)$, with the second parameter being the variance rather than the standard deviation; this is then consistent with the multivariate Gaussian notation in which the second parameter is the covariance matrix. However, most code implementations of the univariate normal distribution take the standard deviation as an argument rather than the variance, so we'll stick with that notation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
