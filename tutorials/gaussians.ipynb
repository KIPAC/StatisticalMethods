{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Gaussians and Least Squares\n",
    "\n",
    "So far in the notes and problems, we've mostly avoided one of the most commonly used probability distributions, the Gaussian or normal distribution:\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu,\\sigma) \\equiv p_\\mathrm{Normal}(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp \\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]$.[$^1$](#Note-1)\n",
    "\n",
    "There are two reasons for this:\n",
    "1. The symmetry between $x$ and $\\mu$ makes it easy to miss the distinction between the sampling distribution and the likelihood function, and to conflate the model parameter $\\sigma$ with an \"error bar\" associated strictly with the data (which it may or may not be).\n",
    "2. The assumption of Gaussian PDFs is baked into various classical statistics methods to the extent that it isn't always obvious to the user. As always, it's important to think about whether an assumption or approximation is justified, and thus to see examples of when it is not.\n",
    "\n",
    "That said, it is certainly common to use Gaussian distributions in practice, particularly in cases where\n",
    "1. the approximation is well justified, as in the large-count limit of the Poisson distribution (typical of optical astronomy and longer wavelengths); or\n",
    "2. we are effectively handed a table of data with \"error bars\" and have no better alternative than to assume a Gaussian sampling distribution.\n",
    "\n",
    "Gaussians have lots of nice mathematical features that make them convenient to work with when we can. For example, see a list of identities for the multivariate Gaussian [here](https://cs.nyu.edu/~roweis/notes/gaussid.pdf) and a whole lot of math [here](http://www.lucamartino.altervista.org/2003-003.pdf).\n",
    "\n",
    "There are a couple of cases that it's useful to work through if you haven't before, to build intuition. This is also good practice to reinforce some of the probability manipulations and reasoning you've learned.\n",
    "\n",
    "We'll look at\n",
    "\n",
    "* the product of two Gaussians\n",
    "* conjugacy of Gaussians\n",
    "* linear transformations of Gaussian-distributed variables\n",
    "* classical weighted least squares and its Bayesian extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TutorialName = 'gaussians'\n",
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiplication\n",
    "\n",
    "The product of Gaussians comes up, for example, when the sampling distributions for different data points are independent Gaussians, or when the sampling distribution and prior are both Gaussian (this is a conjugate pair).\n",
    "\n",
    "So, consider the product $\\mathrm{Normal}(x|\\mu_1,\\sigma_1) \\cdot \\mathrm{Normal}(x|\\mu_2,\\sigma_2)$.\n",
    "\n",
    "Show that this can be manipulated into a different product of two Gaussians,\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu_1,\\sigma_1) \\, \\mathrm{Normal}(x|\\mu_2,\\sigma_2) = \\mathrm{Normal}(x|\\mu_a,\\sigma_a) \\, \\mathrm{Normal}(0|\\mu_b,\\sigma_b)$,\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mu_a = \\sigma_a^{2}\\left(\\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2}\\right)$,\n",
    "* $\\sigma_a^2 = \\left(\\sigma_1^{-2} + \\sigma_2^{-2}\\right)^{-1}$,\n",
    "* $\\mu_b = \\mu_1 - \\mu_2$,\n",
    "* $\\sigma_b^2 = \\sigma_1^2 + \\sigma_2^2$.\n",
    "\n",
    "If $x$ were a model parameter, and $\\mu_i$ and $\\sigma_i$ were independent measurements of $x$ with error bars, how do you interpret each term of this factorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your solution by plugging in some values for $x$, $\\mu_i$ and $\\sigma_i$. The cell below will then compare the log of both sides of the equation above. The exact value you get will depend on exactly what numbers you choose, but generally (**checkpoint**) it should be pretty small, say $<10^{-10}$ in absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# pick some values (where m is mu, s sigma)\n",
    "# x = \n",
    "# m1 = \n",
    "# s1 = \n",
    "# m2 = \n",
    "# s2 = \n",
    "\n",
    "# compute things\n",
    "# sa = \n",
    "# ma = \n",
    "# mb = \n",
    "# sb = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This should be a pretty small number:', \n",
    "      st.norm.logpdf(x,m1,s1) + st.norm.logpdf(x,m2,s2) - ( st.norm.logpdf(x,ma,sa) + st.norm.logpdf(0,mb,sb) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later, note that a multidimensional version of this identity also holds. Here $\\Sigma$ is a covariance matrix, so we are missing some \"squares\" compared with the expressions above in terms of standard deviations (lower-case $\\sigma$), but otherwise things are very similar.\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu_1,\\Sigma_1) \\, \\mathrm{Normal}(x|\\mu_2,\\Sigma_2) = \\mathrm{Normal}(x|\\mu_a,\\Sigma_a) \\, \\mathrm{Normal}(0|\\mu_b,\\Sigma_b)$,\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mu_a = \\Sigma_a\\left(\\Sigma_1^{-1}\\mu_1 + \\Sigma_2^{-1}\\mu_2\\right)$,\n",
    "* $\\Sigma_a = \\left(\\Sigma_1^{-1} + \\Sigma_2^{-1}\\right)^{-1}$,\n",
    "* $\\mu_b = \\mu_1 - \\mu_2$,\n",
    "* $\\Sigma_b = \\Sigma_1 + \\Sigma_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conjugacy\n",
    "\n",
    "When the sampling distribution is normal with a fixed variance, the conjugate prior for the mean is also normal. Using the identity from part 1, show this for the case of a single data point, $y$; that is,\n",
    "\n",
    "$p(\\mu|y,\\sigma) \\propto \\mathrm{Normal}(y|\\mu,\\sigma)\\,\\mathrm{Normal}(\\mu|m_0,s_0) \\propto \\mathrm{Normal}(\\mu|m_1,s_1)$,\n",
    "\n",
    "and find $m_1$ and $s_1$ in terms of $y$, $\\sigma$, $m_0$ and $s_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check your work by choosing some fiducial values and \n",
    "using the cells below to look at the ratio\n",
    "\n",
    "$\\mathrm{Normal}(y|\\mu,\\sigma)\\,\\mathrm{Normal}(\\mu|m_0,s_0) / \\mathrm{Normal}(\\mu|m_1,s_1)$\n",
    "\n",
    "over a range of $\\mu$. It should be constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# pick some values\n",
    "# y = \n",
    "# sigma = \n",
    "# m0 = \n",
    "# s0 = \n",
    "\n",
    "# compute things\n",
    "# s1 = \n",
    "# m1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot over a range that at least includes both y and m0\n",
    "mugrid = np.linspace(min(y,m0), max(y,m0))\n",
    "# we'll compare the log-probabilities, since that's a good habit to be in\n",
    "diff = st.norm.logpdf(y, loc=mugrid, scale=sigma)+st.norm.logpdf(mugrid, loc=m0, scale=s0) - st.norm.logpdf(mugrid, loc=m1, scale=s1)\n",
    "\n",
    "print('This should be approximately constant:')\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(mugrid, diff, 'b-');\n",
    "plt.xlabel(r'$\\mu$');\n",
    "plt.ylabel('log-posterior difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Checkpoint:** Remeber that in this case the plotted function should be close to _constant_, probably so close that matplotlib shows it as a perfectly flat line, but not necessarily small in absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear transformation\n",
    "\n",
    "Consider the distribution $\\mathrm{Normal}\\left[y\\,\\big|\\,\\mu_y(x;a,b),\\sigma_y\\right]$, where $\\mu_y(x;a,b)=a+bx$. \n",
    "\n",
    "Show that this can be rearranged into a Gaussian over $x$, i.e.\n",
    "\n",
    "$\\mathrm{Normal}\\left[x|\\mu_x(y;a,b),\\sigma_x(y;a,b)\\right]$,\n",
    "\n",
    "up to an errant factor of $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classical weighted least squares (WLS)\n",
    "\n",
    "Classical WLS is a simple method for fitting a line to data that you've almost certainly seen before. Consider data consisting of $n$ triplets $(x_i,y_i,\\sigma_i)$, where $x_i$ are assumed to be known perfectly and $\\sigma_i$ is interpreted as a \"measurement error\" for $y_i$. WLS maximizes the likelihood function\n",
    "\n",
    "$\\mathcal{L}(a,b;x,y,\\sigma) = \\prod_{i=1}^n \\mathrm{Normal}(y_i|a+bx_i,\\sigma_i)$.\n",
    "\n",
    "In fact, we can get away with being more general and allowing for the possibility that the different measurements are not independent, with their measurement errors jointly characterized by a known covariance matrix, $\\Sigma$, rather than the individual $\\sigma_i$:\n",
    "\n",
    "$\\mathcal{L}(a,b;x,y,\\Sigma) = \\mathrm{Normal}(y|X\\beta,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp \\left[-\\frac{1}{2}(y-X\\beta)^\\mathrm{T}\\Sigma^{-1}(y-X\\beta)\\right]$,\n",
    "\n",
    "where $X$ is called the _design matrix_, with each row equal to $(1, x_i)$, and $\\beta = \\left(\\begin{array}{c}a\\\\b\\end{array}\\right)$.\n",
    "\n",
    "With a certain amount of algebra, it can be shown that $\\mathcal{L}$ is proportional to a bivariate Gaussian over $\\beta$,\n",
    "\n",
    "$\\mathcal{L} \\propto \\mathrm{Normal}(\\beta | \\mu_\\beta, \\Sigma_\\beta)$,\n",
    "\n",
    "with\n",
    "\n",
    "$\\Sigma_\\beta = (X^\\mathrm{T}\\Sigma^{-1}X)^{-1}$;\n",
    "\n",
    "$\\mu_\\beta = \\Sigma_\\beta X^\\mathrm{T}\\Sigma^{-1} y$.\n",
    "\n",
    "We will not drag you through the derivation, but hopefully the resemblance of these expressions to the simpler ones above makes them plausible.\n",
    "\n",
    "In classical WLS, $\\mu_\\beta$ is the \"best fit\" estimate of $a$ and $b$, and $\\Sigma_\\beta$ is the covariance of the standard errors on those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative simplicity of the computations above, not to mention the fact that they are efficiently implemented in numerous packages, can be useful even in situations beyond the assumption-heavy scenario where WLS is derived. As a simple example, consider a case where the sampling distribution corresponds to the likelihood function above, but we wish to use an informative prior on $a$ and $b$.\n",
    "\n",
    "Taking advantage of the results you derived above (all of which have straightforward multivariate analogs), \n",
    "1. What is the form of prior, $p(a,b|\\alpha)$, that makes this problem conjugate? (Here $\\alpha$ is a stand-in for whatever parameters determine the prior.)\n",
    "2. What are the form and parameters of the posterior, $p(a,b|x,y,\\Sigma,\\alpha)$?\n",
    "3. Verify that you recover the WLS solution in the limit of the prior being uniform over the $(a,b)$ plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answers in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will explicitly show the correspondance in (3) for a WLS fit of some mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data\n",
    "a = 0.0\n",
    "b = 1.0\n",
    "n = 10\n",
    "x = st.norm.rvs(size=n)\n",
    "sigma = st.uniform.rvs(1.0, 2.0, size=n)\n",
    "y = st.norm.rvs(loc=a+b*x, scale=sigma, size=n)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='bo');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses the `statsmodels` package to perform the WLS calculations. You are encouraged to implement the matrix algebra above to verify the results. What we get at the end are $\\mu_\\beta$ and $\\Sigma_\\beta$, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.WLS(y, sm.add_constant(x), weights=sigma**-2)\n",
    "wls = model.fit()\n",
    "mu_beta = np.matrix(wls.params).T # transpose casts this as a column vector\n",
    "Sigma_beta = np.asmatrix(wls.normalized_cov_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the parameters of the posterior for $\\beta$ based on $\\mu_\\beta$ and $\\Sigma_\\beta$ (parameters that appear in the sampling distribution) and the parameters of the conjugate prior. Set the prior parameters to be equivalent to the uniform distribution for the check below (you can put in something different to see how it looks later).\n",
    "\n",
    "Transform `post_mean` to a shape (2,) numpy array for convenience (as opposed to, say, a 2x1 matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "\n",
    "# define prior parameters\n",
    "\n",
    "# do some calculations, possibly\n",
    "\n",
    "# parameters of the posterior:\n",
    "# post_cov = ...\n",
    "# post_mean = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WLS mean and covariance:')\n",
    "print(mu_beta)\n",
    "print(Sigma_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Posterior mean and covariance:')\n",
    "print(post_mean)\n",
    "print(post_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your log-prior to the function definition below. We will then compare your analytic solution to a brute-force calculation of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post_brute(a, b):\n",
    "    like = np.sum( st.norm.logpdf(y, loc=a+b*x, scale=sigma) )\n",
    "    prior = TBC() # some logpdf\n",
    "    return prior + like\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Difference between elegant and brute-force log posteriors for some random parameter values:')\n",
    "print('(The third column should be basically constant, though non-zero.)\\n')\n",
    "for i in range(10):\n",
    "    a = np.random.rand() * 10.0 - 5.0\n",
    "    b = np.random.rand() * 10.0 - 5.0\n",
    "    diff = st.multivariate_normal.logpdf([a,b], mean=post_mean, cov=post_cov) - log_post_brute(a,b)\n",
    "    print([a, b, diff])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endnotes\n",
    "\n",
    "##### Note 1\n",
    "\n",
    "In statistics literature, a more common convention is $p(x|\\mu,\\sigma^2)$, with the second parameter being the variance rather than the standard deviation; this is then consistent with the multivariate Gaussian notation in which the second parameter is the covariance matrix. However, most code implementations of the univariate normal distribution take the standard deviation as an argument rather than the variance, so we'll stick with that notation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
