{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJMC3HI2Bc7q"
   },
   "source": [
    "# Tutorial: Essential Probability\n",
    "\n",
    "In this notebook, we will get some practice with simple probabilistic manipulations and calculations. None of it is terribly complicated in the grand scheme of things, either conceptually or numerically. On the other hand, we will be speaking the language of probability and doing calculations similar to these throughout the course, so it's worth spending a little time to make sure we have the fundamentals down. In particular, if you are already proficient with Python and NumPy, the mechanical aspects may seem very easy. Conversely, if you feel at sea working through the steps below, this is an indication that the programming learning curve later on will be a real challenge.\n",
    "\n",
    "Below, you will\n",
    "* implement functions that evaluate the marginal and conditional distributions of a multivatiate Gaussian\n",
    "* numerically approximate those distributions on a grid, and compare\n",
    "* approximate the same distributions using random samples, and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFqhZ5c2Bc7v"
   },
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "from yaml import safe_load\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UazlyyWyBc7y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13b7b587e2f3c4efb272e00d270a22d7",
     "grade": true,
     "grade_id": "datapath",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thisTutorial = 'essential_probability'\n",
    "if getcwd() == '/content':\n",
    "    # assume we are in Colab, and the user's data directory is linked to their drive/Physics267_data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    datapath = '/content/drive/MyDrive/Physics267_data/' + thisTutorial + '/'\n",
    "else:\n",
    "    # assume we are running locally somewhere and have the data under ./data/\n",
    "    datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfiEPGiRBc7z"
   },
   "source": [
    "## Background\n",
    "\n",
    "As you have seen in the notes, the univariate Gaussian or normal PDF is\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu,\\sigma) \\equiv p_\\mathrm{Normal}(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp \\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]$.\n",
    "\n",
    "The multivariate version of this distribution is\n",
    "\n",
    "$\\mathrm{Normal}_n({\\bf x}|{\\bf \\mu},{\\bf \\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n|{\\bf \\Sigma}|}}\\exp \\left[-\\frac{1}{2}({\\bf x}-{\\bf \\mu})^{\\mathrm{T}}{\\bf \\Sigma}^{-1}({\\bf x}-{\\bf \\mu})\\right]$,[$^1$](#Note-1)\n",
    "\n",
    "where ${\\bf x}$ and ${\\bf \\mu}$ are now vectors of length $n$, and ${\\bf \\Sigma}$ is an $n \\times n$ positive definite matrix. In the $n=2$ case, we have ${\\bf x}=(x,y)$ and ${\\bf \\mu}=(\\mu_x,\\mu_y)$, and ${\\bf \\Sigma}$ is usually decomposed as\n",
    "\n",
    "${\\bf \\Sigma} = \\left( \\begin{array}{cc} \\sigma^2_x & \\rho\\sigma_x\\sigma_y \\\\ \\rho\\sigma_x\\sigma_y & \\sigma^2_y \\end{array} \\right)$.\n",
    "\n",
    "With a little patience, you can expand the PDF into this form\n",
    "\n",
    "$\\mathrm{Normal}_2({\\bf x}|{\\bf \\mu},{\\bf \\Sigma}) = \\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}} \\exp\\left\\{-\\frac{1}{2(1-\\rho^2)}\\left[ \\left(\\frac{x-\\mu_x}{\\sigma_x}\\right)^2 + \\left(\\frac{y-\\mu_y}{\\sigma_y}\\right)^2 - 2\\rho \\left(\\frac{x-\\mu_x}{\\sigma_x}\\right)\\left(\\frac{y-\\mu_y}{\\sigma_y}\\right) \\right]\\right\\}$.\n",
    "\n",
    "With slightly more patience, and some algebra, it's also possible[$^2$](#Note-2) to rearrange the PDF into a factored form\n",
    "\n",
    "$\\mathrm{Normal}_2({\\bf x}|{\\bf \\mu},{\\bf \\Sigma}) = \\mathrm{Normal}\\left(x|\\mu_x,\\sigma_x\\right) \\, \\mathrm{Normal}\\left[y\\left| \\mu_y+\\frac{\\rho\\sigma_y}{\\sigma_x}(x-\\mu_x), \\sigma_y\\sqrt{1-\\rho^2} \\right.\\right] = \\mathrm{Normal}\\left(y|\\mu_y,\\sigma_y\\right) \\, \\mathrm{Normal}\\left[x\\left| \\mu_x+\\frac{\\rho\\sigma_x}{\\sigma_y}(y-\\mu_y), \\sigma_x\\sqrt{1-\\rho^2} \\right.\\right]$.[$^3$](#Note-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGIAozl1Bc70"
   },
   "source": [
    "## Implementing analytically derived PDFs\n",
    "\n",
    "Although we won't need it until the next section, our first task is to implement a function evaluating the bivariate Gaussian PDF, as a function of the parameters in the last equation.\n",
    "\n",
    "In this notebook, please implement everything straightforwardly from (any of) the equations above rather than calling pre-packaged functions from `scipy` or elsewhere.[$^4$](#Note-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0zFS7rX0Bc70",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8848965e93b0c372712744baa1fc9658",
     "grade": false,
     "grade_id": "p_xy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p_xy(x, y, mu_x, mu_y, sigma_x, sigma_y, rho):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29UKfdiwBc71"
   },
   "source": [
    "Next, let's determine the marginal distribution of $x$, $p(x)$, and the conditional distribution of $y$, $p(y|x)$. We won't bother with $p(y)$ and $p(x|y)$, given the evident symmetry of the expressions.[$^3$](#Note-3) Recall that, by definition,\n",
    "\n",
    "$p(x) = \\int dy \\, p(x,y)$\n",
    "\n",
    "(where $p(x,y)$ is shorthand for the bivariate normal density as a function of $\\mu_x$, $\\mu_y$, etc.), and the marginal and conditional distributions jointly satisfy\n",
    "\n",
    "$p(x,y) = p(x) \\, p(y|x)$.\n",
    "\n",
    "Based on the previous section, write down $p(x)$ and $p(y|x)$. Both of these are (spoiler) Gaussians, and should therefore respectively be put in the simplified forms $\\mathrm{Normal}(x|\\ldots)$ and $\\mathrm{Normal}(y|\\ldots)$. This can (and should!) be accomplished by inspection,[$^3$](#Note-3) although you can work things through if you have any doubts. Refer to the \"most convenient mathematical shortcuts ever\" section in the Essential Probability notes if this seems weird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-2KghHpBc71"
   },
   "source": [
    "> Space for your analytical derivations/arguments\n",
    "\n",
    "> **Note:** Equations in markdown cells are coded in LaTeX, between $'s. Examine the cells above in edit mode (double click them) to see. If you're not yet a LaTeX magician, don't worry; you'll probably see everything you need for the moment as we go. When in doubt, the appendices of [this guide](http://www.math.hkbu.edu.hk/TeX/essential.pdf) list many helpful math commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Aob6iJBc72"
   },
   "source": [
    "Implement Python functions for $p(x)$ and $p(y|x)$, analogous to the one for $p(x,y)$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "zYS3NfPBBc72",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e33f935d0eb792a8e87f24f8a66c9e4a",
     "grade": false,
     "grade_id": "p_x",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p_x(x, mu_x, mu_y, sigma_x, sigma_y, rho):\n",
    "    # note that `y' is not an argument of this function!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0qmFjdUpBc73",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6b7a2ceb8de61cc6699a0cd6f356c54",
     "grade": false,
     "grade_id": "p_y_given_x",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p_y_given_x(y, x, mu_x, mu_y, sigma_x, sigma_y, rho):\n",
    "    # note the order of `y' and `x' above!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vfpyv-lBc73"
   },
   "source": [
    "In the remaining sections, we'll compare these functions to numerical approximations for a particular choice of ${\\bf \\mu}$ and ${\\bf \\Sigma}$, and a value of $x$ to condition on. Let's find out what they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zigvKHATBc74"
   },
   "outputs": [],
   "source": [
    "test_values = safe_load(open(datapath+'data.yaml', 'r').read())\n",
    "params = test_values['params']\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nUA-av-Bc74"
   },
   "outputs": [],
   "source": [
    "fixed_x = test_values['fixed_x']\n",
    "fixed_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1LxzhwVBc75"
   },
   "source": [
    "While we're here, we may as well check that your functions satisfy the joint definition of marginal and conditional probabilities for these parameter values and some arbitrary choices of $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4HaoNNqqBc75",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "306253c2e25b4a7c30f293c9e21631d6",
     "grade": true,
     "grade_id": "test_functions",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.isclose(p_xy(0.0, 0.0, **params), p_x(0.0, **params) * p_y_given_x(0.0, 0.0, **params))\n",
    "assert np.isclose(p_xy(1.0, -1.0, **params), p_x(1.0, **params) * p_y_given_x(-1.0, 1.0, **params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJmoLR-nBc76"
   },
   "source": [
    "**Python trivia:** The `**params` in the function calls above passes the entries of the `params` dictionary as named arguments of the function, i.e. it is equivalent to the very tedious `mu_x=params['mu_x'], mu_y=params['mu_y'], sigma_x=params['sigma_x'], ...`. This is a nice convenience, allowing us to write functions with meaningfully named arguments, while also keeping and passing sets of parameter values in organized structures. We'll use this functionality often in these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs3MpVGmBc77"
   },
   "source": [
    "## Numerical comparison: on a grid\n",
    "\n",
    "In this section we'll check your analytical functions against numerical calculations on a grid, the latter being a reasonable way to proceed in some other 2-dimensional case that didn't happen to have simple analytic solutions.\n",
    "\n",
    "Of course, a grid-based calculation will be always be somewhat approximate, since\n",
    "1. the grid can't extend to infinity in $x$ and $y$, and therefore can't account for 100% of the probability in $p(x,y)$, and\n",
    "2. the grid's finite spacing can't perfectly reproduce the _shape_ of $p(x,y)$.\n",
    "\n",
    "Nevertheless, it's possible for grid calculations to be extremely accurate, for sensible choices of bounds and spacing. Go ahead and make some choices below, with the aim of\n",
    "1. containing _most_ of the probability within the grid bounds, and\n",
    "2. resolving the shape of $p(x,y)$ reasonably with the grid spacing.\n",
    "\n",
    "You will be able to come back and adjust these if they don't seem to be doing the job, of course.\n",
    "\n",
    "**Tip:** We recommend defining things such that the number of grid points in $x$ will be different from the number of grid points in $y$. There is no deep reason for this, but it makes it much harder to get confused about which array axis corresponds to which variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "6Wgv5ChFBc77",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "925fb7afefaba6e4a44ab1e2e3b8d55e",
     "grade": false,
     "grade_id": "grid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x bounds and spacing\n",
    "# xmin = ...\n",
    "# xmax = ...\n",
    "# dx = ...\n",
    "\n",
    "# y bounds and spacing\n",
    "# ymin = ...\n",
    "# ymax = ...\n",
    "# dy = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kNlklGEBc78"
   },
   "source": [
    "The cell below defines vectors that hold the $x$ and $y$ values that will be gridded, as well as arrays (using the handy `numpy.meshgrid` function) where each entry holds the $x$ and $y$ value corresponding to that grid point. (If that was confusing, the plot below should clear it up.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7BmsSFGBc78"
   },
   "outputs": [],
   "source": [
    "xvalues = np.arange(xmin, xmax+dx, dx)\n",
    "yvalues = np.arange(ymin, ymax+dy, dy)\n",
    "grid_x, grid_y = np.meshgrid(xvalues, yvalues, indexing='ij')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWfVMNk1Bc79"
   },
   "source": [
    "**Aside:** Note the `indexing='ij'` option to `meshgrid`. This makes the function return arrays where the first index corresponds to $x$ and the second to $y$ instead of vice versa. The result is a much better convention in terms of generalizing to higher dimensions (and it corresponds to our usual mathematical notation, $x,y$), but it means we will need to transpose arrays before plotting them if we want $x$ to appear on the horizontal axis and $y$ to appear on the vertical axis.\n",
    "\n",
    "The cell below illustrates the contents of `grid_x` and `grid_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prqc1tiSBc79"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "fig, ax = plt.subplots(1,2);\n",
    "ax[0].imshow(grid_x.T, cmap='gray', origin='lower', extent=[xmin, xmax, ymin, ymax]);\n",
    "ax[0].set_xlabel('x'); ax[0].set_ylabel('y'); ax[0].set_title('grid_x');\n",
    "ax[1].imshow(grid_y.T, cmap='gray', origin='lower', extent=[xmin, xmax, ymin, ymax]);\n",
    "ax[1].set_xlabel('x'); ax[1].set_ylabel('y'); ax[1].set_title('grid_y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmaaEGevBc7-"
   },
   "source": [
    "Why would we want 2D arrays that just repeat the information we already have in `xvalues` and `yvalues`? Just for convenience. Because `numpy` can do most mathematical operations on arrays transparently, and assuming the functions you wrote above straightforwardly implement the relevant equations, we should be able to pass `grid_x` and `grid_y` in place of the arguments `x` and `y` and get sensible results, without explicitly writing a loop over every array element.\n",
    "\n",
    "Let's see how that works by evaluating $p(x,y)$ on this grid and visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97WnZfQYBc7-"
   },
   "outputs": [],
   "source": [
    "grid_p_xy = p_xy(grid_x, grid_y, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4qZgmbBBc7_"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.imshow(grid_p_xy.T, cmap='gray', origin='lower', extent=[xmin, xmax, ymin, ymax]);\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOshqcV9Bc7_"
   },
   "source": [
    "If the grid you defined was sensible, the bright part of `grid_p_xy` (containing most of the probability) should be comfortably within the  array, and should be well resolved (not look too pixilated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXsn_oW1Bc8A"
   },
   "source": [
    "### Marginalization\n",
    "\n",
    "Next, we'll get more quantitative and check your function for $p(x)$ against the equivalent numerical calculation using `grid_p_xy`. This means numerically carrying out the integral $p(x) = \\int dy \\, p(x,y)$, so the result (`grid_p_x`) should be a 1D array whose values are our numerical approximation of $p(x)$ at the `xvalues`. Numerical integration techniques could easily be a course on their own, but in situations like this it frankly isn't worth doing anything more complex than a Riemann sum. After all, at least in this case, it's practically painless for us to re-create the grid with different bounds and/or spacings if we decide that's needed to make the sum accurate. That said, you can perform the integral however you want, provided it only relies on the `grid_p_xy` array and the variables defining its bounds/spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "YTmDKnuzBc8A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd9f33035873569b71aa14caf83ac331",
     "grade": false,
     "grade_id": "grid_marg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_p_x = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOMb9BFzBc8B"
   },
   "source": [
    "Before moving on, check that your numerical approximation to $p(x)$ on a grid is normalized, as it should be, that is that $\\int dx\\, p(x) = 1$. This can be accomplished with essentially the same technique you used to integrate over $y$ above. If the assertion fails, either your integration is suspect, the grid bounds/spacing are insufficient, or something is fishy in your implementation of `p_xy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "q_eusvtfBc8B",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "734ea68c6abd1aaf0e10d9cf9bb22cb9",
     "grade": false,
     "grade_id": "grid_marg_norm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_p_x_integral = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fWRurwfuBc8C",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20a5407e87cb4ee7ed05c70e04e067eb",
     "grade": true,
     "grade_id": "normtest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(grid_p_x_integral)\n",
    "assert np.isclose(grid_p_x_integral, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoHsyiMmBc8D"
   },
   "source": [
    "Good! Now let's compare this with your analytical function for $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtf5hMecBc8E"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.plot(xvalues, grid_p_x, 'b.', label='grid');\n",
    "plt.plot(xvalues, p_x(xvalues, **params), 'r-', label='analytic');\n",
    "plt.xlabel('x'); plt.ylabel('p(x)'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhUcKJFwBc8F"
   },
   "source": [
    "If the plot above *looks* ok, but the following numerical check fails, you might be not quite as accurate as the `atol` tolerance below. It should be possible to correct this by changing the grid bounds/spacing appropriately. Alternatively, double-check your `p_x` function. (Also note that, occasionally, the points can look a _little_ bit offset just because `matplotlib` is silly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JHChm0GGBc8G",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aead79178747ede27ea0f7ff94922743",
     "grade": true,
     "grade_id": "margtest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(grid_p_x, p_x(xvalues, **params), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7_qBvevBc8G"
   },
   "source": [
    "### Conditioning\n",
    "\n",
    "Numerically checking $p(y|x)$ is more straightforward, since there is no integration involved. We don't even need a 2D grid, but can instead directly evaluate $p(x,y)$ as a function of $y$, with $x$ fixed at some value. Recall that, for our tests, this value is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MM33T65Bc8H"
   },
   "outputs": [],
   "source": [
    "fixed_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziU3hwpQBc8I"
   },
   "source": [
    "As a test of your analytical `p_y_given_x`, let's numerically evaluate $p(x=$ `fixed_x` $,y)$ at $y=$`yvalues`. Don't forget that you will also to divide by $p(x)$ to normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "jwnZfbM5Bc8J",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaa6101fc16cf2ce9dbb7d4287d734e1",
     "grade": false,
     "grade_id": "grid_conditional",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_p_y_given_x = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRzWX3MaBc8L"
   },
   "source": [
    "Again, let's explicitly verify the normalization by directly integrating `grid_p_y_given_x` over $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "WMgnBMoeBc8L",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7940318bff60779f3371065e23118b1d",
     "grade": false,
     "grade_id": "grid_conditional_norm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_p_y_given_x_integral = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t5pmgEfGBc8M",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "005be1ae96620afedb42c39d202f6285",
     "grade": true,
     "grade_id": "normtest2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(grid_p_y_given_x_integral)\n",
    "assert np.isclose(grid_p_y_given_x_integral, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG2r5W4gBc8N"
   },
   "source": [
    "We'll also do visual and quantitative comparison with `p_y_given_x`, analogously to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx-RLB94Bc8O"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.plot(yvalues, grid_p_y_given_x, 'b.', label='grid');\n",
    "plt.plot(yvalues, p_y_given_x(yvalues, fixed_x, **params), 'r-', label='analytic');\n",
    "plt.xlabel('y'); plt.ylabel('p(y|x='+str(fixed_x)+')'); plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3g0DrhqKBc8O",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23ba2143e6daeb9fb58635199d1a3d70",
     "grade": true,
     "grade_id": "condtest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(grid_p_y_given_x, p_y_given_x(yvalues, fixed_x, **params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi6Cn5ZMBc8P"
   },
   "source": [
    "## Numerical comparison: monte carlo\n",
    "\n",
    "To finish this notebook, we'll look at how to perform conditioning and marginalization when we have samples from a PDF rather than a simple analytical expression or an evaluation of the PDF over a grid. This will be our most common mode of operation in the latter part of this course, since analytical and grid-based methods tend to become intractable once we're working in more than a few dimensions.\n",
    "\n",
    "Randomly sampling from general PDFs is a topic that we will cover later on. For the moment, we're using a simple PDF, so we'll use simple methods. In fact, we can take advantage of the marginal and conditional distributions that you've already worked out to produce samples from our 2D PDF using 2 calls to a univariate random number generator. Namely, below,\n",
    "1. Generate samples of $x$ from its marginal PDF, $p(x)$. Then,\n",
    "2. Generate corresponding samples of $y$ from its conditional PDF, $p(y|x)$.\n",
    "\n",
    "Since each of these steps samples a 1D Gaussian, you can just use the `numpy.random.randn` function; `np.random.randn(Nsamples)` will produce an `Nsamples`-length array distributed normally with a mean of zero and a standard deviation of 1. To get samples from some other normal distribution, just multiply these by the desired standard deviation, and add the desired mean.\n",
    "\n",
    "For organization's sake, store these samples in a dictonary with `x` and `y` keys, i.e. `samples={'x':..., 'y':...}`. Corresponding elements of `samples['x']` and `samples['y']` should then be valid draws from $p(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ldHntwCkBc8P",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e787083bf60dfaa47b4e9acf4ef19d9",
     "grade": false,
     "grade_id": "get_samples",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Nsamples = 100000 # this is more than we will typically have in practice, but should be good for illustrating how things work here\n",
    "\n",
    "# samples = ... (a dictionary as described above)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0axwM3kzBc8Q"
   },
   "source": [
    "Let's check visually whether these samples have the correct 2D distribution by plotting some of them over the gridded evaluation of $p(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YtUl_3lSBc8Q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06bff52324a78889042d82991464af65",
     "grade": true,
     "grade_id": "samples",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.imshow(grid_p_xy.T, cmap='gray', origin='lower', extent=[xmin, xmax, ymin, ymax]);\n",
    "plt.plot(samples['x'][:5000], samples['y'][:5000], 'r,');\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygWot0IpBc8R"
   },
   "source": [
    "The red points should be densest where the background image is brightest, and their distribution should conspicuously have the same overall orientation. The following sections will give us additional (if still not comprehensive) double-checks that the samples come from the intended distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKojcqh6Bc8R"
   },
   "source": [
    "### Marginalization\n",
    "\n",
    "When dealing with samples, marginalization is the simplest operation imaginable. To estimate the distribution $p(x)$, we just... look at the distribution of `samples['x']`, paying no attention at all to `samples['y']`. Although there are other methods, the simplest way to estimate a distribution from samples is by making a histogram, as we do below. The `density` argument to `plt.hist` divides the number of samples in each histogram bin by `Nsamples` times the bin width, making it the fractional density of samples in the bin (our estimate of the PDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dco6yQ8WBc8S"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.hist(samples['x'], density=True, bins=50);\n",
    "plt.plot(xvalues, p_x(xvalues, **params), 'r-');\n",
    "plt.xlabel('x'); plt.ylabel('p(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK4W0O6FBc8S"
   },
   "source": [
    "Although the binning and the finite number of samples can make the comparison less than perfect, the histogram should agree with your analytic $p(x)$ very well in this case. If we use a smaller number of samples to make the histogram while keeping the bin width similar, as below, you'll see the estimate become increasingly noisy. The solution to this, if it matters, is to do some smoothing of the histogram or use wider bins (which is the same thing)... or ideally to get more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1Hfh5wUBc8S"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "fig, ax = plt.subplots(1,2);\n",
    "ax[0].hist(samples['x'][:10000], density=True, bins=50);\n",
    "ax[0].plot(xvalues, p_x(xvalues, **params), 'r-');\n",
    "ax[0].set_xlabel('x'); ax[0].set_ylabel('p(x)'); ax[0].set_title('10000 samples');\n",
    "ax[1].hist(samples['x'][:1000], density=True, bins=50);\n",
    "ax[1].plot(xvalues, p_x(xvalues, **params), 'r-');\n",
    "ax[1].set_xlabel('x'); ax[1].set_ylabel('p(x)'); ax[1].set_title('1000 samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9ufvY0MBc8T"
   },
   "source": [
    "### Conditioning\n",
    "\n",
    "In principle, conditioning with samples is also completely straightforward. We simply select the subset of samples that satisfy the condition, in our case $x=$ `fixed_x`, and make a histogram of the corresponding $y$ values. In practice, however, the number of samples that satisfy the condition _exactly_ is probably zero. We will need to compromise, and instead, for example, select those samples where $x$ is within some distance of `fixed_x` (i.e., we will have to relax the condition). The `numpy.flatnonzero` function can accomplish this; below we use it to find the indices for which `|samples['x']-fixed_x|` is less than a tenth of the standard deviation of `samples['x']`. This is not necessarily a smart definition of \"close to `fixed_x`\" in general, but at least it means \"close relative to the overall variation in $x$\". You are encouraged to play around with this selection to see how it changes things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP2xbDjSBc8T"
   },
   "outputs": [],
   "source": [
    "j = np.flatnonzero(np.abs(samples['x'] - fixed_x)<0.1*samples['x'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJL2Tb8JBc8U"
   },
   "source": [
    "An obvious downside to this approach is that we are throwing away many, probably most, of the original samples, which in general might have been very difficult to produce in the first place. Let's see how many are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o1Mv8mnBc8U"
   },
   "outputs": [],
   "source": [
    "len(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvaoXeUIBc8U"
   },
   "source": [
    "Presuming that this is a small fraction of `Nsamples`, we can also expect the resulting histogram to be noisy, although the resemblance to the analytic result should still be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXoAtJioBc8U"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.hist(samples['y'][j], density=True, bins=50);\n",
    "plt.plot(yvalues, p_y_given_x(yvalues, fixed_x, **params), 'r-');\n",
    "plt.xlabel('y'); plt.ylabel('p(y|x='+str(fixed_x)+')');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBx2O95sBc8V"
   },
   "source": [
    "So, while it's indeed possible to condition samples in principle, in practice it's often better to simply generate samples that satisfy the desired condition by construction. Again, this is straightforward in this case using your analytic results. Below, generate `Nsamples` samples of $y$ for $x=$ `fixed_x` using `numpy.random.randn` modulated by the appropriate mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "sShpaBPcBc8V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "551c916093c78b1e0ba2d73cb014b03f",
     "grade": false,
     "grade_id": "samples_conditional",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# samples_y_given_x = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDXCFEVOBc8W"
   },
   "source": [
    "The histogram of these samples should look much better compared with the analytic curve, mainly because there are more of them, and partly because they exactly satisfy the desired conditioning rather than approximately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8GgVu6iZBc8W",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f5ad2006521efffcf1619dd8e75c715",
     "grade": true,
     "grade_id": "cond_samples_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.hist(samples_y_given_x, density=True, bins=50);\n",
    "plt.plot(yvalues, p_y_given_x(yvalues, fixed_x, **params), 'r-');\n",
    "plt.xlabel('y'); plt.ylabel('p(y|x='+str(fixed_x)+')');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PB-5pOjBc8X"
   },
   "source": [
    "## Parting thoughts\n",
    "\n",
    "In this tutorial, you've practiced a little bit of probabilistic reasoning, and worked through some of the common mathematical operations in a simplified setting. While we may not see precisely these implementations again, we'll be building on the concepts used here for the remainder of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlpBtRUkBc8X"
   },
   "source": [
    "#### Endnotes\n",
    "\n",
    "##### Note 1\n",
    "\n",
    "About the notation: It's a little weird that in the multivariate case we write $p(x|\\mu,\\Sigma)$, with the covariance $\\Sigma$ as a parameter, while in the univariate case we write $p(x|\\mu,\\sigma)$, with the standard deviation $\\sigma$ rather than the variance $\\sigma^2$. In statistics literature, it's more common to be consistent, i.e. to write the univariate PDF as $p(x|\\mu,\\sigma^2)$. However, most code implementations of the univariate normal distribution (including `scipy`) take the standard deviation as an argument rather than the variance, so we'll stick with our odd, inconsistent notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fosBCea1Bc8Y"
   },
   "source": [
    "##### Note 2\n",
    "\n",
    "And highly recommended practice! But also a bit tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG_ICbCABc8Y"
   },
   "source": [
    "##### Note 3\n",
    "\n",
    "You're welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p7fPhTRBc8Z"
   },
   "source": [
    "##### Note 4\n",
    "\n",
    "`numpy` functions are fine. We mean it about `scipy`, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbDX4sAqBc8Z"
   },
   "outputs": [],
   "source": [
    "assert('scipy' not in sys.modules)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
