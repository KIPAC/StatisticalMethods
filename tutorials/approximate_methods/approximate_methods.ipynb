{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd98a3e0-f0d1-48e9-8efa-4595828e67dd",
   "metadata": {
    "id": "dd98a3e0-f0d1-48e9-8efa-4595828e67dd"
   },
   "source": [
    "# Tutorial: Approximate Methods\n",
    "\n",
    "In this notebook, you'll practice using the approximate methods covered in the notes. To make it as straightforward as possible, we'll start with the contrived problem of fitting a line to data under conditions where Ordinary Least Squares (OLS) provides an exact solution to compare to. Then we'll move on the the contrived problem of fitting a line _and_ scatter, where suddenly that simple exact solution no longer works.\n",
    "\n",
    "You will\n",
    "* implement the simplest possible version of Approximate Bayes,\n",
    "* see how it works (or doesn't) for different choices of summary statistics and distance metrics,\n",
    "* perform the Laplace Approximation for comparison,\n",
    "* perform a frequentist bootstrap with the same model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27377b3-6c82-44ee-9aff-8fb6c5ce7c24",
   "metadata": {
    "id": "e27377b3-6c82-44ee-9aff-8fb6c5ce7c24"
   },
   "outputs": [],
   "source": [
    "# !pip install incredible lrgs\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import exists as file_exists\n",
    "from yaml import safe_load\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import incredible as cr\n",
    "import lrgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdda81c-a784-4bca-879a-e8f605e40ec5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4cdda81c-a784-4bca-879a-e8f605e40ec5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d92f92f2919cea14a2a4b25177cc860",
     "grade": true,
     "grade_id": "datapath",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thisTutorial = 'approximate_methods'\n",
    "if getcwd() == '/content':\n",
    "    # assume we are in Colab, and the user's data directory is linked to their drive/Physics267_data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    datapath = '/content/drive/MyDrive/Physics267_data/' + thisTutorial + '/'\n",
    "else:\n",
    "    # assume we are running locally somewhere and have the data under ./data/\n",
    "    datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b9b67-d6fb-46b7-b6fd-56ffc2e19027",
   "metadata": {
    "id": "e62b9b67-d6fb-46b7-b6fd-56ffc2e19027"
   },
   "source": [
    "## Case: ordinary least squares\n",
    "\n",
    "The classic OLS problem corresponds to the following scenario and assumptions:\n",
    "* We have data in the form of a list of $(x,y)$ pairs.\n",
    "* The $x$ values are fixed (we assume no uncertainty in their generation).\n",
    "* Each $y$ values is independently generated from a linear model: $y_i = a + b x_i + \\varepsilon_i$, where $\\varepsilon_i$ follows the standard normal distribution (zero mean and unit variance).\n",
    "* Priors on $a$ and $b$ are both uniform over the real line.\n",
    "\n",
    "You've probably already done it in a previous exercise, but draw the PGM and write down the generative model for this setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655c99a-9d4d-4b3c-a371-935e99d16046",
   "metadata": {
    "id": "7655c99a-9d4d-4b3c-a371-935e99d16046"
   },
   "source": [
    "> Space for your PGM and expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc7a83-3f14-4a80-a72b-abb1ca7a4357",
   "metadata": {
    "id": "94fc7a83-3f14-4a80-a72b-abb1ca7a4357"
   },
   "source": [
    "As usual, read in yout very own data set to work with, stored below as 1D `x` and `y` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28274d-55fa-4078-850e-b17c2b34d5bb",
   "metadata": {
    "id": "fe28274d-55fa-4078-850e-b17c2b34d5bb"
   },
   "outputs": [],
   "source": [
    "table = np.loadtxt(datapath+'data.txt')\n",
    "x = table[:,0]\n",
    "y = table[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571d38d-e6c8-410b-a16b-9fbd9dd0834e",
   "metadata": {
    "id": "2571d38d-e6c8-410b-a16b-9fbd9dd0834e"
   },
   "source": [
    "They look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ddce2-a790-4efa-86d6-0778e05f5490",
   "metadata": {
    "id": "148ddce2-a790-4efa-86d6-0778e05f5490"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.0, 3.0)\n",
    "plt.plot(x, y, '.');\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eff54-888a-4253-98b6-797959a620fa",
   "metadata": {
    "id": "a18eff54-888a-4253-98b6-797959a620fa"
   },
   "source": [
    "As you might know, the strong assumptions in OLS make the calculation of the posterior algebraically solvable. This is convenient, as we can easily compute the exact posterior and compare our approximate results to it. The calculations are not especially hard to implement manually, but we will nevertheless let the `statsmodels` package do them for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedac32e-ec39-4e30-9321-f874ba6b75be",
   "metadata": {
    "id": "cedac32e-ec39-4e30-9321-f874ba6b75be"
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(x))\n",
    "ols = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1760-d11b-401e-8fbb-38490457e995",
   "metadata": {
    "id": "db8f1760-d11b-401e-8fbb-38490457e995"
   },
   "source": [
    "The posterior, $p(a,b|x,y)$, is a 2D Gaussian with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd245652-06b5-4947-a59d-ff2e98302946",
   "metadata": {
    "id": "cd245652-06b5-4947-a59d-ff2e98302946"
   },
   "outputs": [],
   "source": [
    "ols.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2892219-fea9-4e5a-b71d-b35013fd2c93",
   "metadata": {
    "id": "d2892219-fea9-4e5a-b71d-b35013fd2c93"
   },
   "source": [
    "and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a7162-8beb-4490-96d4-0d53fdfa538f",
   "metadata": {
    "id": "686a7162-8beb-4490-96d4-0d53fdfa538f"
   },
   "outputs": [],
   "source": [
    "ols.normalized_cov_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e67f91-c160-4a02-9938-faabe5fbb937",
   "metadata": {
    "id": "91e67f91-c160-4a02-9938-faabe5fbb937"
   },
   "outputs": [],
   "source": [
    "np.sqrt(ols.normalized_cov_params.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c333e5-82bd-4957-87af-f0ed86a312bb",
   "metadata": {
    "id": "b1c333e5-82bd-4957-87af-f0ed86a312bb"
   },
   "source": [
    "Below we codify this by storing frozen `st.norm` objects representing the 1D marginalized posteriors, and ellipses deliniating the 2D credible regions, for ease of comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c5b0c-c776-4088-8736-150bdb9921ab",
   "metadata": {
    "id": "ed4c5b0c-c776-4088-8736-150bdb9921ab"
   },
   "outputs": [],
   "source": [
    "param_names = ['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aab1c2-5298-45f8-9a2e-f255b894b38c",
   "metadata": {
    "id": "98aab1c2-5298-45f8-9a2e-f255b894b38c"
   },
   "outputs": [],
   "source": [
    "true_post = {'1D':{'a':st.norm(ols.params[0], np.sqrt(ols.normalized_cov_params[0,0])),\n",
    "                   'b':st.norm(ols.params[1], np.sqrt(ols.normalized_cov_params[1,1]))},\n",
    "             '2D':{'68.3':cr.cov_ellipse(ols.normalized_cov_params, center=ols.params, level=0.68268949),\n",
    "                   '95.4':cr.cov_ellipse(ols.normalized_cov_params, center=ols.params, level=0.95449974)}\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cfba3-374e-4adb-8709-61a3f547907e",
   "metadata": {
    "id": "202cfba3-374e-4adb-8709-61a3f547907e"
   },
   "source": [
    "### Laplace Approximation\n",
    "\n",
    "In this excrutiatingly simple example, the true posterior is Gaussian, and so the Laplace Approximation should give us the exact posterior! Let's see.\n",
    "\n",
    "Recall that to implement the LA, you will need to\n",
    "1. find the maximum of the posterior, and\n",
    "2. estimate the matrix of second derivatives of the log-posterior at that point.\n",
    "\n",
    "For OLS, `statsmodels` effectively did this for us algebraically. Since that isn't an option in general, below you will use `scipy.optimize.minimize` to accomplish the same thing. First, we will need a function returning $-\\ln p(a,b|x,y)$; minimizing this will accomplish (1) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027250c-1694-40c1-9751-d7f559a4e57f",
   "metadata": {
    "deletable": false,
    "id": "5027250c-1694-40c1-9751-d7f559a4e57f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff0bd15684581c34fe804477d17feb1d",
     "grade": false,
     "grade_id": "lnp_func",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minus_lnp(params, x, y):\n",
    "    # return -ln(posterior) given params, a 1D np.array [a,b]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4951fff-cf46-4a99-93c1-2a71d2882d2d",
   "metadata": {
    "id": "d4951fff-cf46-4a99-93c1-2a71d2882d2d"
   },
   "source": [
    "We also need a guess at the parameter values, for the minimizer to use as a starting point. You could choose something by inspection of the plot above. Or, being confident that the fitter will manage this problem well enough, we could just start at $(0,0)$, which is not a million miles away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36c908-fcfc-41fe-b991-dcdb476a2605",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7f36c908-fcfc-41fe-b991-dcdb476a2605",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21669a0ab1ef6218549107fc3ca6ed23",
     "grade": true,
     "grade_id": "lnp_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "guess = [0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36dcf65-2508-4d31-b462-df79ad1466ec",
   "metadata": {
    "id": "d36dcf65-2508-4d31-b462-df79ad1466ec"
   },
   "source": [
    "Now we turn things over to `scipy`. If you're not familiar, there are a handful of algorithms that `minimize` can use. We'd like one that internally estimates and uses the second derivatives of the function being minimized, since it means we can just get those derivatives about the minimum from the fitter instead of estimating them ourselves. The default algorithm does this, though note that this is not true of all the options, and the specific method for accessing the derivatives varies (see the documentation for `minimize` for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca704b-09ec-4a5e-8442-5d71e37cfa76",
   "metadata": {
    "id": "cfca704b-09ec-4a5e-8442-5d71e37cfa76"
   },
   "outputs": [],
   "source": [
    "LA = minimize(minus_lnp, guess, args=(x,y))\n",
    "LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975daff-a6e8-4592-9872-937e9a4bfe5a",
   "metadata": {
    "id": "0975daff-a6e8-4592-9872-937e9a4bfe5a"
   },
   "source": [
    "The posterior mode is stored in `LA.x`, and should be _extremely_ close to the algebraic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb28fa5-61a8-4cd1-aa83-1de82811abe1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "acb28fa5-61a8-4cd1-aa83-1de82811abe1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71d0f8b7364f064f01bc8521b97d690c",
     "grade": true,
     "grade_id": "LA_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(LA.x, ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9245e3-772f-41c4-bc13-404be0ac4a66",
   "metadata": {
    "id": "5f9245e3-772f-41c4-bc13-404be0ac4a66"
   },
   "source": [
    "The inverse-Hessian, which is the posterior covariance matrix of the parameters, as helpfully provided in `LA.hess_inv`, and shown above. It, too, should be extremely close to the OLS calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65668d-adcb-4b36-a3d1-18d789dbc8bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb65668d-adcb-4b36-a3d1-18d789dbc8bc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d139001b52ab5e3ca618744b2aa138f0",
     "grade": true,
     "grade_id": "LA_covtest",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(LA.hess_inv, ols.normalized_cov_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7e83d-0bcc-44f9-88ce-bc560f094176",
   "metadata": {
    "id": "76b7e83d-0bcc-44f9-88ce-bc560f094176"
   },
   "source": [
    "Let's go ahead and visualize the usual 2D credible regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12349afd-20e3-4bfb-a96a-86d4f05c7b6d",
   "metadata": {
    "id": "12349afd-20e3-4bfb-a96a-86d4f05c7b6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.0, 3.0)\n",
    "plt.plot(true_post['2D']['68.3'][0], true_post['2D']['68.3'][1], '-', color='b', label='OLS');\n",
    "plt.plot(true_post['2D']['95.4'][0], true_post['2D']['95.4'][1], '-', color='b');\n",
    "cr.cov_ellipse(LA.hess_inv, center=LA.x, level=0.68268949, plot=plt, fmt='--', color='r', label=\"LA\");\n",
    "cr.cov_ellipse(LA.hess_inv, center=LA.x, level=0.95449974, plot=plt, fmt='--', color='r');\n",
    "plt.xlabel(\"a\"); plt.ylabel(\"b\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8638ca8-eab2-4c5a-9bc0-b24dd2481bba",
   "metadata": {
    "id": "b8638ca8-eab2-4c5a-9bc0-b24dd2481bba",
    "tags": []
   },
   "source": [
    "## Approximate Bayesian Computation\n",
    "\n",
    "Next, we turn to ABC. Below, you'll implement the simplest version of this algorithm:\n",
    "1. Choose a set of summary statistics that encode important features of the data.\n",
    "2. Choose a distance function providing a metric for comparing summary statistics computed from different data, and a distance threshold.\n",
    "3. For many iterations,\n",
    "* sample a set of parameters from the prior;\n",
    "* simulate a mock data set from those parameters using the generative model;\n",
    "* compute the summary statistics from those mock data;\n",
    "* if the distance between that summary and the summary statistics computed on the real data is smaller than the threshold, accept the parameter sample.\n",
    "\n",
    "Needless to say, this is not the most efficient version of ABC, but it will serve. However, there is an immediate, practical issue, namely that we chose infinite, uniform priors above in order to make the problem equivalent to OLS. Clearly we can't sample from the full interval $(-\\infty,\\infty)$. In any case, the credible regions above are tiny compared with the range of finite numbers that we can, in principle, draw from. So a naive implementation can be expected to take an exceptionally long time to produce even a single acceptable sample. (This is a good argument for choosing sensible priors, if you needed another one.)\n",
    "\n",
    "We will inelegantly work around this issue by remembering that, for parameter estimation, we only really need to consider the prior over the region of parameter space where the posterior will be non-tiny. Seeing how we have an approximate posterior from LA, let's use it to sneakily redefine the functional bounds of the priors for this exercise. Truncating $p(a)$ and $p(b)$ at the 0.0001 and 0.9999 quantiles of their respective marginalized posteriors seems to work well in this case.\n",
    "\n",
    "As we've done before, store the priors for $a$ and $b$ as frozen `scipy.stats` distributions in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facb4a6-01c4-480e-a11e-de5e99041f3b",
   "metadata": {
    "deletable": false,
    "id": "5facb4a6-01c4-480e-a11e-de5e99041f3b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "628b754eb859b5c3aca2fc451dea9ff4",
     "grade": false,
     "grade_id": "priors",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# priors = {'a':..., 'b':...}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1e309-eefc-4b32-9b6f-2add3cf7288b",
   "metadata": {
    "id": "23c1e309-eefc-4b32-9b6f-2add3cf7288b"
   },
   "source": [
    "To make it simpler to try out different summary statistics or distance metrics, we'll use classes and inheritance to package up reusable code. The cell below has just one method for you to complete, namely the one that performs the loop in the pseudocode above. **Do not** include an accept/reject step here (that is, accept all samples); instead, record the distance for each sample. That way, we can easily look at how the choice distance threshold affects our results later.\n",
    "\n",
    "Also, try to make your code general enough that we can reuse it with another generative model and data set later on; that is, don't reference variables at global scope if they're stored in the `ABC` object already by its constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f37e2-2e47-46c1-a48a-9bc646060cea",
   "metadata": {
    "deletable": false,
    "id": "b98f37e2-2e47-46c1-a48a-9bc646060cea",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3fc1839d35f678fa5e2733f44dd680a",
     "grade": false,
     "grade_id": "ABC_class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ABC:\n",
    "    def __init__(self, x, ytrue, priors):\n",
    "        self.x = x\n",
    "        self.y = ytrue\n",
    "        self.s_true = self.summaries(x, ytrue)\n",
    "        self.priors = priors\n",
    "        self.param_names = [p for p in priors.keys()]\n",
    "    # derived classes must define the following\n",
    "    def summaries(self, x, y):\n",
    "        # return summary statistics as a 1D np.array\n",
    "        raise Exception('ABC::summaries should be defined by a deriving class')\n",
    "    def distance(self, s1, s2):\n",
    "        # return scalar distance between 2 sets of summary statistics\n",
    "        raise Exception('ABC::distance should be defined by a deriving class')\n",
    "    def simulate(self):\n",
    "        # return a simulated 1D array y, given self.x and additional parameters arguments (with default values)\n",
    "        raise Exception('ABC::simulate should be defined by a deriving class')\n",
    "    # the important part\n",
    "    def run(self, Nsim):\n",
    "        self.samples = np.full((Nsim, len(self.param_names)), np.nan)\n",
    "        self.distances = np.full(Nsim, np.nan)\n",
    "        for i in range(Nsim):\n",
    "            params = {p:self.priors[p].rvs() for p in self.param_names}\n",
    "            ysim = self.simulate(**params)\n",
    "            # compute summaries and distance from self.s_true\n",
    "            # store params in the ith row of self.samples\n",
    "            # store distance in the ith entry of self.distances\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    # useful visualizations\n",
    "    def distance_hist(self):\n",
    "        plt.rcParams['figure.figsize'] = (4.0, 3.0)\n",
    "        plt.hist(self.distances);\n",
    "        plt.xlabel(\"d\");\n",
    "    def select_distances(self, eps):\n",
    "        self.selection = np.flatnonzero(self.distances < eps)\n",
    "        self.samples2 = self.samples[self.selection,:]\n",
    "        print('Acceptance rate:', self.samples2.shape[0]/self.samples.shape[0], '(', self.samples2.shape[0], 'accepted samples )')\n",
    "    def plot_acceptances(self, truth=None, truth_tri=None, size=8.0, show_rejects=True):\n",
    "        plt.rcParams['figure.figsize'] = (size, size)\n",
    "        n = len(self.param_names)\n",
    "        fig = plt.figure()\n",
    "        axes = []\n",
    "        for i,p in enumerate(self.param_names):\n",
    "            axes.append([])\n",
    "            ax = fig.add_subplot(n, n, (i*n)+i+1)\n",
    "            ax0 = ax\n",
    "            ax.hist(self.samples2[:,i], density=True, label='ABC');\n",
    "            if truth is not None:\n",
    "                aa = np.linspace(*self.priors[p].ppf([0.0001,0.9999]), 100)\n",
    "                ax.plot(aa, truth['1D'][p].pdf(aa), label='truth');\n",
    "            if i==n-1:\n",
    "                ax.set_xlabel(p);\n",
    "            else:\n",
    "                ax.get_xaxis().set_ticklabels([])\n",
    "            ax.get_yaxis().set_ticklabels([])\n",
    "            for j,q in enumerate(self.param_names):\n",
    "                if j == i:\n",
    "                    break\n",
    "                ax = fig.add_subplot(n, n, (i*n)+j+1)\n",
    "                axes[-1].append(ax)\n",
    "                if show_rejects:\n",
    "                    ax.plot(self.samples[:,j], self.samples[:,i], ',', color='C2');\n",
    "                ax.plot(self.samples2[:,j], self.samples2[:,i], '.');\n",
    "                if truth is not None:\n",
    "                    for k in truth['2D'].keys():\n",
    "                        ax.plot(truth['2D'][k][0], truth['2D'][k][1], color='C1', label='truth');\n",
    "                if i==n-1:\n",
    "                    ax.set_xlabel(q);\n",
    "                else:\n",
    "                    ax.get_xaxis().set_ticklabels([])\n",
    "                if j==0:\n",
    "                    ax.set_ylabel(p);\n",
    "                else:\n",
    "                    ax.get_yaxis().set_ticklabels([])\n",
    "            axes[-1].append(ax0)\n",
    "        if truth_tri is not None:\n",
    "             cr.whist_triangle_plot(truth_tri, axes=axes, linecolor1D='C1', linecolor2D='C1', fill2D=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefd765-4800-432b-96fb-6f12c881e891",
   "metadata": {
    "id": "dbefd765-4800-432b-96fb-6f12c881e891"
   },
   "source": [
    "### Arbitrary summary statistics\n",
    "\n",
    "Next, we need to choose summary statistics and a distance function. Given that this problem has an algebraic solution, it's very tempting to use that solution to inform both of these choices, and we will do so below. First, however, pretend that we didn't have an exact solution to this problem to consult. **In class, we will jointly decide on summary statistics and a distance function to use in this part.** (The public solutions use Euclidean distance throughout this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d660391-8855-49f1-91da-fd3a9f2b9919",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7d660391-8855-49f1-91da-fd3a9f2b9919",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "807353fe4d84a9d5db07edc086d2c595",
     "grade": true,
     "grade_id": "ABC_ours_soln",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Any notes about these decisions can go here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837099e2-ae4a-40f6-b2ff-50f798e29f93",
   "metadata": {
    "id": "837099e2-ae4a-40f6-b2ff-50f798e29f93"
   },
   "source": [
    "We can define a derived class of `ABC` that implements these choices by specifying just the 3 functions that were left out above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db3526-5c79-4b94-aca9-33fc337409de",
   "metadata": {
    "deletable": false,
    "id": "c6db3526-5c79-4b94-aca9-33fc337409de",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "870d69b8544de9454d274496aedeb77b",
     "grade": false,
     "grade_id": "ABC_ours",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ABC_ours(ABC):\n",
    "#     def summaries(self, x, y):\n",
    "#         # return summary statistics as a 1D np.array\n",
    "#     def distance(self, s1, s2):\n",
    "#         # return scalar distance between 2 sets of summary statistics\n",
    "#     def simulate(self, a=0.0, b=0.0):\n",
    "#         # return a simulated 1D array y, given self.x and additional parameters arguments (with default values)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "abc = ABC_ours(x, y, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3e7bd-c192-44e7-937c-49d54ceb968b",
   "metadata": {
    "id": "3ab3e7bd-c192-44e7-937c-49d54ceb968b"
   },
   "source": [
    "Check that it doesn't crash when trying to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5e27f-1e36-46a5-aa5a-c58d69881412",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "05c5e27f-1e36-46a5-aa5a-c58d69881412",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8163cb8d82667929bb68f96491c26d55",
     "grade": true,
     "grade_id": "abc_def",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "abc.run(1)\n",
    "print(abc.samples)\n",
    "print(abc.distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5856656-9c62-45e1-8194-562147dc88ac",
   "metadata": {
    "id": "a5856656-9c62-45e1-8194-562147dc88ac"
   },
   "source": [
    "If so, let's produce a good number of samples and associated distances (this takes <10s on my aging laptop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bebec-1b8f-492b-8d08-6ef781b04c31",
   "metadata": {
    "id": "377bebec-1b8f-492b-8d08-6ef781b04c31"
   },
   "outputs": [],
   "source": [
    "%time abc.run(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47b616-d647-4f93-9cfe-e7895a5d9c60",
   "metadata": {
    "id": "2a47b616-d647-4f93-9cfe-e7895a5d9c60"
   },
   "source": [
    "You might be wondering why we haven't said anything about the distance threshold for accepting samples yet. The reason is, simply, that the absolute magnitude of threshold that makes sense depends on both the choice of summary statistics (and their dimensionality) and the choice of distance function. For most sensible distance definitions, zero is the ideal value, but it makes sense to first take a look at the distribution of distances corresponding to our simulated data sets so far (using this handy provided function!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b6188-13e6-47de-be46-18c1883e6c12",
   "metadata": {
    "id": "5e8b6188-13e6-47de-be46-18c1883e6c12"
   },
   "outputs": [],
   "source": [
    "abc.distance_hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd0c24-7c17-4573-9a61-40ddffa5880d",
   "metadata": {
    "id": "05dd0c24-7c17-4573-9a61-40ddffa5880d"
   },
   "source": [
    "The cells below visualize how the accepted samples change as we reduce the threshold from the median of the distribution above to the first percentile. Each one shows a \"triangle\" plot comparing the exact posterior (orange) to the distribution of accepted samples (blue). The rejected samples are shown in green in the off-diagonal panel, just to verify that they fill the space uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d4af4-5d7f-4c63-a022-fe3bd9b6bb1a",
   "metadata": {
    "id": "559d4af4-5d7f-4c63-a022-fe3bd9b6bb1a"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc.distances, 0.5)\n",
    "print(\"eps =\", eps)\n",
    "abc.select_distances(eps)\n",
    "abc.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f1141-69d3-4f68-80eb-0ce211161a02",
   "metadata": {
    "id": "dd9f1141-69d3-4f68-80eb-0ce211161a02"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc.distances, 0.1)\n",
    "print(\"eps =\", eps)\n",
    "abc.select_distances(eps)\n",
    "abc.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448d24f-6155-4936-93d0-cd72a2043bd0",
   "metadata": {
    "id": "9448d24f-6155-4936-93d0-cd72a2043bd0"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc.distances, 0.1)\n",
    "print(\"eps =\", eps)\n",
    "abc.select_distances(eps)\n",
    "abc.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e9058-01b6-419f-8aba-2f1aa7a2fb6a",
   "metadata": {
    "id": "dd5e9058-01b6-419f-8aba-2f1aa7a2fb6a"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc.distances, 0.01)\n",
    "print(\"eps =\", eps)\n",
    "abc.select_distances(eps)\n",
    "abc.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad65ef9-4ee6-4ec8-adbe-d7c24904c30c",
   "metadata": {
    "id": "1ad65ef9-4ee6-4ec8-adbe-d7c24904c30c"
   },
   "source": [
    "Depending on the choices of summary statistic (especially) and distance function, you may or may not see the ABC samples converging to the truth as the threshold is reduced. Comment on the outcome, and how it might motivate different choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65db0e-6639-4d19-a2f1-3d20c5e869b2",
   "metadata": {
    "id": "3d65db0e-6639-4d19-a2f1-3d20c5e869b2"
   },
   "source": [
    "> Space to respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b4212-99d0-44bc-8ab6-4e21da0e8aa5",
   "metadata": {
    "deletable": false,
    "id": "904b4212-99d0-44bc-8ab6-4e21da0e8aa5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a87df3a26c06d5379d6bf89a80be9d",
     "grade": false,
     "grade_id": "comments",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_have_recorded_my_thoughts = False # change to True when it is true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ac0a1-c049-442d-a252-1c4bd843f56a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "047ac0a1-c049-442d-a252-1c4bd843f56a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d4507ffbf77a7e674a6e5eadb315ce4",
     "grade": true,
     "grade_id": "comments_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert I_have_recorded_my_thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1afb1-30a1-4365-aad7-18f6dd7f8458",
   "metadata": {
    "id": "e1d1afb1-30a1-4365-aad7-18f6dd7f8458"
   },
   "source": [
    "### Optimal summary statistics\n",
    "\n",
    "Depending on what summaries we decided to use above, the previous section's ABC may have worked extremely poorly. If so, this hopefully impresses on you the importance of thinking these decisions through. If not, have a look at the web version of this solved notebook to see how a deliberately poor choice works out.\n",
    "\n",
    "Either way, we will now run an ABC using the optimal summary statistics for this particular problem, which are the OLS estimators of $a$ and $b$ (surprise!). We will not change the distance function initially.\n",
    "\n",
    "In code, we can derive a new class from `ABS_ours` (which has appropriate `distance` and `simulate` methods already), and overload (i.e. replace) only the `summaries` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4147e-a546-4fec-bece-8aecb6353729",
   "metadata": {
    "deletable": false,
    "id": "94f4147e-a546-4fec-bece-8aecb6353729",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e6681b029d8540095663fa40dc3726e",
     "grade": false,
     "grade_id": "ABC_OLS",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ABC_OLS(ABC_ours):\n",
    "#     def summaries(self, x, y):\n",
    "#         # return summary statistics, the OLS estimators of a and b, as a 1D np.array\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "abc1 = ABC_OLS(x, y, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c349e82-5daa-4848-b3e2-f21053c936e9",
   "metadata": {
    "id": "7c349e82-5daa-4848-b3e2-f21053c936e9"
   },
   "source": [
    "As a quick check, this object should give us back the OLS estimators we got at the beginning of the notebook if we run `summaries` on the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba44f77-8d10-4653-ba36-e67147905521",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dba44f77-8d10-4653-ba36-e67147905521",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a46456441d9538a25446702514437bc2",
     "grade": true,
     "grade_id": "ABC_OLS_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert all(ols.params == abc1.summaries(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fad5ee-0eda-4ff3-897a-b8ffa8ef75b9",
   "metadata": {
    "id": "10fad5ee-0eda-4ff3-897a-b8ffa8ef75b9"
   },
   "source": [
    "Let's run and see how these results converge as the threshold is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd0eb0-9917-421e-9f40-963a4251ffec",
   "metadata": {
    "id": "2cbd0eb0-9917-421e-9f40-963a4251ffec"
   },
   "outputs": [],
   "source": [
    "%time abc1.run(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdcefe-3ff3-402f-b265-820482649d54",
   "metadata": {
    "id": "0ebdcefe-3ff3-402f-b265-820482649d54"
   },
   "outputs": [],
   "source": [
    "abc1.distance_hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dac416-acb8-429c-abca-c7c07d618e1f",
   "metadata": {
    "id": "87dac416-acb8-429c-abca-c7c07d618e1f"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc1.distances, 0.5)\n",
    "print(\"eps =\", eps)\n",
    "abc1.select_distances(eps)\n",
    "abc1.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ae744-f842-49d6-a7c3-9f532b3815de",
   "metadata": {
    "id": "451ae744-f842-49d6-a7c3-9f532b3815de"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc1.distances, 0.1)\n",
    "print(\"eps =\", eps)\n",
    "abc1.select_distances(eps)\n",
    "abc1.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a6471-6264-4007-83cd-a58793a8167a",
   "metadata": {
    "id": "9d6a6471-6264-4007-83cd-a58793a8167a"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc1.distances, 0.05)\n",
    "print(\"eps =\", eps)\n",
    "abc1.select_distances(eps)\n",
    "abc1.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15e3a2-9778-40a9-80f9-cefa7d138202",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eb15e3a2-9778-40a9-80f9-cefa7d138202",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdbd5f1799d2bef9fbbba08218fb9cc3",
     "grade": true,
     "grade_id": "ols2_def",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc1.distances, 0.01)\n",
    "print(\"eps =\", eps)\n",
    "abc1.select_distances(eps)\n",
    "abc1.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d055d-6b79-4cc3-b835-cf5e52a1b6c7",
   "metadata": {
    "id": "094d055d-6b79-4cc3-b835-cf5e52a1b6c7"
   },
   "source": [
    "Unless we've chosen a particularly strange distance function, you should see the ABC samples converging to the truth, but not equally quickly in both parameters. We can do better than this! If we have a guess of the width of the posterior in each parameter, it would make sense to incorporate those scales into the distance function. That is, we could redefine the distance such that\n",
    "\n",
    "$d(\\Delta \\hat{a}, \\Delta \\hat{b}) \\rightarrow d\\left(\\frac{\\Delta\\hat{a}}{\\sigma_a}, \\frac{\\Delta\\hat{b}}{\\sigma_b}\\right)$.\n",
    "\n",
    "Here $\\hat{a}$ and $\\hat{b}$ are our summary statistics, and $\\sigma_a$ and $\\sigma_b$ are estimates of the width of the posterior in each parameter (the marginalized standard deviations of our posterior estimate). Even better would be to use the 2D covariance estimate, but this simpler modification will suffice (not to mention, we're already using a lot of information we might not have in general).\n",
    "\n",
    "Put this modification into code by deriving a new class inheriting from `ABC_OLS` and overloading the `distances` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21db23-0a47-4ea9-900f-6744ee5c7959",
   "metadata": {
    "deletable": false,
    "id": "ad21db23-0a47-4ea9-900f-6744ee5c7959",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d6b52d4a54d3f21450189ccc3b5b40e",
     "grade": false,
     "grade_id": "ABC_OLS2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ABC_OLS2(ABC_OLS):\n",
    "#     def distance(self, s1, s2):\n",
    "#         # return scalar distance between 2 sets of summary statistics\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "abc2 = ABC_OLS2(x, y, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca5579-80d3-47a5-9988-06bbc36b6b5c",
   "metadata": {
    "id": "baca5579-80d3-47a5-9988-06bbc36b6b5c"
   },
   "source": [
    "Quick check for crashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7406252-0825-4971-8413-579a72ffd727",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e7406252-0825-4971-8413-579a72ffd727",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a01051c898816210835ee40816d54505",
     "grade": true,
     "grade_id": "distance_test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "abc2.run(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d4f3-f48c-4482-ac88-b999dd52b041",
   "metadata": {
    "id": "09d6d4f3-f48c-4482-ac88-b999dd52b041"
   },
   "source": [
    "Again, let's produce a bunch of samples and see how the convergence looks as the distance threshold is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efb74a-2282-4f1e-83c0-a1ebe3963163",
   "metadata": {
    "id": "a5efb74a-2282-4f1e-83c0-a1ebe3963163"
   },
   "outputs": [],
   "source": [
    "%time abc2.run(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a94dc-f6ce-45cf-b654-f50c016d9686",
   "metadata": {
    "id": "f05a94dc-f6ce-45cf-b654-f50c016d9686"
   },
   "outputs": [],
   "source": [
    "abc2.distance_hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534e21e-5673-4dbf-b6da-3507c3d7a865",
   "metadata": {
    "id": "1534e21e-5673-4dbf-b6da-3507c3d7a865"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc2.distances, 0.5)\n",
    "print(\"eps =\", eps)\n",
    "abc2.select_distances(eps)\n",
    "abc2.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da2793-ae92-4f05-a287-51781dc4ae99",
   "metadata": {
    "id": "33da2793-ae92-4f05-a287-51781dc4ae99"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc2.distances, 0.1)\n",
    "print(\"eps =\", eps)\n",
    "abc2.select_distances(eps)\n",
    "abc2.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985daba0-9bc0-4317-b786-46063a209e65",
   "metadata": {
    "id": "985daba0-9bc0-4317-b786-46063a209e65"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc2.distances, 0.05)\n",
    "print(\"eps =\", eps)\n",
    "abc2.select_distances(eps)\n",
    "abc2.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234b50c-705c-4e45-a11f-d08849762f48",
   "metadata": {
    "id": "a234b50c-705c-4e45-a11f-d08849762f48"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc2.distances, 0.01)\n",
    "print(\"eps =\", eps)\n",
    "abc2.select_distances(eps)\n",
    "abc2.plot_acceptances(truth=true_post, size=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d55a1-eeba-47a6-a339-4c98c028b32c",
   "metadata": {
    "id": "613d55a1-eeba-47a6-a339-4c98c028b32c"
   },
   "source": [
    "You should now see a progression that plausibly looks like it's smoothly approaching the true posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256adb04-175e-44ce-b897-9f3e3b2f257a",
   "metadata": {
    "deletable": false,
    "id": "256adb04-175e-44ce-b897-9f3e3b2f257a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bb7c6f2e884181692d890585d4c58a5",
     "grade": false,
     "grade_id": "abc2_check",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Yes_I_totally_see_that = False # change to True when true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84eb089-8a41-4cae-8b02-c81abd5f5eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a84eb089-8a41-4cae-8b02-c81abd5f5eb7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfc766d958d1808336932ea563e4f167",
     "grade": true,
     "grade_id": "abc2_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert Yes_I_totally_see_that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0c201-8639-418c-8a93-afac5f7c6af7",
   "metadata": {
    "id": "b4e0c201-8639-418c-8a93-afac5f7c6af7"
   },
   "source": [
    "## Case: linear model with unknown scatter\n",
    "\n",
    "Now that you have some experience with the nuts and bolts of LA and ABC, we'll apply them to a situation that's only slightly more complex, but doesn't have a pretty exact solution. Namely,\n",
    "\n",
    "* We have data in the form of a list of $(x,y)$ pairs.\n",
    "* The $x$ values are fixed (we assume no uncertainty in their generation).\n",
    "* Each $y$ values is independently generated from a linear model: $y_i = a + b x_i + \\varepsilon_i$, where $\\varepsilon_i$ follows the a normal distribution with zero mean and unknown variance, $\\sigma^2$, that we will fit for.\n",
    "* Priors on $a$ and $b$ are both uniform over the real line; the prior on $\\sigma^2$ is uniform over non-negative values.\n",
    "\n",
    "Comparing with above, the key difference here is that the scatter of $y$ about the linear model is a free parameter that we will fit for, in addition to the slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bbd36-6eb6-446d-9990-2e934cf1f196",
   "metadata": {
    "id": "bd5bbd36-6eb6-446d-9990-2e934cf1f196"
   },
   "outputs": [],
   "source": [
    "param_names2 = param_names + ['s2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa6634-167b-43a8-8cd0-a46577ef9946",
   "metadata": {
    "id": "b1aa6634-167b-43a8-8cd0-a46577ef9946"
   },
   "source": [
    "As always, draw and write down the generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5bc24-80a7-4887-8051-8ae50c8682f4",
   "metadata": {
    "id": "bbe5bc24-80a7-4887-8051-8ae50c8682f4"
   },
   "source": [
    "> Space for the generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a54b9-9a78-46d8-8ee3-598fd68d7de6",
   "metadata": {
    "id": "6e0a54b9-9a78-46d8-8ee3-598fd68d7de6"
   },
   "source": [
    "To keep things from being too boring, we will read in a second data set for which $\\sigma^2 \\neq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c237ce-51a0-4558-88e2-aba34aa570bf",
   "metadata": {
    "id": "32c237ce-51a0-4558-88e2-aba34aa570bf"
   },
   "outputs": [],
   "source": [
    "table2 = np.loadtxt(datapath+'data2.txt')\n",
    "x2 = table2[:,0]\n",
    "y2 = table2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15201e74-b58f-4f74-bd70-42a30e952dce",
   "metadata": {
    "id": "15201e74-b58f-4f74-bd70-42a30e952dce"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.0, 3.0)\n",
    "plt.plot(x2, y2, '.');\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bacaf-c448-4a47-bf28-8ee179164537",
   "metadata": {
    "id": "aa5bacaf-c448-4a47-bf28-8ee179164537"
   },
   "source": [
    "While there isn't a simple exact solution for this scenario, it would still be nice to compare to a solution we're confident in. Fortunately, this model, for data like those we're using, is easily handled with conjugate Gibbs sampling. So we'll do that quickly before moving on.\n",
    "\n",
    "We are extremely confident that `LRGS` can handle this problem without oversight, but since your data are randomly generated there is no 100% guarantee. If things look weird below and you want to look closer, the commented cell below will show the traces. Without further commentary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131b2e0-8c97-4d73-b8cf-390a75982006",
   "metadata": {
    "id": "c131b2e0-8c97-4d73-b8cf-390a75982006"
   },
   "outputs": [],
   "source": [
    "par = lrgs.Parameters(np.matrix([x2]).T, np.matrix([y2]).T, M_inv=[np.asmatrix(np.zeros(2)) for i in range(len(x2))], Sigma_prior=(-2., np.matrix(np.zeros((1,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4c6ed-b592-4e82-8d42-875766e3f680",
   "metadata": {
    "id": "97b4c6ed-b592-4e82-8d42-875766e3f680"
   },
   "outputs": [],
   "source": [
    "chain = lrgs.Chain(par, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d105b0-6101-4cdd-9bd4-0b4673320e9a",
   "metadata": {
    "id": "e1d105b0-6101-4cdd-9bd4-0b4673320e9a"
   },
   "outputs": [],
   "source": [
    "%time chain.run(fix='y') # <10s on an old laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76f414-e2f2-4903-91db-52c77bc096ca",
   "metadata": {
    "id": "2d76f414-e2f2-4903-91db-52c77bc096ca"
   },
   "outputs": [],
   "source": [
    "chdict = chain.to_dict(['B','Sigma'])\n",
    "charr = np.array([chdict[p] for p in ['B_0_0','B_1_0','Sigma_0_0']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7434f7b-4d6e-4aaf-8da8-501497b18427",
   "metadata": {
    "id": "e7434f7b-4d6e-4aaf-8da8-501497b18427"
   },
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = (12.0, 2.0*len(param_names2))\n",
    "#fig, ax = plt.subplots(len(param_names2), 1);\n",
    "#cr.plot_traces(charr, ax, labels=param_names2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0cd9e-6bff-466f-80d4-8ab3c33a7ba1",
   "metadata": {
    "id": "68a0cd9e-6bff-466f-80d4-8ab3c33a7ba1"
   },
   "outputs": [],
   "source": [
    "charr = charr[10:,:]\n",
    "tri = cr.whist_triangle(charr, bins=50, smooth2D=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb517c-03da-4f23-a303-8c7090f39251",
   "metadata": {
    "id": "17eb517c-03da-4f23-a303-8c7090f39251"
   },
   "outputs": [],
   "source": [
    "cr.whist_triangle_plot(tri, paramNames=param_names2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b9554-828a-4963-b6d8-63c11c96581e",
   "metadata": {
    "id": "f72b9554-828a-4963-b6d8-63c11c96581e"
   },
   "source": [
    "### Laplace Approximation\n",
    "\n",
    "Now that we hopefully have a reliable posterior from MCMC to compare to, let's try the LA for this problem. As before, write a function to return minus the log-posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf78d7-9a38-4e1f-b3ca-aec4daea8929",
   "metadata": {
    "deletable": false,
    "id": "0ccf78d7-9a38-4e1f-b3ca-aec4daea8929",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b76a1f1950f6a67ffa6431f423567e2d",
     "grade": false,
     "grade_id": "logpost2_func",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minus_lnp2(params, x, y):\n",
    "    # return -ln(posterior) given params, a 1D np.array [a,b,s2]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cb5c5-972f-49db-ae17-70ddd844cb01",
   "metadata": {
    "id": "4f0cb5c5-972f-49db-ae17-70ddd844cb01"
   },
   "source": [
    "Again, a very crude guess should work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cc6c5-5363-4f77-8abb-30cb6390e8ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "972cc6c5-5363-4f77-8abb-30cb6390e8ac",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa8de8109091a9003be46497ffe4456",
     "grade": true,
     "grade_id": "lnp2_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "guess2 = [0.0, 0.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce19f40-2017-455c-96aa-09aa3e8a55fd",
   "metadata": {
    "id": "1ce19f40-2017-455c-96aa-09aa3e8a55fd"
   },
   "source": [
    "Find the posterior maximum! Note that we need to use the `bounds` argument so that `minimize` knows that the `s2` parameter may not be negative. This causes it to internally choose a different algorithm than it would have used by default otherwise, with the upshot that we need to do something slightly different to access the inverse-Hessian (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d94a8-2a22-4902-87bf-d0f980f599d0",
   "metadata": {
    "id": "810d94a8-2a22-4902-87bf-d0f980f599d0"
   },
   "outputs": [],
   "source": [
    "LA2 = minimize(minus_lnp2, guess2, bounds=[(None,None), (None,None), (0.0,None)], args=(x2, y2))\n",
    "LA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86393fa0-353f-4da1-9cf3-057ec86a9ea9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "86393fa0-353f-4da1-9cf3-057ec86a9ea9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7383bc22b47f7a7adf26445784f45dad",
     "grade": true,
     "grade_id": "abc3_def",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA2_mean = LA2.x\n",
    "LA2_cov = LA2.hess_inv.todense()\n",
    "print(\"mean:\", LA2_mean)\n",
    "print(\"cov:\", LA2_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429171de-c3cb-47b2-872d-6326f499a787",
   "metadata": {
    "id": "429171de-c3cb-47b2-872d-6326f499a787"
   },
   "source": [
    "The cell below will compare the LA (red/dashed curves) with our MCMC posterior (blue/solid curves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8b01a-33eb-434d-9d66-67f81ed8b7f1",
   "metadata": {
    "id": "a8c8b01a-33eb-434d-9d66-67f81ed8b7f1"
   },
   "outputs": [],
   "source": [
    "fig,ax = cr.whist_triangle_plot(tri, paramNames=param_names2, linecolor1D='b', linecolor2D='b', fill2D=False);\n",
    "for i,p in enumerate(param_names2):\n",
    "    aa = np.linspace(LA2_mean[i]-4*np.sqrt(LA2_cov[i,i]), LA2_mean[i]+4*np.sqrt(LA2_cov[i,i]), 100)\n",
    "    ax[i][i].plot(aa, st.norm.pdf(aa, LA2_mean[i], np.sqrt(LA2_cov[i,i])), '--', color='r');\n",
    "    ax[i][i].set_xlim(min(ax[i][i].get_xlim()[0], LA2_mean[i]-4*np.sqrt(LA2_cov[i,i])), max(ax[i][i].get_xlim()[1], LA2_mean[i]+4*np.sqrt(LA2_cov[i,i])))\n",
    "    ax[i][i].set_ylim(0.0, max(ax[i][i].get_ylim()[1], st.norm.pdf(aa, LA2_mean[i], np.sqrt(LA2_cov[i,i])).max()*1.1))\n",
    "    for j,q in enumerate(param_names2):\n",
    "        if i==j:\n",
    "            break\n",
    "        cr.cov_ellipse(LA2_cov[np.ix_([j,i],[j,i])], center=LA2_mean[[j,i]], level=0.68268949, plot=ax[i][j], fmt='--', color='r')\n",
    "        cr.cov_ellipse(LA2_cov[np.ix_([j,i],[j,i])], center=LA2_mean[[j,i]], level=0.95449974, plot=ax[i][j], fmt='--', color='r')\n",
    "        ax[i][j].set_xlim(min(ax[i][j].get_xlim()[0], LA2_mean[j]-4*np.sqrt(LA2_cov[j,j])), max(ax[i][j].get_xlim()[1], LA2_mean[j]+4*np.sqrt(LA2_cov[j,j])))\n",
    "        ax[i][j].set_ylim(min(ax[i][j].get_ylim()[0], LA2_mean[i]-4*np.sqrt(LA2_cov[i,i])), max(ax[i][j].get_ylim()[1], LA2_mean[i]+4*np.sqrt(LA2_cov[i,i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6b248-a54c-4192-8b91-4f810c451a42",
   "metadata": {
    "id": "98f6b248-a54c-4192-8b91-4f810c451a42"
   },
   "source": [
    "For our data, the LA gets close to the posterior mode, but its estimate of the covariance leaves something to be desired. Note that, for this setup, we have found something of a phase change when the data set gets small enough that the scatter is tough to distinguish from zero - LA's covariance in that case suddenly becomes much larger. For bigger data sets that the one here, the approximation, including the covariance, gets better. We've deliberately aimed in the middle for this demonstration, which is to say that you might see any of this behaviour in your own, random data sets. (It's also possible that some of this is due to the specific way the Hessian is estimated within `minimize`, and a more careful estimate of the second derivatives would do better; we haven't investigated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce30cc8-b412-4f57-8e6a-0a2fa987b9dd",
   "metadata": {
    "id": "fce30cc8-b412-4f57-8e6a-0a2fa987b9dd"
   },
   "source": [
    "### Approximate Bayes\n",
    "\n",
    "As before, let's functionally redefine the range of our priors based on what you see above. In real life, you can imagine doing this based on the LA in the first place, and iterating if you see the resulting approximate posterior doesn't become tiny before hitting the edge of the prior. We won't mind if you cheat a little and use the MCMC result to motivate the refined priors in this case, but keep in mind that they should include at least the entire volume of parameter space where the chain has samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f737be-5390-4902-b4a3-e4a49ab0629f",
   "metadata": {
    "deletable": false,
    "id": "61f737be-5390-4902-b4a3-e4a49ab0629f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cff86f1b574ab18d32da72718eeb4506",
     "grade": false,
     "grade_id": "priors2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# priors2 = {'a':..., 'b':..., 's2':...}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea06e0f-4459-433a-996c-d7b46ea69909",
   "metadata": {
    "id": "cea06e0f-4459-433a-996c-d7b46ea69909"
   },
   "source": [
    "What summary statistics and distance function would you use for this problem? **Note: in class we will reach a consensus for what to implement below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c01087-9428-4e7b-aa4b-d53139c07dc4",
   "metadata": {
    "deletable": false,
    "id": "a9c01087-9428-4e7b-aa4b-d53139c07dc4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94940e72efcd6a207b32ec6134454a0b",
     "grade": false,
     "grade_id": "ABC_scat",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ABC_scat(ABC):\n",
    "#     def summaries(self, x, y):\n",
    "#         # return summary statistics as a 1D np.array\n",
    "#     def distance(self, s1, s2):\n",
    "#         # return scalar distance between 2 sets of summary statistics\n",
    "#     def simulate(self, a=0.0, b=0.0, s2=1.0):\n",
    "#         # return a simulated 1D array y, given self.x and additional parameters arguments (with default values)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "abc3 = ABC_scat(x2, y2, priors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3318ec8-5664-45fd-a004-6f147082d2ba",
   "metadata": {
    "id": "c3318ec8-5664-45fd-a004-6f147082d2ba"
   },
   "source": [
    "As always, we next check for obvious code bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264d2aa-1b9e-454b-b4bd-8b642724c0fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c264d2aa-1b9e-454b-b4bd-8b642724c0fa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bb4bf9a74a2af6ebd54cf415b07226e",
     "grade": true,
     "grade_id": "scat_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "abc3.run(1)\n",
    "print(abc3.samples)\n",
    "print(abc3.distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea42db0-09d4-486a-ac85-489de21ef58e",
   "metadata": {
    "id": "2ea42db0-09d4-486a-ac85-489de21ef58e"
   },
   "source": [
    "Given the additional free parameter, we expect to need to run for longer to get a useful number of accepted samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709a85d-1eba-480e-b8d4-0e107eb220bd",
   "metadata": {
    "id": "2709a85d-1eba-480e-b8d4-0e107eb220bd"
   },
   "outputs": [],
   "source": [
    "%time abc3.run(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5f0e7-f839-44bc-9df7-dd21a0d9f862",
   "metadata": {
    "id": "75d5f0e7-f839-44bc-9df7-dd21a0d9f862"
   },
   "source": [
    "Let's see what that gave us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3c2e1-e72d-4335-978b-5e4d2c74d3ae",
   "metadata": {
    "id": "2ae3c2e1-e72d-4335-978b-5e4d2c74d3ae"
   },
   "outputs": [],
   "source": [
    "abc3.distance_hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57517a-dd5c-4c88-9286-2ed37a7e8858",
   "metadata": {
    "id": "ce57517a-dd5c-4c88-9286-2ed37a7e8858"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc3.distances, 0.5)\n",
    "print(\"eps =\", eps)\n",
    "abc3.select_distances(eps)\n",
    "abc3.plot_acceptances(truth_tri=tri, size=6.0, show_rejects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314530f6-29b2-47d3-a05b-aa980864ec40",
   "metadata": {
    "id": "314530f6-29b2-47d3-a05b-aa980864ec40"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc3.distances, 0.1)\n",
    "print(\"eps =\", eps)\n",
    "abc3.select_distances(eps)\n",
    "abc3.plot_acceptances(truth_tri=tri, size=6.0, show_rejects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958a874-34e8-4411-a94a-0d2eab63a390",
   "metadata": {
    "id": "f958a874-34e8-4411-a94a-0d2eab63a390"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc3.distances, 0.05)\n",
    "print(\"eps =\", eps)\n",
    "abc3.select_distances(eps)\n",
    "abc3.plot_acceptances(truth_tri=tri, size=6.0, show_rejects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcd474-edf5-444a-95db-e074eacc2c7f",
   "metadata": {
    "id": "64fcd474-edf5-444a-95db-e074eacc2c7f"
   },
   "outputs": [],
   "source": [
    "eps = np.quantile(abc3.distances, 0.01)\n",
    "print(\"eps =\", eps)\n",
    "abc3.select_distances(eps)\n",
    "abc3.plot_acceptances(truth_tri=tri, size=6.0, show_rejects=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43a8f6-fc3f-4bad-b414-c713ec077bcd",
   "metadata": {
    "id": "6c43a8f6-fc3f-4bad-b414-c713ec077bcd"
   },
   "source": [
    "Hopefully you can see indications that ABC is working, although chances are you would also need to run longer and decrease the acceptance threshold even more before the comparison to the MCMC results really starts to look good. We stress that there are more intelligent, adaptive implementations of ABC than our simplistic version, which one would turn to in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebc0f7-e23d-4e2b-8ce3-e85a7e9f0504",
   "metadata": {
    "deletable": false,
    "id": "53ebc0f7-e23d-4e2b-8ce3-e85a7e9f0504",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aff03c290a3c54c33ecbf83e60eb23bd",
     "grade": false,
     "grade_id": "check3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Yes_this_all_checks_out = False # change to True when true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcdf272-8d5f-4788-aa76-d18681b16d4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3bcdf272-8d5f-4788-aa76-d18681b16d4b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c79060169ecd30d693b27cc8ccbbe4",
     "grade": true,
     "grade_id": "check3_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert Yes_this_all_checks_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811a15d-a78b-4aff-9696-fa1f440fad3f",
   "metadata": {
    "id": "b811a15d-a78b-4aff-9696-fa1f440fad3f"
   },
   "source": [
    "### Bootstrap\n",
    "\n",
    "We conclude this notebook with a quick implementation of the bootstrap. Recall that this is a frequentist method for \"robustly\" estimating uncertainties in estimates when the sampling distribution is not well known enough to do maximum likelihood (for example). The procedure is\n",
    "* randomly sample from the data with replacement,\n",
    "* compute the estimator(s) from the sampled data,\n",
    "* repeat many times, and look at the distribution of estimates.\n",
    "\n",
    "Let's go ahead and do this using the summary statistics from the above exercise (with free scatter) as our estimators. In this context, to be clear, resampling the data means resampling the _pairs_ $(x,y)$, not sampling $x$s and $y$s independently.\n",
    "\n",
    "Below, complete a function that resamples the given data and returns it in a tuple. (This hardly seems worth making an exercise, which says something about the simplicity of implementing the bootstrap.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2315402-2242-44b9-8f59-a433e9715aea",
   "metadata": {
    "deletable": false,
    "id": "e2315402-2242-44b9-8f59-a433e9715aea",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bc2c19d2c6e6e3ce52e56c4da25cae5",
     "grade": false,
     "grade_id": "boot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap(x, y):\n",
    "    # i = list of indices into x and y encoding the resampling; see np.random.choice\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x[i],y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a1e93-c30e-45f9-9beb-33480e757ac0",
   "metadata": {
    "id": "9a7a1e93-c30e-45f9-9beb-33480e757ac0"
   },
   "source": [
    "Let's run it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2f57d-260a-40b8-b422-e4724e9a1899",
   "metadata": {
    "id": "0ec2f57d-260a-40b8-b422-e4724e9a1899"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "boot = np.full((20000,3), np.nan)\n",
    "for i in range(boot.shape[0]):\n",
    "    xb,yb = bootstrap(x2, y2)\n",
    "    boot[i,:] = abc3.summaries(xb, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a783c90-4321-4311-99e7-0624b2ee7278",
   "metadata": {
    "id": "4a783c90-4321-4311-99e7-0624b2ee7278"
   },
   "source": [
    "... and compare to the MCMC posterior for this problem. Below, the bootstrap distribution is red/dashed curves, and the MCMC posterior is blue/solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62137a-f162-4cd1-a870-4f4863871649",
   "metadata": {
    "id": "ac62137a-f162-4cd1-a870-4f4863871649"
   },
   "outputs": [],
   "source": [
    "boot_tri = cr.whist_triangle(boot, bins=50, smooth2D=1);\n",
    "fig,ax = cr.whist_triangle_plot(boot_tri, paramNames=param_names2, linecolor1D='r', linecolor2D='r', linestyle1D='--', linestyle2D='--', fill2D=False);\n",
    "cr.whist_triangle_plot(tri, linecolor1D='b', linecolor2D='b', fill2D=False, axes=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a8421-8b4d-41a1-9b7e-e82320fd0f65",
   "metadata": {
    "id": "cf2a8421-8b4d-41a1-9b7e-e82320fd0f65"
   },
   "source": [
    "With our data, the bootstrap was a little too optimistic for $a$ and $b$, but not bad, while it produces a _much_ tighter distribution for $\\sigma^2$. In principle, it should do better for larger data sets, where more information is available from resampling, but we haven't explicitly checked (exercise for the reader!). On the other hand, for our particular data set, the bootstrap actually looks to be doing a slightly better job that the Laplace Approximation for the intercept and slope parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934c7f6-0f01-44d1-854d-f1ea6daa5296",
   "metadata": {
    "id": "9934c7f6-0f01-44d1-854d-f1ea6daa5296"
   },
   "source": [
    "## Parting thoughts\n",
    "\n",
    "We hope you have gained an appreciation for and understanding of some of the approximate methods covered in the notes. Possibly, you're now more motivated to avoid using them whenever possible. Still, while a little ingenuity can go a long way in making MCMC and related methods more tractable, it's nice to know that there are approximate methods that still live in the realm of principled inference."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
