{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: AGN Photometry with Gibbs Sampling\n",
    "\n",
    "Note: this tutorial follows [AGN Photometry on a Grid](agn_photometry_grid.ipynb). Along with [AGN Photometry with Metropolis Sampling](agn_photometry_metro.ipynb), it should be done before [MCMC Diagnostics](mcmc_diagnostics.ipynb).\n",
    "\n",
    "Having laboriously done Baysian inference on a grid to fit an AGN source to X-ray data, we will now turn to solving the same problem using the Gibbs sampling method of MCMC. A heads up, this is the longest tutorial to date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import astropy.io.fits as pyfits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from io import StringIO   # StringIO behaves like a file object\n",
    "import scipy.stats as st\n",
    "from pygtc import plotGTC\n",
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will read in the X-ray image data, and extract a small image around an AGN that we wish to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xray_image import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # datadir = '../ignore/' # or whatever - path to where you put the downloaded files\n",
    "\n",
    "imagefile = datadir + 'P0098010101M2U009IMAGE_3000.FTZ'\n",
    "expmapfile = datadir + 'P0098010101M2U009EXPMAP3000.FTZ'\n",
    "\n",
    "imfits = pyfits.open(imagefile)\n",
    "exfits = pyfits.open(expmapfile)\n",
    "\n",
    "im = imfits[0].data\n",
    "ex = exfits[0].data\n",
    "\n",
    "orig = Image(im, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 417\n",
    "y0 = 209\n",
    "stampwid = 5 # very small! see below\n",
    "stamp = orig.cutout(x0-stampwid, x0+stampwid, y0-stampwid, y0+stampwid)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "stamp.display(log_image=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting for 2 parameters\n",
    "\n",
    "We're going use this AGN photometry example to try out Gibbs sampling, specifically conjugate Gibbs. This doesn't work out beautifully for all the model parameters, but shouldn't be too bad if we\n",
    "1. assume the background is zero. If we narrow the postage stamp down tightly around the AGN, then a very small fraction of counts should be background, and this will not be a terrible assumption. (Note the choice of `stampwid` above.)\n",
    "2. assume that the exposure map is uniform over the stamp. Again, not bad if it's small enough.\n",
    "3. fit only for the flux and PSF width. (We'll generalize to include the AGN position later.)\n",
    "\n",
    "Please note that, even though the assumptions above should not be terrible, they aren't ones we would normally make in real life. Probably we wouldn't use Gibbs sampling in this particular case. Still, it's neat that we can even use this real example to illustrate the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The likelihood\n",
    "\n",
    "Time for some math. (Recall that conjuate Gibbs sampling sometimes lets us design a very efficient sampler at the cost of doing some math ourselves.)\n",
    "\n",
    "First, it will be convenient to work with the expected total number of counts instead of the count rate. So, instead of `lnF0` as a parameter, we'll have $\\mu$.\n",
    "\n",
    "Recalling that the counts are Poisson distributed, our likelihood is\n",
    "\n",
    "$p(\\{N_i\\}|\\mu,\\sigma) = \\prod_i \\frac{e^{-\\mu_i}\\mu_i^{N_i}}{N_i!}$,\n",
    "\n",
    "where $N_i$ is the number of counts in pixel $i$ and $\\mu_i$ is the expected number in that pixel ($\\sum_i \\mu_i=\\mu$). Furthermore, our symmetric, Gaussian model of the PSF says that\n",
    "\n",
    "$\\mu_i = \\mu \\, F_i(x_0,y_0,\\sigma)$,\n",
    "\n",
    "where the AGN center is $(x_0,y_0)$, and $F_i$ is the integral of the Gaussian PSF over the area of the $i$th pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a little bit of a mess, but we can simplify it to a more intuitive form. Namely, with $N=\\sum_i N_i$,\n",
    "\n",
    "$p(\\{N_i\\}|\\mu,\\sigma) \\propto \\mathrm{Poisson}(N|\\mu) \\prod_j F_{i(j)}(x_0,y_0,\\sigma)$,\n",
    "\n",
    "where the index $j$ only runs over detected counts (i.e. those pixels where $N_i>0$, repeated $N_i$ times in the product), and the notation $i(j)$ clumsily refers to the pixel where the $j$th count landed.\n",
    "\n",
    "In other words, we can factor the likelihood into\n",
    "1. a probability for the total number of counts, and\n",
    "2. a probability for the spatial distribution of those counts.\n",
    "\n",
    "This is one of those results that may be more intuitive to see than to derive, even if the derivation is not complex. So, please take a minute to check that you can get to this version of the likelihood from the previous one (up to factors of $N$ and $N_i$, which don't depend on model parameters; see Endnote 1), and write down how it works in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _your own words..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be slightly dodgy, and replace $F_{i(j)}(x_0,y_0,\\sigma)$ with the PSF (the Gaussian density for two uncorrelated variables) evaluated at the center of pixel $i$, $(x_i,y_i)$. This should not be the worst thing ever if the PSF is much larger than a pixel, which is the case here. The likelihood will no longer be correctly normalized, but as we'll see, this doesn't matter to us.\n",
    "\n",
    "$p(\\{N_i\\}|\\mu,\\sigma) \\propto \\mathrm{Poisson}(N|\\mu) \\prod_j \\mathrm{Normal}(x_j|x_0,\\sigma) \\, \\mathrm{Normal}(y_j|y_0,\\sigma)$.\n",
    "\n",
    "Finally, we can shorten this even more by concatenating $\\mathbf{x}-x_0$ and $\\mathbf{y}-y_0$ into $\\mathbf{z}$, indexed by $k$, writing\n",
    "\n",
    "$p(\\{N_i\\}|\\mu,\\sigma) \\propto \\mathrm{Poisson}(N|\\mu) \\prod_k \\mathrm{Normal}(z_k|0,\\sigma)$.\n",
    "\n",
    "This last step is just a notation trick to emphasize that all the displacements in both $x$ and $y$ from $(x_0,y_0)$ tell us about the PSF width, $\\sigma$. The index $k$ would run from 1 to twice the number of detected counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugacy\n",
    "\n",
    "The form of the likelihood is now about as simple as we could hope for. Next, we need to work out the conjugate relations for updating each parameter, and decide whether we can live with the form of the prior that this would require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation for $\\mu$\n",
    "\n",
    "The dependence of the likelihood on $\\mu$ is all in a Poisson term, and we see from [the repository of all knowledge](https://en.wikipedia.org/wiki/Conjugate_prior) that the conjugate prior is the Gamma distribution. Since this conjugacy was worked out [in the notes](../notes/bayes_law.ipynb), we won't make you derive it here. Lucky you!\n",
    "\n",
    "Recall that, for conjugate Gibbs sampling, we want to work out the posterior for $\\mu$ _conditioned on all other parameters_. This is much simpler than dealing with marginalization or some other operation. So, using the Poisson-Gamma conjugacy relation, we have that\n",
    "\n",
    "$\\mathrm{Gamma}(\\mu|\\alpha_0,\\beta_0) \\, \\mathrm{Poisson}(N|\\mu,\\ldots) \\propto \\mathrm{Gamma}(\\mu|\\alpha_0+N,\\beta_0+1) = p(\\mu|\\{N_i\\};\\ldots)$,\n",
    "\n",
    "where \"$\\ldots$\" stands for all the other parameters: $x_0,y_0,\\sigma$.\n",
    "\n",
    "(Remember that the posterior _must be_ a normalized PDF. Hence, the RHS _is_ the posterior for $\\mu$. If we had carefully kept track of all the normalizing factors and computed the evidence, this is what it would work out to be. Aren't we lucky that we didn't need to do so explicitly?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we _could_ use conjugate Gibbs sampling for $\\mu$, but should we? In other words, can we find a prior we can live with within the [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution) family? Probably. Here are some special cases of the distribution, some of which you can verify by inspection of its PDF:\n",
    "* $(\\alpha_0=1,\\beta_0\\rightarrow0)$ is uniform on $\\mu>0$;\n",
    "* $(\\alpha_0=1/2,\\beta_0\\rightarrow0)$ turns out to be the Jefferys prior, $p(\\mu) \\propto \\mu^{-1/2}$;\n",
    "* $(\\alpha_0\\rightarrow0,\\beta_0\\rightarrow0)$ is $p(\\mu)\\propto 1/\\mu$, or uniform in $\\ln(\\mu)$, thus equivalent to the uniform prior on `lnF0` we used previously. Let's use this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation for $\\sigma$\n",
    "\n",
    "Specifically, we are looking for a conjugate relation for the standard deviation of a Normal likelihood, with a known mean, since we are conditioning on $x_0$ and $y_0$. This conjugacy relation is much more annoying to work out, but the Wikipedia tells us what the update rule for the **variance** (not the standard deviation, but $\\sigma^2$) is, and that the conjugate prior is the \"scaled inverse chi-square\" distribution:\n",
    "\n",
    "$\\mathrm{SclInv}\\chi^2(\\sigma^2|\\nu_0,\\sigma_0^2) \\, \\prod_k\\mathrm{Normal}(z_k|0,\\sigma,\\ldots) \\propto \\mathrm{SclInv}\\chi^2\\left(\\sigma^2\\left|\\nu_0+n_k, \\frac{\\nu_0\\sigma_0^2 + \\sum_k z_k^2}{\\nu_0+n_k}\\right.\\right) = p(\\sigma^2|\\{N_i\\};\\ldots)$.\n",
    "\n",
    "Here $n_k=2N$ is the number of items in the sum over $k$, i.e. _twice_ the number of counts (since both their $x$ and $y$ positions provide independent information about $\\sigma$).\n",
    "\n",
    "The [scaled inverse $\\chi^2$ distribution](https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution) is a little uglier than we're used to, but not all that hard to deal with due to it's close relationship with the $\\chi^2$ distribution. Again we can find values or limits of the parameters that are compatible with some of the \"uninformative\" choices we might want to make. See if these make sense by inspection of the PDF (all these have non-positive-integer degrees of freedom, which is admittedly bizarre, but the math works out):\n",
    "* $(\\nu_0=-2,\\sigma_0^2\\rightarrow0)$ is uniform in $\\sigma^2$;\n",
    "* $\\nu_0\\rightarrow0$ and any $\\sigma_0^2$ is the Jeffreys prior, $p(\\sigma^2) \\propto \\sigma^{-2}$;\n",
    "* $(\\nu_0=-1,\\sigma_0^2\\rightarrow0)$ is uniform in $\\sigma$, $p(\\sigma^2) \\propto \\sigma^{-1}$.\n",
    "\n",
    "Let's use the third one. But remember that we've made a non-linear change of variables here ($\\sigma$ to $\\sigma^2$) and that in general the prior density is not invariant under such a transformation. Use what you learned way back in [Probability Tranformations](../notes/probability_transformations.ipynb) to work out the equivalent prior on $\\sigma$, $p(\\sigma)$. This will be important to know so that we can use the same prior when comparing to a different method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $p(\\sigma) = $ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "So, now we have rules from drawing samples from the posterior distributions of both parameters. In fact, we even have the posterior distribution in closed form, since in this case it factored cleanly into $p(\\mu,\\sigma|\\{N_i\\})=p(\\mu|\\{N_i\\})p(\\sigma|\\{N_i\\})$. That is, we won't actually need to generate a Markov chain, we can sample directly from the posterior without correlation between samples! This is called \"independence sampling\".\n",
    "\n",
    "Later, we'll see that this is no longer the case when we introduce the AGN $x$ and $y$ positions as free parameters; then we will need to sample from the _fully conditional_ posterior of each parameter in turn, producing a Markov chain of samples that approximate draws from the full posterior.\n",
    "\n",
    "But, before tackling that, let's get some code together to produce samples from the simpler case we're starting with, with just the two free parameters. Note that sampling from the scaled inverse $\\chi^2$ distribution is slightly involved, but not too complicated; it just follows from the definition of the distribution, so you ultimately end up drawing from a $\\chi^2$ distribution and applying some functions to those random draws.\n",
    "\n",
    "The cell below just defines dictionaries for the model parameters and prior hyperparameters as usual. Note that, because we can do independence sampling here, we don't actually need starting guesses for $\\mu$ and $\\sigma$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'mu':None, 'sigma':None, 'x0':x0, 'y0':y0}\n",
    "hyperparams = {'alpha0':0.0, 'beta0':0.0, 'nu0':-1, 'sigmasq0':0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part: write a function that takes the data, model parameters and prior hyperparameters as input, and returns a new sample of $\\mu$ and $\\sigma$ based on the derivations above. Note that **you should return $\\sigma$, not $\\sigma^2$**. (There is nothing to this other than taking a square root - post-facto transformations of parameters is simple; the only wrinkle is in chosing the appropriate prior density as we did above.)\n",
    "\n",
    "Even though we chose specific hyperparameter values above, you should write your code such that any valid values can be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def independence_sampler(img, par, hypar):\n",
    "    \"\"\"\n",
    "    img is of type Image (our postage stamp)\n",
    "    par and hypar are our params and hyperparams dictionaries\n",
    "    \"\"\"\n",
    "    # You will need to do some calculations involving the image data...\n",
    "    # (some of these could be done once instead of repeating every time this function is called, but whatever)\n",
    "    TBC()\n",
    "    # and then work out parameters of the conditional posterior for mu, and draw a sample from it...\n",
    "    TBC()\n",
    "    # and then do the same for sigma^2...\n",
    "    TBC()\n",
    "    # and then return the new samples.\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a few samples generated by this function look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([independence_sampler(stamp, params, hyperparams) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We could generate lots of samples with a python construction like the one above (because, again, this case is independence sampling). But, instead, let's belabor it as though we actually had to create a Markov chain, where each sample is dependent on the previous one. The cell below does this, updating the `params` dictionary within a loop (compare with the pseudocode in the [notes](../notes/montecarlo.ipynb)), and collecting all the samples in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nsamples = 10000\n",
    "samples2 = np.zeros((nsamples, 2))\n",
    "for i in range(samples2.shape[0]):\n",
    "    p = independence_sampler(stamp, params, hyperparams)\n",
    "    params['mu'] = p[0]\n",
    "    params['sigma'] = p[1]\n",
    "    samples2[i,:] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plotGTC` to quickly visualize the posterior. This package shows us all the 1D marginalized posteriors and every pair of 2D marginalized posteriors (as a contour plot), after some smoothing, in a triangular grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(samples2, paramNames=[r'$\\mu$', r'$\\sigma$'],\n",
    "        figureSize=5, customLabelFont={'size':12}, customTickFont={'size':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The cell below will compare your samples with some we have generated. They won't be identical, but should be extremely close if you've used the priors and data specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = np.loadtxt('solutions/gibbs.dat')\n",
    "plotGTC([samples2, ours], paramNames=[r'$\\mu$', r'$\\sigma$'], chainLabels=['yours', 'ours'],\n",
    "        figureSize=5, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare to what we get using other methods, we'll also want to transform from $\\mu$ back to `lnF0`. We can do this (roughly) by dividing our the median value of the exposure map (remember that we are assuming a uniform exposure in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2[:,0] = np.log(samples2[:,0] / np.median(stamp.ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(samples2, paramNames=[r'$\\ln F_0$', r'$\\sigma$'],\n",
    "        figureSize=5, customLabelFont={'size':12}, customTickFont={'size':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with what you found from the grid exercise? Keep in mind that we made different assumptions here (including that the background is zero). Do any differences in the posterior make sense in light of that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Your commentary ..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting for 4 parameters\n",
    "\n",
    "Let's now fit for the source position also ($x_0$ and $y_0$). We'll do this on simulated data (with _really_ no background), so that we can check whether our results are consistent with the input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mocking up an image\n",
    "\n",
    "First, I declare that these shall be the \"true\" parameters according to the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = {'mu': 35.0, 'sigma': 2.5, 'x0': 417, 'y0': 209}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, over to you to write a function that produces a mock image for us to work with. Remember that the sampling distribution/likelihood, written down somewhere way above, is a guide to exactly how to do this. Store the result of calling the function in a variable called `mock`. (You might find it convenient to just overwrite the image data in `stamp`, in which case you can just run `mock = stamp` afterward.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_image(data, x0, y0, mu, sigma):\n",
    "    '''\n",
    "    Generate a mock image from the model given by x0, y0, mu, sigma.\n",
    "    Either return an Image object containing this image, along with the other metadata help by `data`, or just\n",
    "    overwrite the counts image in `data`.\n",
    "    '''\n",
    "    TBC()\n",
    "\n",
    "# mock = mock_image(stamp, ...) ?\n",
    "# or, mock_image(stamp, ...); mock = stamp ?\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "mock.display(log_image=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the math\n",
    "\n",
    "Let's explicitly write down the fully conditional posteriors for the case where the source position is free.\n",
    "\n",
    "Nothing has changed for the total mean number of counts, which remains indepedent of the other parameters:\n",
    "\n",
    "$p(\\mu|\\{N_i\\};\\ldots) = \\mathrm{Gamma}(\\mu|\\alpha_0+N,\\beta_0+1)$.\n",
    "\n",
    "\n",
    "The fully conditional posterior for $\\sigma^2$ is just as it was before, but we should explicitly admit that we are conditioning on $x_0$ and $y_0$, and show where they enter the expression. Make sure you're happy with this, comparing with our previous equations, before going on.\n",
    "\n",
    "$p(\\sigma^2|\\{N_i\\};x_0, y_0,\\ldots) = \\mathrm{SclInv}\\chi^2\\left(\\sigma^2\\left|\\nu_0+n_k, \\frac{1}{\\nu_0+n_k}\\left[\\nu_0\\sigma_0^2 + \\sum_j \\left\\{\\frac{(x_0-x_j)^2}{\\sigma^2}+\\frac{(y_0-y_j)^2}{\\sigma^2}\\right\\}\\right]\\right.\\right)$.\n",
    "\n",
    "Finally, we need the fully conditional posteriors for $x_0$ and $y_0$, which are each independently the mean of a normal distribution with standard deviation $\\sigma$. Looking it up, we see that the conjugate prior is also normal. This is a little bit fiddly to show, but relatively straightforward using the Gaussian identities you've worked with [previously](gaussians.ipynb). Whether or not you take the time to work this out yourself, it's worth checking that the way the information from the prior and data are combined in the expressions below makes sense.\n",
    "\n",
    "Denoting the hyperparameters of the conjugate priors $(m_x,s_x,m_y,s_y)$, we have\n",
    "\n",
    "$p(x_0|\\sigma, \\{N_i\\}) = \\mathrm{Normal}\\left(x_0\\left|\\left[\\frac{1}{s_x^2}+\\frac{N}{\\sigma^2}\\right]^{-1}\\left[\\frac{m_x}{s_x^2}+\\frac{\\sum_j x_j}{\\sigma^2}\\right], \\left[\\frac{1}{s_x^2}+\\frac{N}{\\sigma^2}\\right]^{-1/2}\\right.\\right)$,\n",
    "\n",
    "$p(y_0|\\sigma, \\{N_i\\}) = \\mathrm{Normal}\\left(y_0\\left|\\left[\\frac{1}{s_y^2}+\\frac{N}{\\sigma^2}\\right]^{-1}\\left[\\frac{m_y}{s_y^2}+\\frac{\\sum_j y_j}{\\sigma^2}\\right], \\left[\\frac{1}{s_y^2}+\\frac{N}{\\sigma^2}\\right]^{-1/2}\\right.\\right)$.\n",
    "\n",
    "In practice, let's use uniform priors for $x_0$ and $y_0$, which correspond to the limit $s_x,s_y\\rightarrow\\infty$ (at which point the values of $m_x$ and $m_y$ cease to be important). However, you should write your code such that any valid hyperparameters can be used. Here we add them to the `hyperparameters` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['mx'] = 0.0\n",
    "hyperparams['sx'] = np.inf\n",
    "hyperparams['my'] = 0.0\n",
    "hyperparams['sy'] = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Remember that now each parameter must be updated in turn, meaning the new value of one parameter is used when updating the next, etc. We don't update everything all at once based on the current position, as we could before. So, let's explicitly write separate functions for updating different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu(img, par, hypar):\n",
    "    \"\"\"\n",
    "    img is of type Image (our mock postage stamp)\n",
    "    par and hypar are our params and hyperparams dictionaries\n",
    "    Instead of returning anything, we UPDATE par in place\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    # par['mu'] = ...\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sigma(img, par, hypar):\n",
    "    \"\"\"\n",
    "    img is of type Image (our mock postage stamp)\n",
    "    par and hypar are our params and hyperparams dictionaries\n",
    "    Instead of returning anything, we UPDATE par in place\n",
    "    (Remember to return sigma instead of signa^2!)\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    # par['sigma'] = ...\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update $x_0$ and $y_0$ in a single function, since their posteriors do not depend on one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_x0y0(img, par, hypar):\n",
    "    \"\"\"\n",
    "    img is of type Image (our mock postage stamp)\n",
    "    par and hypar are our params and hyperparams dictionaries\n",
    "    Instead of returning anything, we UPDATE par in place\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    # par['x0'] = ...\n",
    "    # par['y0'] = ...\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test all of that by calling each function and verifying that all the parameters changed (to finite, allowed values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "update_mu(mock, params, hyperparams)\n",
    "update_sigma(mock, params, hyperparams)\n",
    "update_x0y0(mock, params, hyperparams)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "As before, we can fill in an array with samples generated with the functions above. Note that, this time, the for loop is necessary, since we can't fill in row $i$ without knowing the contents of row $(i-1)$. The order of the individual parameter updates is arbitrary, and could even be randomized if you particularly wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples4 = np.zeros((nsamples,4))\n",
    "for i in range(samples4.shape[0]):\n",
    "    update_mu(mock, params, hyperparams)\n",
    "    update_sigma(mock, params, hyperparams)\n",
    "    update_x0y0(mock, params, hyperparams)\n",
    "    samples4[i,:] = [params['mu'], params['sigma'], params['x0'], params['y0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the most basic (yet still extremely important) visual check to see how our sampler performed, looking at traces of the Markov chain for each parameter. (It's ok if you haven't read the notes on [MCMC Diagnostics](../notes/mcmc_diagnostics.ipynb) yet; we will go more in-depth later.) These trace plots show the value of each parameter as a function of iteration, and we'll add a line showing the value that was used to create the mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = [r'$\\mu$', r'$\\sigma$', r'$x_0$', r'$y_0$']\n",
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "fig, ax = plt.subplots(4,1);\n",
    "cr.plot_traces(samples4, ax, labels=param_labels, \n",
    "            truths=[sim_params['mu'], sim_params['sigma'], sim_params['x0'], sim_params['y0']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if you started with pretty reasonable parameter values, it's entirely possible that there isn't a clear burn-in phase that needs to be thrown out.\n",
    "\n",
    "We can similarly look at the triangle-plot summary of the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGTC(samples4, paramNames=param_labels,\n",
    "        truths=[sim_params['mu'], sim_params['sigma'], sim_params['x0'], sim_params['y0']],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The details of these plots will depend on your random mock data set, but, statistically speaking, your posterior should look by eye to be pretty consistent with most of the input parameters. It's entirely possible for one or perhaps a pair of parameters to look a bit discrepant (again, by eye) by chance.\n",
    "\n",
    "We weren't overly concerned with the starting point for the test chain above. But, for later notebooks, we'll want to see how multiple, independent chains with different starting points behave when using this method. The cell below will take care of running 4 chains, started at random positions broadly in the vicinity of the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chains = [np.zeros((10000,4)) for j in range(4)]\n",
    "\n",
    "for samples in chains:\n",
    "    params = {'mu':st.uniform.rvs()*50.0,\n",
    "              'sigma':st.uniform.rvs()*4.9 + 0.1,\n",
    "              'x0':st.uniform.rvs()*10.0 + 412.0,\n",
    "              'y0':st.uniform.rvs()*10.0 + 204.0}\n",
    "    for i in range(samples.shape[0]):\n",
    "        update_mu(mock, params, hyperparams)\n",
    "        update_sigma(mock, params, hyperparams)\n",
    "        update_x0y0(mock, params, hyperparams)\n",
    "        samples[i,:] = [params['mu'], params['sigma'], params['x0'], params['y0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a more colorful version of the trace plots, showing all of the chains simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "fig, ax = plt.subplots(len(param_labels), 1);\n",
    "cr.plot_traces(chains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0},\n",
    "           truths=[sim_params['mu'], sim_params['sigma'], sim_params['x0'], sim_params['y0']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save them for later, and we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # change path below, if desired\n",
    "#for i,samples in enumerate(chains):\n",
    "#    np.savetxt('../ignore/agn_gibbs_chain_'+str(i)+'.txt', samples, header='mu sigma x0 y0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it - at the cost of engaing our brains, you've now fit a model with enough free parameters to make a grid-based solution uncomfortably slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endnotes\n",
    "\n",
    "1. You might recognize the leftover coefficient in this expression as a multinomial coefficient, enumerating the various ways that $N$ counts can be distributed among pixels such that the final number in each pixel is given by ${N_i}$. This is the kind of detail that we normally don't have to worry about - since model parameters make no appearance, the term has no effect on parameter constraints - but we'll see that such combinatorics are important to keep track of in the context of [missing data](../notes/missingdata.ipynb) later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
