{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: O-ring Failure Rates Prior to the Challenger Shuttle Loss\n",
    "## Coping with missing information\n",
    "\n",
    "In this tutorial, we will use a real data set where unwise interpretation of incomplete data had serious consequences to illustrate how such selection effects could be modeled. You will\n",
    "\n",
    "* implement and fit a simple model for the probability of O-ring failure as a function of ambient temperature, assuming a complete data set;\n",
    "* modify the model to account for censoring of the data;\n",
    "* modify the model further to account for truncation of the data set;\n",
    "* compare the inferences following from each of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "On January 28, 1986, the Space Shuttle Challenger was destroyed in an explosion during launch. The cause was eventually found to be the failure of an O-ring seal that normally prevents hot gas from leaking between two segments of the solid rocket motors during their burn. The ambient atmospheric temperature of just 36 degrees Fahrenheit,  significantly colder than any previous launch, was determined to be a significant factor in the failure.\n",
    "\n",
    "A relevant excerpt from the [Report of the Presidential Commission on the Space Shuttle Challenger Accident](https://history.nasa.gov/rogersrep/genindex.htm) reads:\n",
    "\n",
    "> #### Temperature Effects\n",
    "\n",
    "> The record of the fateful series of NASA and Thiokol meetings, telephone conferences, notes, and facsimile transmissions on January 27th, the night before the launch of flight 51L, shows that only limited consideration was given to the past history of O-ring damage in terms of temperature. The managers compared as a function of temperature the flights for which thermal distress of O-rings had been observed-not the frequency of occurrence based on all flights (Figure 6). In such a comparison, there is nothing irregular in the distribution of O-ring \"distress\" over the spectrum of joint temperatures at launch between 53 degrees Fahrenheit and 75 degrees Fahrenheit. When the entire history of flight experience is considered, including\"normal\" flights with no erosion or blow-by, the comparison is substantially different (Figure 7).\n",
    "\n",
    "> This comparison of flight history indicates that only three incidents of O-ring thermal distress occurred out of twenty flights with O-ring temperatures at 66 degrees Fahrenheit or above, whereas, all four flights with O-ring temperatures at 63 degrees Fahrenheit or below experienced O-ring thermal distress.\n",
    "\n",
    "> Consideration of the entire launch temperature history indicates that the probability of O-ring distress is increased to almost a certainty if the temperature of the joint is less than 65.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/v1p146.jpg\" width=75% alt=\"Top: number of incidents as a function of temperature, showing only launches with at least 1 incident; Bottom: same, including launches that suffered 0 incidents\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above show the number of incidences of O-ring damage found in previous missions as a function of the temperature at launch; these have been transcribed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TutorialName = 'missing_data'\n",
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from pygtc import plotGTC\n",
    "import emcee\n",
    "import incredible as cr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the figure above are transcribed and read into an array here. We store the launch temperatures in `oring_temps` and the corresponding number of incidents in `oring_incidents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oring_data_string = \\\n",
    "\"\"\"# temperature incidents\n",
    "53 3\n",
    "56 1\n",
    "57 1\n",
    "63 1\n",
    "66 0\n",
    "67 0\n",
    "67 0\n",
    "67 0\n",
    "68 0\n",
    "69 0\n",
    "70 1\n",
    "70 1\n",
    "70 0\n",
    "70 0\n",
    "72 0\n",
    "73 0\n",
    "75 2\n",
    "75 0\n",
    "76 0\n",
    "76 0\n",
    "78 0\n",
    "79 0\n",
    "80 0\n",
    "81 0\n",
    "\"\"\"\n",
    "oring_data = np.loadtxt(StringIO(oring_data_string), skiprows=1)\n",
    "oring_temps = oring_data[:,0]\n",
    "oring_incidents = oring_data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick plot to show that we did that right (cf above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.plot(oring_temps, oring_incidents, 'bo');\n",
    "plt.xlabel('temperature (F)');\n",
    "plt.ylabel('Number of incidents');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will simplify the data for each launch from integer (how many incidents of O-ring damage) to boolean (was there any damage, or not). This cell stores the temperatures corresponding to \"failure\" (any incidents) and \"success\" (no incidents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_temps = oring_temps[np.where(oring_incidents > 0)[0]]\n",
    "Nfailure = len(failure_temps)\n",
    "success_temps = oring_temps[np.where(oring_incidents == 0)[0]]\n",
    "Nsuccess = len(success_temps)\n",
    "print('temperatures corresponding to failures:', failure_temps)\n",
    "print('temperatures corresponding to successes:', success_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining a model\n",
    "\n",
    "Before worrying about missing data, let's define a model that we might want to fit to the complete data. We're interested in whether the probability of having zero O-ring incidents (or non-zero incidents, conversely) is a function of temperature. One possible parametrization that allows this is the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), which squeezes the real line onto the range (0,1).\n",
    "\n",
    "For reasons that may be clear later, I suggest defining the model in terms of the probability of success (zero incidents)\n",
    "\n",
    "$P_\\mathrm{success}(T|T_0,\\beta,P_\\mathrm{cold},P_\\mathrm{hot}) = P_\\mathrm{cold} + \\frac{P_\\mathrm{hot} - P_\\mathrm{cold}}{1 + e^{-\\beta(T-T_0)}}$,\n",
    "\n",
    "with parameters $T_0$ and $\\beta$ respectively determining the center and width of the logistic function, and $P_\\mathrm{cold}$ and $P_\\mathrm{hot}$ determine the probabilities of success at very low and high temperatures (which need not be 0 or 1).\n",
    "\n",
    "As we'll see in a moment, a model like this provides a smooth, linear-ish transition between two extreme values, without imposing the strong prior that $P_\\mathrm{success}$ _must_ drop to zero at some point, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Implement this function and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_success(T, T0, beta, Pcold, Phot):\n",
    "    \"\"\"\n",
    "    Evaluate Psuccess as given above, as a function of T, for parameters T0, beta, Pcold, Phot.\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the function for a few different parameter values. If you've never worked with the logistic function (or a similar _sigmoid_ function) before, this will give you an idea of how flexible it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "T_axis = np.arange(32., 100.)\n",
    "plt.plot(T_axis, P_success(T_axis, 70.0, 0.3, 0.0, 1.0));\n",
    "plt.plot(T_axis, P_success(T_axis, 65.0, 0.1, 0.4, 0.9));\n",
    "plt.plot(T_axis, P_success(T_axis, 45.0, 1.0, 0.1, 0.5));\n",
    "plt.plot(T_axis, P_success(T_axis, 80.0, 0.5, 0.9, 0.2));\n",
    "plt.xlabel('temperature (F)');\n",
    "plt.ylabel('probability of a clean launch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. PGM and priors\n",
    "\n",
    "Given the definition of the data and model above, draw the PGM for this problem, write down the corresponding probability expressions, and write down the likelihood (all assuming we have the complete data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing priors is a little tricky because we're interested in the model's predictions at $T=36$ degrees F, which is an extrapolation even for the complete data set.\n",
    "\n",
    "We'd like our model to be consistent with no trend a priori - that way we can see relatively straightforwardly whether the _data_ require there to be a trend. A pleasingly symmetric way to allow this is to put identical, independent priors on $P_\\mathrm{cold}$ and $P_\\mathrm{hot}$, in particular including the possibility that $P_\\mathrm{cold} > P_\\mathrm{hot}$ even though that isn't what we're looking for. Thus, a solution with $P_\\mathrm{cold}=P_\\mathrm{hot}$, i.e. no trend, is perfectly allowed.\n",
    "\n",
    "Our temperature data are given in integer degrees, so it doesn't make sense to allow values of $\\beta$ too much greater than 1, since the data would not resolve such a sudden change (which would increasingly make $P_\\mathrm{success}$ resemble a step function). By definition, $\\beta>0$ (it's a \"rate\" parameter).\n",
    "\n",
    "In principle, we might allow $T_0$ to take any value. But, arguably, the most sensible thing we can do with such limited information is test whether there is evidence for a trend in the probability of O-ring failure _within the range of the available data_ (or, a little more casually, the range of the figure from the report, above). Given the flexibility already provided by the choices above, there's little obvious benefit to allowing $T_0$ to vary more than this.\n",
    "\n",
    "In summary, my suggestion is the following uniform priors:\n",
    "* $0<P_\\mathrm{cold}<1$\n",
    "* $0<P_\\mathrm{hot}<1$\n",
    "* $0<\\beta<3$\n",
    "* $45<T_0<80$\n",
    "\n",
    "As always, you're welcome to mess around with other priors if you disagree. However, for the work you turn in, use the priors above.\n",
    "\n",
    "Implement a log-prior function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior(T0, beta, Pcold, Phot):\n",
    "    \"\"\"\n",
    "    Return the log-prior density for parameters T0, beta, Pcold, Phot\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Model fitting code\n",
    "\n",
    "Since the point of this tutorial is model design rather than carrying out a fit, a bunch of code is given below. Naturally, you should ensure that you understand what the code is doing, even though there's nothing to add.\n",
    "\n",
    "Here we follow a similar, though simpler, approach to the object oriented code used in the model evaluation/selection notebooks, since the models we'll compare all have the same set of free parameters. The `Model` object will take log-prior and log-likelihood functions as inputs in its constructor (instead of deriving new classes corresponding to different likelihoods), and will deal with the computational aspects of fitting the parameters. It will also provide a posterior prediction for the thing we actually care about, the failure probability at a given temperature. To do this, we need to marginalize over the model parameters; that is, we compute the posterior-weighted average of $1-P_\\mathrm{success}$, at some temperature of interest, over the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, log_prior, log_likelihood):\n",
    "        self.log_prior = log_prior\n",
    "        self.log_likelihood = log_likelihood\n",
    "        self.param_names = ['T0', 'beta', 'Pcold', 'Phot']\n",
    "        self.param_labels = [r'$T_0$', r'$\\beta$', r'$P_\\mathrm{cold}$', r'$P_\\mathrm{hot}$']\n",
    "        self.sampler = None\n",
    "        self.samples = None\n",
    "    def log_posterior(self, pvec=None, **params):\n",
    "        '''\n",
    "        Our usual log-posterior function, able to take a vector argument to satisfy emcee\n",
    "        '''\n",
    "        if pvec is not None:\n",
    "            pdict = {k:pvec[i] for i,k in enumerate(self.param_names)}\n",
    "            return self.log_posterior(**pdict)\n",
    "        lnp = self.log_prior(**params)\n",
    "        if lnp != -np.inf:\n",
    "            lnp += self.log_likelihood(**params)\n",
    "        return lnp\n",
    "    def sample_posterior(self, nwalkers=8, nsteps=10000, guess=[65.0, 0.1, 0.25, 0.75], threads=1):\n",
    "        # use emcee to sample the posterior\n",
    "        npars = len(self.param_names)\n",
    "        self.sampler = emcee.EnsembleSampler(nwalkers, npars, self.log_posterior, threads=threads)\n",
    "        start = np.array([np.array(guess)*(1.0 + 0.01*np.random.randn(npars)) for j in range(nwalkers)])\n",
    "        self.sampler.run_mcmc(start, nsteps)\n",
    "        plt.rcParams['figure.figsize'] = (16.0, 3.0*npars)\n",
    "        fig, ax = plt.subplots(npars, 1);\n",
    "        cr.plot_traces(self.sampler.chain[:min(8,nwalkers),:,:], ax, labels=self.param_labels);\n",
    "    def check_chains(self, burn=500, maxlag=500):\n",
    "        '''\n",
    "        Ignoring `burn` samples from the front of each chain, compute convergence criteria and\n",
    "        effective number of samples.\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        tmp_samples = [self.sampler.chain[i,burn:,:] for i in range(nwalk)]\n",
    "        print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "        print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))\n",
    "        print('NB: Since walkers are not independent, these will be optimistic!')\n",
    "    def remove_burnin(self, burn=500):\n",
    "        '''\n",
    "        Remove `burn` samples from the front of each chain, and concatenate.\n",
    "        Store the result in self.samples.\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        self.samples = self.sampler.chain[:,burn:,:].reshape(nwalk*(nsteps-burn), npars)\n",
    "    def posterior_prediction_Pfailure(self, temperatures=np.arange(30., 85.), probs=[0.5, 0.16, 0.84]):\n",
    "        '''\n",
    "        For the given temperatures, compute and store quantiles of the posterior predictive distribution for O-ring failure.\n",
    "        By default, return the median and a 68% credible interval (defined via quantiles).\n",
    "        '''\n",
    "        Pfail = np.array([1.0-P_success(T, self.samples[:,0], self.samples[:,1], self.samples[:,2], self.samples[:,3]) for T in temperatures])\n",
    "        res = {'T':temperatures, 'p':[str(p) for p in probs]}\n",
    "        for p in probs:\n",
    "            res[str(p)] = np.quantile(Pfail, p, axis=1)\n",
    "        self.post_failure = res\n",
    "    def plot_Pfailure(self, color, label):\n",
    "        '''\n",
    "        Plot summaries of the posterior predictive distribution for O-ring failure.\n",
    "        Show the center as a solid line and credible interval(s) bounded by dashed lines.\n",
    "        '''\n",
    "        plt.plot(self.post_failure['T'], self.post_failure[self.post_failure['p'][0]], color+'-', label=label)\n",
    "        n = len(self.post_failure['p'])\n",
    "        if n > 1:\n",
    "            for j in range(1,n):\n",
    "                plt.plot(self.post_failure['T'], self.post_failure[self.post_failure['p'][j]], color+'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solution for complete data\n",
    "\n",
    "First, let's see what the solution looks like when there are no missing data. Complete the likelihood function appropriate for a complete data set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like_complete(T0, beta, Pcold, Phot):\n",
    "    \"\"\"\n",
    "    Return the log-likelihood corresponding to a complete data set\n",
    "    (Go ahead and access `failure_temps' and `success_temps' from global scope)\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the `Model` code to work. The default options below _should_ work well enough, but do keep an eye the usual basic diagnostics as provided below, and make any necessary changes. First we instantiate the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model = Model(ln_prior, ln_like_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run the fit. Note that the parameters are not likely to be individually well contrained by the data, compared with the prior. We don't necessarily care about this - the important question is what the posterior predictive distribution for the probability of failure at a given temperature looks like. (We do, or course, need the chains to be converged and adequately sampled, however.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "complete_model.sample_posterior(nwalkers=8, nsteps=10000, guess=[65.0, 0.1, 0.25, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the usual diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.check_chains(burn=1000, maxlag=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove burn-in and plot the marginal posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.remove_burnin(burn=1000)\n",
    "plotGTC(complete_model.samples, paramNames=complete_model.param_labels,\n",
    "       figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that went well, let's visualize the predicted failure probability as a function of temperature. The solid and dashed lines show the posterior-predictive median and and percentile-based 68% credible interval for $P_\\mathrm{failure} = 1 - P_\\mathrm{success}$ at each temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.posterior_prediction_Pfailure()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6., 4.)\n",
    "complete_model.plot_Pfailure('C0', 'complete')\n",
    "plt.xlabel(r'$T$');\n",
    "plt.ylabel(r'$P_\\mathrm{failure}(T)$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make curve sense compared with inspection of the data? Any surprises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Let's look at the probability of failure at 36 F. This will print the posterior prediction median and CI lower and upper bounds. For comparison, I find approximately $0.83_{-0.25}^{+0.13}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.where(complete_model.post_failure['T']==36.)[0]\n",
    "thing = [float(complete_model.post_failure[p][j]) for p in complete_model.post_failure['p']]\n",
    "print('Pfailure(T=36F|data) =', round(thing[0],2), '+'+str(round(thing[2]-thing[0],2)), round(thing[1]-thing[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Censored (but somewhat informed) success temperatures\n",
    "\n",
    "Imagine we are in a slightly better situation than that shown in the top panel of Figure 6 from the report. Namely, we are given\n",
    "1. the temperatures of launches where there were O-ring failures (`failure_temps` and `Nfailure` above),\n",
    "2. the number of launches with no failures (`Nsuccess = len(success_temps)`),\n",
    "3. a range of temperatures containing the successful launches, but **not** the precise temperatures of each.\n",
    "\n",
    "For (3), we'll just use the actual min and max of `success_temps`, and implementing the prior on unknown temperatures as uniform in this range. In the next section, we'll look at the results with a less informed prior on the success temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_Tmin = np.min(success_temps)\n",
    "success_Tmax = np.max(success_temps)\n",
    "Nsuccess = len(success_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Censored model definition\n",
    "\n",
    "Work out how to adjust your PGM and expression for the likelihood to reflect our ignorance of the temperatures of successful launches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the (log)-likelihood for the censored model. Here are some hints/suggestions if you want them:\n",
    "1. This doesn't require as dramatic a change to the model as truncation would, more a re-definition of the sampling distribution for the censored points.\n",
    "2. A model component that was previously fixed by observation or effectively determined precisely is now indeterminate.\n",
    "3. We can marginalize over our newfound ignorance analytically, taking advantage of the fact that the integral of the logistic function is analytic ([see Wikipedia](https://en.wikipedia.org/wiki/Logistic_function))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like_censored(T0, beta, Pcold, Phot):\n",
    "    \"\"\"\n",
    "    Return the log-likelihood for the case of censored success temperatures.\n",
    "    This prototype assumes the success temperatures will be marginalized over within this function; otherwise\n",
    "    they would need to be additional parameters to be sampled.\n",
    "    \n",
    "    (Go ahead and access `failure_temps', `Nsuccess', `success_Tmin' and `success_Tmax' from global scope)\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Censored model fit\n",
    "\n",
    "We can now carry out the usual steps. Again, the choices made below will _probably_ work, but change them if need be **and check the usual diagnostics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_model = Model(ln_prior, ln_like_censored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "censored_model.sample_posterior(nwalkers=8, nsteps=10000, guess=[65.0, 0.1, 0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_model.check_chains(burn=1000, maxlag=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_model.remove_burnin(burn=1000)\n",
    "plotGTC([complete_model.samples, censored_model.samples], paramNames=complete_model.param_labels,\n",
    "        chainLabels=['complete', 'censored'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the posterior predictions to the previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_model.posterior_prediction_Pfailure()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6., 4.)\n",
    "complete_model.plot_Pfailure('C0', 'complete')\n",
    "censored_model.plot_Pfailure('C1', 'censored')\n",
    "plt.xlabel(r'$T$');\n",
    "plt.ylabel(r'$P_\\mathrm{failure}(T)$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your censored model manage to make consistent predictions to the model fitted to the complete data? If there are clear differences, do they make sense in light of what information has been hidden? Is there still evidence for a temperature-dependent failure rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Looking at a balmy temperature of 75 F this time, I find a failure probability of approximately $0.18_{-0.08}^{+0.10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.where(censored_model.post_failure['T']==75.)[0]\n",
    "thing = [float(censored_model.post_failure[p][j]) for p in censored_model.post_failure['p']]\n",
    "print('Pfailure(T=75F|data) =', round(thing[0],2), '+'+str(round(thing[2]-thing[0],2)), round(thing[1]-thing[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Censored (less informed) success temperatures\n",
    "\n",
    "As a point of comparison, let's fit a model in which the temperature range for the censored (success) data is much less well constrained. This is arguably more analogous to what we might do by eye if presented with the first figure in this notebook, knowing that successful launches were absent from the figure, but without the context that those launches has all taken place in warm weather.\n",
    "\n",
    "In particular, let's take the prior on the success temperatures to be uniform over the range shown in the figure. We followed poor practice by defining `success_Tmin` and `success_Tmax` at global scope earlier, and then using them from global scope in `ln_like_censored`, but it does mean we can just redefine them below and then re-use the likelihood function. If your implementation differs (i.e., was more sensible), you might need to change some more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_Tmin = 45.0\n",
    "success_Tmax = 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verycensored_model = Model(ln_prior, ln_like_censored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "verycensored_model.sample_posterior(nwalkers=8, nsteps=10000, guess=[65.0, 0.1, 0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verycensored_model.check_chains(burn=1000, maxlag=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verycensored_model.remove_burnin(burn=1000)\n",
    "plotGTC([complete_model.samples, censored_model.samples, verycensored_model.samples], paramNames=complete_model.param_labels,\n",
    "        chainLabels=['complete', 'censored', 'very censored'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like it will lead to somewhat different posterior predictions. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verycensored_model.posterior_prediction_Pfailure()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7., 5.)\n",
    "complete_model.plot_Pfailure('C0', 'complete')\n",
    "censored_model.plot_Pfailure('C1', 'censored')\n",
    "verycensored_model.plot_Pfailure('C2', 'very censored')\n",
    "plt.axvline(36.0, color='k', linestyle='dotted')\n",
    "plt.xlabel(r'$T$');\n",
    "plt.ylabel(r'$P_\\mathrm{failure}(T)$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical, dotted line added in this plot marks the ambient temperature of 36 F at the Challenger launch.\n",
    "\n",
    "Does this more censored model manage to make consistent predictions to the model fitted to the complete data? If there are clear differences, do they make sense in light of what information has been hidden?  Is there still evidence for a temperature-dependent failure rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # answer in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** looking now at 60 F, I find a failure probability of approximately $0.32_{-0.10}^{+0.11}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.where(verycensored_model.post_failure['T']==60.)[0]\n",
    "thing = [float(verycensored_model.post_failure[p][j]) for p in verycensored_model.post_failure['p']]\n",
    "print('Pfailure(T=60F|data) =', round(thing[0],2), '+'+str(round(thing[2]-thing[0],2)), round(thing[1]-thing[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Draw a conclusion\n",
    "\n",
    "In light of all your work above, comment on the report's assertion that \"Consideration of the entire launch temperature history indicates that the probability of O-ring distress is increased to almost a certainty if the temperature of the joint is less than 65.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # your wisdom in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OPTIONAL: Truncated success temperatures\n",
    "\n",
    "Modify the model such that the launches with zero O-ring incidents are truncated rather than censored. We no longer know how many such data points there are in the complete data set. If you want, you can still use prior information on the temperature range that incident-free launches happened in (as above), and some vague prior on the total number of launches (say, 25-ish). Run through the analysis in this scenario and compare the results with those above.\n",
    "\n",
    "Note that `emcee` cannot readily sample over an integer-value parameter, so the most straightforward implementation would involve marginalizing over the number of truncated data points within the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
