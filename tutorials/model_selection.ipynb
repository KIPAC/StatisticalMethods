{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Model Selection\n",
    "\n",
    "In the [previous tutorial](model_evaluation.ipynb), we evaluated the \"goodness of fit\" of two different models on a data set representing the distribution of centering offsets in a galaxy cluster sample. Here we will continue by doing two styles of quantitative test to compare the two models, with the goal of deciding whether the improved goodness of fit actually justifies the additional complexity of the second model. Specifically, we will compute:\n",
    "1. The Deviance Information Criterion, one of several possible information criteria, which has the advantages of being relatively simple and having a straightforward Bayesian interpretation.\n",
    "2. The Bayesian Evidence, a more principled but more complex approach fully in the Bayesian framework.\n",
    "\n",
    "First, we'll restore everything from the last notebook, including the model class definitions and instances, and the Markov chains they hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import dill\n",
    "\n",
    "# may need to change the load path\n",
    "TBC() # dill.load_session('../ignore/model_evaluation.db')\n",
    "\n",
    "exec(open('tbc.py').read()) # (re-)define TBC and TBC_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports go here\n",
    "from scipy.special import logsumexp\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate the DIC for each model\n",
    "\n",
    "Recall that the Deviance Information Criterion is given by:\n",
    "\n",
    "$\\mathrm{DIC} = \\langle D(\\theta) \\rangle + p_D; \\quad p_D = \\langle D(\\theta) \\rangle - D(\\langle\\theta\\rangle)$\n",
    "\n",
    "where $\\theta$ are the parameters of a model, the deviance $D(\\theta)=-2\\log P(\\mathrm{data}|\\theta)$, and averages $\\langle\\rangle$ are over the posterior distribution of $\\theta$.\n",
    "\n",
    "Write this function, and execute it for the simple model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIC(Model):\n",
    "    \"\"\"\n",
    "    Compute the Deviance Information Criterion for the given model.\n",
    "    (In a less pedagogical world, this would logically be a method of the base Model class.)\n",
    "    \"\"\"\n",
    "    # Compute the deviance D for each sample\n",
    "    D = -2.0*np.array([ Model.log_likelihood(*params) for params in Model.samples ])\n",
    "    #pD = \n",
    "    #DIC = \n",
    "    #return DIC, pD\n",
    "\n",
    "TBC_above(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the DIC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC1, pD1 = DIC(Model1)\n",
    "print('Model 1:')\n",
    "print(\"Effective number of fitted parameters =\", pD1)\n",
    "print(\"DIC =\", DIC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check:** For Model 1 (the exponential), I get $p_D \\approx 0.9$ and DIC $\\approx 1668$. As with anything else computed from chains, there will be some stochasticity to the values you compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC2, pD2 = DIC(Model2)\n",
    "print('Model 2:')\n",
    "print(\"Effective number of fitted parameters =\", pD2)\n",
    "print(\"DIC =\", DIC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your values of $p_D$ make intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to interpret this, we compare the reduction (hopefully) in the DIC of Model 2 compared with Model 1 to the Jeffreys scale (see the notes). By this metric, is your second model better at explaining the data than the exponential model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC1 - DIC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: your answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute the evidence by Monte Carlo integration\n",
    "\n",
    "To do this, note that\n",
    "\n",
    "$p(\\mathrm{data}|H)=\\int d\\theta \\, p(\\mathrm{data}|\\theta,H) \\, p(\\theta|H)$\n",
    "\n",
    "can be approximated by an average over samples from the prior:\n",
    "\n",
    "$p(\\mathrm{data}|H) \\approx \\frac{1}{m}\\sum_{k=1}^m p(\\mathrm{data}|\\theta_k,H)$, with $\\theta_k\\sim p(\\theta|H)$.\n",
    "\n",
    "This estimate is better than trying to use samples from the posterior to calculate the evidence, if only because it's unbiased. But in general, and especially for large-dimensional parameter spaces, it is very inefficient (because the likelihood typically is large in only a small fraction of the prior volume). Still, let's give it a try.\n",
    "\n",
    "Write a function to draw a large number of samples from the prior and use them to calculate the evidence. To avoid numerical over/underflows, use the special `scipy` function `logsumexp` (which we imported directly, way at the top of the notebook) to do the sum. As the name implies, this function is equivalent to `log(sum(exp(...)))`, but is more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_evidence(Model, N=1000):\n",
    "    \"\"\"\n",
    "    Compute the log evidence for the model using N samples from the prior\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick test to check for NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_evidence(Model1, N=2), log_evidence(Model2, N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly how precisely do we need to know the log Evidence, to be able to compare models? Run `log_evidence` with different values of `N` (the number of prior samples in the average) to until you're satisfied that you're getting a usefully accurate result for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Model 1:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]: # Do you need even more than this?\n",
    "    %time logE1 = log_evidence(Model1, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check:** For the exponential model, I converge to a log-evidence of $\\approx -837$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Model 2:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]: # Do you need even more than this?\n",
    "    %time logE2 = log_evidence(Model2, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have log evidences computed for each model. Now what? We just compare their difference to the Jeffreys scale again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE2 - logE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we might end up with a different conclusion as to the strength of any preference of Model 2, compared with the DIC! The reason is that the evidence explicitly accounts for the information in the prior (which, recall, counts as part of the model definition), while the DIC does this much less directly.\n",
    "\n",
    "We could also be good Bayesians and admit that there should be a prior distribution in model space. For example, maybe I have a very compelling theoretical reason why the offset distribution should be exponential (I don't, but just for example). Then, I might need some extra convincing that an alternative model is required.\n",
    "\n",
    "We would then compute the ratio of the posterior probabilities of the models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_H1 = 0.9 # or your choice\n",
    "prior_H2 = 1.0 - prior_H1 # assuming only these two options\n",
    "\n",
    "log_post_H1 = np.log(prior_H1) + logE1\n",
    "log_post_H2 = np.log(prior_H2) + logE2\n",
    "\n",
    "print('Difference of log posteriors (H2-H1):', log_post_H2-log_post_H1)\n",
    "print('Ratio of posteriors (H2/H1):', np.exp(log_post_H2-log_post_H1)) # NB this one might over/underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the fitness of the alternative model you chose, you may find that only an extremely lopsided prior in model space would influence your conclusion.\n",
    "\n",
    "Comment on what you find from the evidence, compared with the DIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TBC: _your answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the evidence with `dynesty`\n",
    "\n",
    "To get some experience with a package that uses nested sampling to compute the evidence, let's repeat Section 2 using `dynesty`.\n",
    "\n",
    "Looking at [the docs](https://dynesty.readthedocs.io/en/latest/crashcourse.html), we first need a function that maps the unit cube onto our prior. For a uniform prior, this is a simple translation and rescaling, but for other priors it involves going through the prior distribution's quantile function. Fortunately, we assumed uniform priors for Model 1! However, if you followed the advice in the previous notebook to keep a dictionary of univariate priors in the `Model` object, and those priors are `scipy.stats` distributions, it's relatively easy to handle this slightly more general case. However you choose to do it, implement a function that performs this transformation for the 1-parameter Exponential model (and priors) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a sanity check - the cell below should return the quartiles of the uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptform([0.25]), ptform([0.5]), ptform([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a log-likelihood function that takes a vector of parameters as input, in contrast to how it's written in the Model class. This could be solved similarly to what's done in the `Model.log_posterior` method, or we could just define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like(params):\n",
    "    return Model1.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just go ahead and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like, ptform, len(Model1.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at some stuff (see the Dynesty documentation for an explanation of what all this is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a summary of the run.\n",
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evidence panel appears to be utterly unhelpful in this case (at least when I ran it), so below is a plot of the log-evidence as a function of iteration, discarding the beginning when the accumulated evidence is truly tiny. It should show us converging to a value similar to what we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=14);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts the log-evidence at the last iteration, which may or not be the approved method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with what we got from simple monte carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE1 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was so easy, let's go ahead and do the same for Model 2. Implement the transformation from the unit cube to your model's parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform2(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this elegant definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like2(params):\n",
    "    return Model2.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like2, ptform2, len(Model2.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and produce the same set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=14);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, extract the final evidence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and compare it to the SMC result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE2 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the work you'd already done, that was (I hope) staggeringly easy to set up, even if it did take longer to compute the same answer (I find) in this case. In larger-dimensional parameter spaces, it's easy to imagine this method being far preferable to blindly sampling from the prior. Note that we run `dynesty` out of the box here - there are many options (and some potential failure modes) that you should read about before using it in the real world."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
