{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Bayes' Law\n",
    "\n",
    "In this problem, we'll work through a simple application of Bayes' Law, both analytically and numerically (on a grid).\n",
    "\n",
    "You will learn to:\n",
    "\n",
    "* use conjugacy relations to find an analytic form for a posterior\n",
    "* compute a posterior on a grid (brute force)\n",
    "* apply Bayes law to incorporate new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We're interested in knowing what fraction, $f$, of galaxy clusters in the Universe are dynamically relaxed (close to equilibrium). We can think of $f$ equivalently as the a priori probability that a randomly chosen, or newly discovered, cluster is relaxed.\n",
    "\n",
    "From X-ray imaging of 361 clusters, which we will assume are representative, 57 were found to be morphologically \"relaxed\" according to some metric. We will use this information to constrain $f$. Our data could be laboriously written as\n",
    "\n",
    "$X_1,X_2,\\ldots,X_{361}$,\n",
    "\n",
    "where 57 $X$'s are 1 (relaxed) and 304 are 0 (unrelaxed). Equivalently we can just speak of $s=57$ \"successes\" out of $n=361$ trials. Let's go ahead and put that into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'n':361, 's':57}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we write Bayes' Law for this application as\n",
    "\n",
    "$p(f|s) = \\frac{p(f)P(s|f)}{P(s)}$.\n",
    "\n",
    "First, as always, we choose a prior. Clearly, by definition, $0\\leq f \\leq 1$. Without thinking too hard about it, let's take $p(f)$ to be uniform over this interval.\n",
    "\n",
    "As the problem is defined, the sampling distribution, $P(s|f)$, is binomial:\n",
    "\n",
    "$P(s|f) = {n \\choose s} f^s (1-f)^{n-s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution and implementation\n",
    "\n",
    "It's always nice to have a simple, analytic way of expressing our solution, and this problem happens to provide one. Consult [the Wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior) to see whether this sampling distribution has a conjugate prior and determine whether that conjugate distribution can be used to express the uniform prior chosen above. (Note: it does, and it can.) Find the parameters of the conjugate prior that reproduce the uniform distribution on [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing on, prove the conjugacy relation that you looked up above. Multiply $p(f)$ and $P(s|f)$, and simplify until you obtain a normalized posterior distribution expressed in a standard form. Note that you are allowed to multiply by any constants you want - by definition whatever constant you need to multiply by to produce a normalized PDF must be $1/P(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $p(f) \\, P(s|f) = \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "\n",
    "Fill in the cells below with these prior parameter values and complete the function that evaluates the prior (you can call the `scipy.stats` implementation of the distribution, or spell out the density)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(f, some, parameters):\n",
    "    # return the prior density evaluated at x, p(f)\n",
    "    TBC()\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters that make the prior equivalent to Uniform(0,1), in a dictionary (as with data, above),\n",
    "# Make the dictionry keys the names of the parameter arguments to \"prior\", so we can pass it **prior_params.\n",
    "TBC() # prior_params = { ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Let's now plot the prior with your chosen parameters, to verify that it's the same as a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgrid = np.linspace(0.0, 1.0, 500)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, prior(fgrid, **prior_params), 'k-');\n",
    "plt.xlabel('f');\n",
    "plt.ylabel('p(f)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "Next, code up the likelihood function. Recall that this is just the sampling distribution evaluated as a function of the model parameters, with the data fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(f, s, n):\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, prior(fgrid, **prior_params), 'k-', label='p(f)');\n",
    "plt.plot(fgrid, likelihood(fgrid, **data), 'b-', label='P(s|f)');\n",
    "plt.xlabel('f');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold on! You didn't (necessarily) make a mistake. Remember that the likelihood function is not a PDF - it is not normalized over $f$ for a given $s$. So there is no reason the normalizations of the two curves above should be comparable.\n",
    "\n",
    "In fact, run the cell below to numerically verify that $P(s|f)$ is **not** normalized over $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This does NOT need to be 1.0:', np.trapz(likelihood(fgrid, **data), x=fgrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in its guise as the sampling distribution, it _is_ normalized over $s$ for a given $f$. Verify this for the arbitrary choice of $f$ below, or some other value(s) of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrid = np.arange(0.0, data['n']+1)\n",
    "\n",
    "test_f = np.pi/10.0\n",
    "print('This had better be exactly 1.0 (to within numerical error):', likelihood(test_f, sgrid, data['n']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, here is what $P(s|f)$ looks like as a function of $s$ (i.e. as a sampling distribution) for a few different $f$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(sgrid, likelihood(0.01, sgrid, data['n']), 'b.', label='f=0.01');\n",
    "plt.plot(sgrid, likelihood(0.1, sgrid, data['n']), 'r.', label='f=0.1');\n",
    "plt.plot(sgrid, likelihood(0.7, sgrid, data['n']), 'k.', label='f=0.07');\n",
    "plt.xlabel('s');\n",
    "plt.ylabel('P(s|f)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "\n",
    "Moving on, code up the posterior distribution. Since its functional form is the same as that of the prior, we can go ahead and use the same Python function. Then all we need is a function that determines the posterior distribution's parameters given the prior and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior # you can have this one\n",
    "\n",
    "def get_post_params(prior, parameters, n, s):\n",
    "    # Note: this function's argument list should include the prior's parameters at the front.\n",
    "    # Return the posterior parameters as a dictionary.\n",
    "    # Since the posterior distribution (function) is the same as the prior distribution \n",
    "    # (function), this dictionary's keys should be the same as \"prior_params\" has.\n",
    "    TBC()\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_params = get_post_params(**prior_params, **data)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, prior(fgrid, **prior_params), 'k-', label='p(f)');\n",
    "plt.plot(fgrid, likelihood(fgrid, **data), 'b-', label='p(s|f)');\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params), 'r-', label='p(f|s)');\n",
    "plt.xlabel('f');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with brute force\n",
    "\n",
    "Just to drive the point home, let's imagine we didn't recognize this problem as conjugate. Evaluate the posterior over `fgrid` by brute force (i.e. multiplying the likelihood and prior), and we'll compare. Don't forget to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # post_fgrid = ...posterior evaluated at f = \"fgrid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it along with the conjugate solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params), 'r-', label='analytic');\n",
    "plt.plot(fgrid, post_fgrid, 'b.', label='grid');\n",
    "plt.xlabel('f');\n",
    "plt.ylabel('p(f|s)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that all this messing around with conjugacy relations is silly, and in this simple case, which takes very little time to evaluate numerically, that's fair enough. While conjugacy doesn't apply to every problem, there are a few reasons it's worth knowing about for those times that it is a viable strategy:\n",
    "1. Since the posterior has a well known functional form, we instantly know its mean, median, mode, variance, skewness, kurtosis, etc., etc., etc. to arbitrary precision - things that we might be interested in that would be more annoying to estimate numerically. They are simple functions of the distribution's parameters that we can look up.\n",
    "2. When dealing with multi-parameter distributions and/or large amounts of data, leaping straight to the final answer (after, at most, some linear algebra) can sometimes represent a significant speed-up over more brute-force methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the constraint on $f$\n",
    "\n",
    "While the posterior for $f$, evaluated one of the ways above, in principle is the entire answer, we normally want to summarize the constraint in terms of a best value and an interval. You'll see more about this in the Credible Region [notes](../notes/credible_regions.ipynb) and [tutorial](credible_intervals.ipynb). For now, we'll go ahead and summarize the constraint according to one convention for doing so.\n",
    "\n",
    "Write a function that finds the median, 15.85th percentile and 84.15th percentile of the posterior distribution for $f$. Do this based on the parameters of the conjugate posterior, which will let you take advantage of `scipy` functions (e.g. `scipy.<distribution>.median`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_posterior(post_params):\n",
    "    # Find the 50th, 15.85th and 84.15th percentiles of the posterior distribution.\n",
    "    # Return these as a numpy array with shape (3,), in that order.\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare what you found with this pre-computed solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('My best fit and interval:', summarize_posterior(post_params))\n",
    "print(\"Should be pretty much zeros:\", summarize_posterior(post_params) - np.loadtxt('solutions/bayeslaw.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we get more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = {'n':200, 's':40}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `get_post_params` function to compute the posterior parameters for\n",
    "1. the second data set combined with the original prior (not using the first data set), and\n",
    "2. the combination of both data sets and the original prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# post_params_2 = posterior from just the second data set\n",
    "# post_params_both = posterior from both data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the posteriors from the individual data sets compare with the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, prior(fgrid, **prior_params), 'k-', label='p(f)');\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params), 'r-', label='p(f|s_1)');\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params_2), 'b-', label='p(f|s_2)');\n",
    "plt.xlabel('f');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualizes the accumulation of data as we add first one and then the second data set to the prior information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(fgrid, prior(fgrid, **prior_params), 'k-', label=r'$p(f)$');\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params), 'r-', label=r'$p(f|s_1)$');\n",
    "plt.plot(fgrid, posterior(fgrid, **post_params_both), 'b-', label=r'$p(f|s_1,s_2)$');\n",
    "plt.xlabel('f');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compare the constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Just data1:', summarize_posterior(post_params))\n",
    "print('Just data2:', summarize_posterior(post_params_2))\n",
    "print('Both data1 and data2', summarize_posterior(post_params_both))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
