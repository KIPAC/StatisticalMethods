{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d3221c-b064-4813-a5d5-9e7dec2c057e",
   "metadata": {
    "id": "d6d3221c-b064-4813-a5d5-9e7dec2c057e"
   },
   "source": [
    "# Tutorial: Cluster redshift distribution\n",
    "\n",
    "In this notebook, you will implement two of the simpler algorithms for Markov chain monte carlo. In practice, you will probably use a package provided by someone else in most circumstances, but it's useful to build these samplers from the ground up at least once to see how they work under the hood.\n",
    "\n",
    "To be concrete, you will\n",
    "* define a generative model for redshifts of the member galaxies of a cluster\n",
    "* determine the conditional of each parameter, and implement conjugate Gibbs updates\n",
    "* implement proposal distributions and the Metropolis accept/reject rule\n",
    "* run both algorithms, and compare the resulting samples from the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16766ddc-e3e1-4144-851b-19eaac3be507",
   "metadata": {
    "id": "16766ddc-e3e1-4144-851b-19eaac3be507"
   },
   "outputs": [],
   "source": [
    "# !pip install incredible\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import exists as file_exists\n",
    "from yaml import safe_load\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915c76d-a24f-40ec-ab6b-f7328cfb486d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3915c76d-a24f-40ec-ab6b-f7328cfb486d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9246af7159cb018307d07f006b9e0ffa",
     "grade": true,
     "grade_id": "datapath",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thisTutorial = 'clredshift'\n",
    "if getcwd() == '/content':\n",
    "    # assume we are in Colab, and the user's data directory is linked to their drive/Physics267_data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    datapath = '/content/drive/MyDrive/Physics267_data/' + thisTutorial + '/'\n",
    "else:\n",
    "    # assume we are running locally somewhere and have the data under ./data/\n",
    "    datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb15851-a23d-4793-8a5a-b72d45ad4430",
   "metadata": {
    "id": "9fb15851-a23d-4793-8a5a-b72d45ad4430"
   },
   "source": [
    "## Background\n",
    "\n",
    "Since the emphasis here is on sampling methods, we'll use a relatively simple data set and model. The data are spectroscopically determined redshifts for galaxies in cluster XLSSC 122 ([Willis et al. 2020](https://ui.adsabs.harvard.edu/abs/2020Natur.577...39W)). The cluster was discovered through its X-ray emission, and intially assigned a photometrically determined redshift of $1.9 \\pm 0.2$. This is (still, at this writing) an extremely high redshift for a massive cluster, and as a result pinning it down more precisely (i.e. spectroscopically) required space-based data, hence the Hubble observations presented by Willis et al. (In fact, this is a rare case where the first spectroscopic redshift came from follow-up X-ray data... which is not relevant, since the Hubble redshift is 100 times more precise, but still fun.)\n",
    "\n",
    "Here is the top of the data table from the paper, showing redshifts measured for individual galaxies. We'll be using these galaxy redshifts to estimate the redshift of the cluster itself, assuming they follow a known distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7894b-4c94-4ef7-98fe-e324cd177c86",
   "metadata": {
    "id": "03e7894b-4c94-4ef7-98fe-e324cd177c86"
   },
   "source": [
    "ID    | RA       | Dec      | Magnitude  | Colour  | Redshift  | Notes\n",
    "----- | -----    | -----    | -----      | -----   | -----     | -----\n",
    "526   | 34.43422 | -3.75880 | 20.64      | 1.44    | 1.980     | G\n",
    "451   | 34.42228 | -3.76351 | 21.95      | 1.29    | 1.981     | G\n",
    "657   | 34.43410 | -3.75766 | 21.67      | 1.49    | 1.983     | G\n",
    "1032  | 34.43245 | -3.74992 | 22.38      | 1.33    | 1.982     | G\n",
    "295   | 34.43503 | -3.76795 | 22.50      | 1.56    | 1.987     | G\n",
    "...   | ...      | ...      | ...        | ...     | ...       | ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ae11e-dd3b-4a55-a95d-d21883e542f2",
   "metadata": {
    "id": "489ae11e-dd3b-4a55-a95d-d21883e542f2"
   },
   "source": [
    "The data in the public data directory are the \"G\" and \"GE\" class redshifts from this table, which is a bit sketchy, as a redshift cut is already used in those classifications. We're keeping it simple in this notebook, and just using the data to demonstrate some sampling methods. One of the \"Practice\" tutorials involves fitting a cluster galaxy redshift distribution simultaneously with a background distribution of non-cluster galaxy redshifts, if you're interested.\n",
    "\n",
    "In any case, as usual, the data you'll be using have been randomly generated just for you! Let's read them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ad90a-fc10-4617-8c7f-e0fccf41e35d",
   "metadata": {
    "id": "f57ad90a-fc10-4617-8c7f-e0fccf41e35d"
   },
   "outputs": [],
   "source": [
    "z_gal = np.loadtxt(datapath+'redshifts.txt')\n",
    "z_gal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2ce9e-2abe-49f9-af22-843aebdfe3d6",
   "metadata": {
    "id": "38f2ce9e-2abe-49f9-af22-843aebdfe3d6"
   },
   "source": [
    "This will show a histogram of the galaxy redshifts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd81e3-b64c-418f-b376-831f73d5c3b7",
   "metadata": {
    "id": "8bfd81e3-b64c-418f-b376-831f73d5c3b7"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.0, 3.0)\n",
    "plt.hist(z_gal);\n",
    "plt.xlabel('redshift');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64845d0-6ada-4168-86d0-bf793f04de87",
   "metadata": {
    "id": "f64845d0-6ada-4168-86d0-bf793f04de87"
   },
   "source": [
    "## Define the model and priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0fef8-e5ab-442e-b266-f0b30ccd1e28",
   "metadata": {
    "id": "d6e0fef8-e5ab-442e-b266-f0b30ccd1e28"
   },
   "source": [
    "Physically, we expect the distribution of cluster member velocities (hence redshifts) to be approximately Gaussian. Our model will therefore simply be that the redshifts above follow a Gaussian distribution, whose mean is the cluster redshift. The width of the distrbution is related to the mass of the cluster, although we won't pursue that here other than obtaining constraints on it.\n",
    "\n",
    "Because it's most natural for the conjugate Gibbs method we'll use, let's parameterize the model with\n",
    "* the Gaussian's mean, $z_\\mathrm{cl}$, and\n",
    "* its _variance_, $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f1c97-071d-4c62-8816-cd2add33100b",
   "metadata": {
    "id": "d22f1c97-071d-4c62-8816-cd2add33100b"
   },
   "outputs": [],
   "source": [
    "param_names = ['z_cl', 'sigma2']\n",
    "param_labels = [r'$z_\\mathrm{cl}$', r'$\\sigma^2$']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab74a4-da72-4afe-8025-04671e03bd05",
   "metadata": {
    "id": "6cab74a4-da72-4afe-8025-04671e03bd05"
   },
   "source": [
    "For actual humans, it's usually more intuitive to think about the standard deviation rather than variance, so once we have our samples we'll transform $\\sigma^2$ back to $\\sigma$. This is a little awkward, but it's nice to take advantage of conjugacies when possible, so we'll deal with it this one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b89e4-1de9-4389-8f99-3ca293da7347",
   "metadata": {
    "id": "ff0b89e4-1de9-4389-8f99-3ca293da7347"
   },
   "outputs": [],
   "source": [
    "final_names = ['z_cl', 'sigma']\n",
    "final_labels = [r'$z_\\mathrm{cl}$', r'$\\sigma$']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b774c-115f-4f96-9c63-88ddd2135b3e",
   "metadata": {
    "id": "3d6b774c-115f-4f96-9c63-88ddd2135b3e"
   },
   "source": [
    "As always, you will need to choose priors for parameters. Again to take advantage of conjugacies, we'll want to use a Gaussian prior on $z_\\mathrm{cl}$ and an inverse-gamma (!) prior on $\\sigma^2$; these are both 2-parameter distributions and, while not infinitely flexible, both admit a wide range of behaviors, including common \"uninformative\" choices. This is probably the last tutorial where we will be so prescriptive about the form of priors.\n",
    "\n",
    "**Note: In class we will discuss and agree on a common set of priors that everyone should use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f48aa3-667d-4050-b6d0-061009ab7944",
   "metadata": {
    "id": "98f48aa3-667d-4050-b6d0-061009ab7944"
   },
   "source": [
    "Since there's no reason you should already be familiar with it, the inverse-gamma density is\n",
    "\n",
    "$\\mathrm{InvGamma}(x|\\alpha,\\beta) \\propto x^{-\\alpha-1} e^{-\\beta/x}$ **put constants in**\n",
    "\n",
    "for $x\\geq0$, $\\alpha>0$ and $\\beta>0$.\n",
    "\n",
    "If you stare at this for a while, you'll see that certain \"uninformative\" priors that we might want to use (e.g. uniform in $\\sigma^2$, uniform in $\\sigma$, or the Jeffreys prior), can be accomodated - but only by using values of $\\alpha$ and/or $\\beta$ that are not allowed by the distribution's definition. **This is fine.** All that actually matters is that the parameters of the conditional posterior are allowed, which is not the same as requiring the parameters of the prior distribution to be allowed. Math, but true. These improper priors will, however, cause `scipy.stats.invgamma` to break if we try to evaluate the prior; so, just in case we choose to use one of them, it will be helpful to have the PDF and log-PDF coded in a form that retains the correct functional dependences but omits the problematic normalizing constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4158cd6-ab70-4b7a-9417-a53d6426397f",
   "metadata": {
    "id": "d4158cd6-ab70-4b7a-9417-a53d6426397f"
   },
   "outputs": [],
   "source": [
    "# These are lazy but functional - you may see runtime warnings about dividing by or taking the log of zero\n",
    "def invgamma_unpdf(x, alpha, beta):\n",
    "    'Inverse-gamma PDF without the normalization constants'\n",
    "    return np.where(x<=0, 0.0, x**(-alpha-1) * np.exp(-beta/x))\n",
    "def invgamma_unlogpdf(x, alpha, beta):\n",
    "    'Inverse-gamma log-PDF without the normalization constants'\n",
    "    return np.where(x<=0, -np.inf, (-alpha-1)*np.log(x) -beta/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ed8b4-bda5-4980-a003-5827f1acb01d",
   "metadata": {
    "id": "e85ed8b4-bda5-4980-a003-5827f1acb01d"
   },
   "source": [
    "Before going on, draw the PGM and specify all the probabilistic relationships in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60964863-62ee-4eb2-835f-a15e81860a16",
   "metadata": {
    "id": "60964863-62ee-4eb2-835f-a15e81860a16",
    "tags": []
   },
   "source": [
    "> Space for PGM and generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8994df7-0ad4-4e02-b2c1-677f041c7223",
   "metadata": {
    "id": "f8994df7-0ad4-4e02-b2c1-677f041c7223"
   },
   "source": [
    "Next, define the hyperparameters corresponding to the priors you chose above:\n",
    "```\n",
    "mu0: mean of the Gaussian prior on z_cl\n",
    "tau0: STANDARD DEVIATION of the Gaussian prior on z_cl\n",
    "alpha0: alpha parameter of the inverse-gamma prior on sigma2\n",
    "beta0: beta parameter of the inverse-gamma prior on sigma2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a58914-028a-4ea5-88e8-eec0d5766e20",
   "metadata": {
    "deletable": false,
    "id": "03a58914-028a-4ea5-88e8-eec0d5766e20",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b4b6f2ba8090d6fd9726db6509f237f",
     "grade": false,
     "grade_id": "hyper",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparams = {'mu0':..., 'tau0':..., 'alpha0':..., 'beta0':...}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3aa33-4d87-4f39-8e47-0371cc06d234",
   "metadata": {
    "id": "a2e3aa33-4d87-4f39-8e47-0371cc06d234"
   },
   "source": [
    "Let's have a look at the priors PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ce63-8af3-4557-97ef-846a0731f0ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4ad3ce63-8af3-4557-97ef-846a0731f0ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c12cac1a1ca839a70e21dda02063dc19",
     "grade": true,
     "grade_id": "test_hyper",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Normal prior hyperparameters for z_cl:', hyperparams['mu0'], hyperparams['tau0'])\n",
    "print('Inverse-gamma prior hyperparameters for sigma2:', hyperparams['alpha0'], hyperparams['beta0'])\n",
    "plt.rcParams['figure.figsize'] = (10.0, 3.0)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "xx = np.linspace(hyperparams['mu0']-5*hyperparams['tau0'], hyperparams['mu0']+5*hyperparams['tau0'], 1000)\n",
    "ax[0].plot(xx, st.norm.pdf(xx, hyperparams['mu0'], hyperparams['tau0']));\n",
    "ax[0].set_xlabel(param_labels[0]); ax[0].set_ylabel(r'$p($'+param_labels[0]+r'$)$');\n",
    "xx = np.linspace(1e-4, 0.004, 1000) # limits are hacky\n",
    "ax[1].plot(xx, invgamma_unpdf(xx, hyperparams['alpha0'], hyperparams['beta0']));\n",
    "ax[1].set_xlabel(param_labels[1]); ax[1].set_ylabel(r'$p($'+param_labels[1]+r'$)$ (unnormalized)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101ba2b-f67e-47f1-8329-282361432f85",
   "metadata": {
    "id": "3101ba2b-f67e-47f1-8329-282361432f85"
   },
   "source": [
    "## Solution using conjugate Gibbs sampling\n",
    "\n",
    "### Conjugacy relations\n",
    "\n",
    "Recall that the strength of conjugate Gibbs sampling is that, where applicable, it lets us tailor a very efficient sampler to a given problem. The cost of this is having to juggle a little math. Given that you can look up all solutions for the common scenarios in [the Wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior), we won't make you work out all of the algebra, but it's worth doing if you are so inclined (and not terribly involved).\n",
    "\n",
    "#### Relation for $z_\\mathrm{cl}$\n",
    "\n",
    "Thanks to our choice of a normal prior, the fully conditional posterior for $z_\\mathrm{cl}$ is proportional to a product of exclusively Gaussians:\n",
    "\n",
    "$p(z_\\mathrm{cl}|z,\\sigma^2) \\propto p(z_\\mathrm{cl}) \\prod_i p(z_i|z_\\mathrm{cl},\\sigma^2) = \\mathrm{Normal}(z_\\mathrm{cl}|\\mu_0,\\tau_0) \\prod_i \\mathrm{Normal}(z_i|z_\\mathrm{cl},\\sigma)$,\n",
    "\n",
    "where $z$ is the vector of galaxy redshifts. First, we'll swap $z_i$ and $z_\\mathrm{cl}$ in the second term, using the symmetry of the Gaussian density with respect to these two parameters:\n",
    "\n",
    "$p(z_\\mathrm{cl}|z,\\sigma^2) \\propto \\mathrm{Normal}(z_\\mathrm{cl}|\\mu_0,\\tau_0) \\prod_i \\mathrm{Normal}(z_\\mathrm{cl}|z_i,\\sigma)$.\n",
    "\n",
    "We can then take advantage of an _extremely_ useful identity for the product of two Gaussians, which can be written\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu_1,\\sigma_1) \\, \\mathrm{Normal}(x|\\mu_2,\\sigma_2) = \\mathrm{Normal}(x|\\mu_a,\\sigma_a) \\, \\mathrm{Normal}(0|\\mu_b,\\sigma_b)$,\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mu_a = \\sigma_a^{2}\\left(\\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2}\\right)$,\n",
    "* $\\sigma_a^2 = \\left(\\sigma_1^{-2} + \\sigma_2^{-2}\\right)^{-1}$,\n",
    "* $\\mu_b = \\mu_1 - \\mu_2$,\n",
    "* $\\sigma_b^2 = \\sigma_1^2 + \\sigma_2^2$.\n",
    "\n",
    "That is, we end up with the product a Gaussian PDF for $x$, whose mean is a weighted average of the original means and whose width is smaller than either original width; and a second Gaussian PDF _that doesn't depend on $x$_, whose mean is the difference between the original means, and whose variance is the quadrature sum of the original variances. If the original expression represents two independent constraints on $x$, this has a natural interpretation: the first term on the right encodes the joint constraint, while the second (which could be rewritten $\\mathrm{Normal}[\\mu_1|\\mu_2,\\sigma_b]$) is encodes the agreement between the two original constraints.\n",
    "\n",
    "In any case, repeated application of this identity simplifies our conditional posterior to\n",
    "\n",
    "$p(z_\\mathrm{cl}|z,\\sigma^2) = \\mathrm{Normal}\\left(z_\\mathrm{cl}\\left|\\left[\\frac{1}{\\tau_0^2}+\\frac{N}{\\sigma^2}\\right]^{-1}\\left[\\frac{\\mu_0}{\\tau_0^2}+\\frac{\\sum_i z_i}{\\sigma^2}\\right], \\left[\\frac{1}{\\tau_0^2}+\\frac{N}{\\sigma^2}\\right]^{-1/2}\\right.\\right)$,\n",
    "\n",
    "where $N$ is the length of $z$.\n",
    "\n",
    "Here we have only kept the first term when using the above identity each time, since we only need to deal with terms that depend on $z_\\mathrm{cl}$. In this case, that yields a normalized PDF, so we can call this last line an equality automatically. That is, if we had kept track of those second terms from the identity, they would certainly end up exactly canceling the evidence, such that the conditional posterior ends up being normalized. This is known as a shortcut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba232ca6-8b96-4997-a42b-d4ef7b16a1d9",
   "metadata": {
    "id": "ba232ca6-8b96-4997-a42b-d4ef7b16a1d9"
   },
   "source": [
    "#### Relation for $\\sigma^2$\n",
    "\n",
    "The conjugate distribution for $\\sigma^2$ in this problem is inverse-gamma, which is why we used it for the prior above. (You may also see the scaled-inverse-chisquare distribution mentioned; this is a special case of inverse-gamma, so it also works.) As noted above, the functional form of this distribution is\n",
    "\n",
    "$\\mathrm{InvGamma}(x|\\alpha,\\beta) \\propto x^{-\\alpha-1} e^{-\\beta/x}$.\n",
    "\n",
    "So, the conditional posterior for $\\sigma^2$ looks like\n",
    "\n",
    "$p(\\sigma^2|z,z_\\mathrm{cl}) \\propto p(\\sigma^2) \\prod_i p(z_i|z_\\mathrm{cl},\\sigma^2) \\propto \\sigma^{-2(\\alpha_0+1)} e^{-\\beta_0/\\sigma^2} \\prod_i \\frac{1}{\\sigma} e^{-\\frac{(z_i-z_\\mathrm{cl})^2}{2\\sigma^2}}$,\n",
    "\n",
    "where we have expanded out the Gaussian PDF in the sampling distribution and thrown away terms that do not involve $\\sigma$. For not much effort, you can simplify this to an expression with the same form as the inverse-gamma density above, with $\\alpha=\\alpha_0+N/2$ and $\\beta = \\beta_0 + \\frac{1}{2}\\sum_i (z_i - z_\\mathrm{cl})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec41de-e4d3-469e-8df6-83589476956a",
   "metadata": {
    "id": "6fec41de-e4d3-469e-8df6-83589476956a"
   },
   "source": [
    "### Implementation\n",
    "\n",
    "So, now we have rules from drawing samples from the conditional posterior distributions of both parameters. Note that this is not the same as being able to sample both parameters in a single step; we still need to construct a Markov chain by updating the parameters in series.\n",
    "\n",
    "That being the case, we will need an initial position in parameter space at which to start the chain. Any reasonable guess will do, given that this is a fairly simple problem. (An unreasonable guess will likely also work, although we don't recommend it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72a02e-3466-48f8-9936-5512453b3fe9",
   "metadata": {
    "deletable": false,
    "id": "3a72a02e-3466-48f8-9936-5512453b3fe9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a61bee5c6777834e78d94dffc596fd1d",
     "grade": false,
     "grade_id": "guess",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# guess = {'z_cl':..., 'sigma2':...}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d76c-35be-4abe-9b3b-0f403c6a5ca6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "78e7d76c-35be-4abe-9b3b-0f403c6a5ca6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "352ccc5d465aecc8d7b9a612a681331d",
     "grade": true,
     "grade_id": "hidden_update_defs",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175945c-889f-4609-93b6-14da31f6b363",
   "metadata": {
    "id": "b175945c-889f-4609-93b6-14da31f6b363"
   },
   "source": [
    "Now for the fun part: write a function that takes the data (`z_gal`), model parameter dictionary and prior hyperparameter dictionary as input, and returns the parameters of the conditional posterior for $z_\\mathrm{cl}$ Even though you'll have chosen specific hyperparameter already, you should write your code such that any valid values can be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94022795-2c54-45e0-9d5d-31301d2fbffa",
   "metadata": {
    "deletable": false,
    "id": "94022795-2c54-45e0-9d5d-31301d2fbffa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d66ec16adc2bc0eca23ef17a34938e38",
     "grade": false,
     "grade_id": "update_zcl",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conditional_post_z_cl(data, par, hypar):\n",
    "    # return a tuple (mean, stdev) encoding the normal distribution from which z_cl should be drawn\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97482215-cb5a-4243-b0c5-1bee70e63b3d",
   "metadata": {
    "id": "97482215-cb5a-4243-b0c5-1bee70e63b3d"
   },
   "source": [
    "Now do the same for $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295bfcb-2256-4f71-a4af-91de1665befd",
   "metadata": {
    "deletable": false,
    "id": "c295bfcb-2256-4f71-a4af-91de1665befd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ad720f77301206ca06719c4aadff153",
     "grade": false,
     "grade_id": "update_sigma2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conditional_post_sigma2(data, par, hypar):\n",
    "    # return a tuple (alpha, beta) encoding the inverse-gamma distribution from which sigma2 should be drawn\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41972724-5453-4dc6-b676-2a9df49f23d8",
   "metadata": {
    "id": "41972724-5453-4dc6-b676-2a9df49f23d8"
   },
   "source": [
    "This will test that each function works correctly for a given, arbitrary input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4a523-b9b0-4a98-8f0a-ccd30bf58753",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "32a4a523-b9b0-4a98-8f0a-ccd30bf58753",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "291f54d75bbf76e93d1f2308e0a811eb",
     "grade": true,
     "grade_id": "test_updates",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "condpost_test = safe_load(open(datapath+'test_condpost.yaml', 'r').read())\n",
    "assert np.allclose(conditional_post_z_cl(z_gal, condpost_test['testpar'], condpost_test['testhypar']), condpost_test['z_cl'])\n",
    "assert np.allclose(conditional_post_sigma2(z_gal, condpost_test['testpar'], condpost_test['testhypar']), condpost_test['sigma2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6fb8f-3999-4f97-b9e4-5c82d0fee1cd",
   "metadata": {
    "id": "edc6fb8f-3999-4f97-b9e4-5c82d0fee1cd"
   },
   "source": [
    "If those conjugacy definitions are working, we can move on. The following, given functions take the same arguments, and will update the parameter dictionary _in place_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824d89e-7461-434c-b27c-0f8cf81aa663",
   "metadata": {
    "id": "1824d89e-7461-434c-b27c-0f8cf81aa663"
   },
   "outputs": [],
   "source": [
    "def update_z_cl(data, par, hypar):\n",
    "    mean,sd = conditional_post_z_cl(data, par, hypar)\n",
    "    par['z_cl'] = st.norm.rvs(mean, sd)\n",
    "\n",
    "def update_sigma2(data, par, hypar):\n",
    "    alpha,beta = conditional_post_sigma2(data, par, hypar)\n",
    "    par['sigma2'] = st.invgamma.rvs(alpha, scale=beta)\n",
    "    #par['sigma2'] = 1 / st.gamma.rvs(alpha, scale=1/beta) # equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d817ff-484f-45ac-a2e3-40342d429039",
   "metadata": {
    "id": "c6d817ff-484f-45ac-a2e3-40342d429039"
   },
   "source": [
    "Finally, let's test all of that by calling each function and verifying that all the parameters have changed (to finite, allowed values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46595e90-4785-4bf5-93d9-57551b054195",
   "metadata": {
    "id": "46595e90-4785-4bf5-93d9-57551b054195"
   },
   "outputs": [],
   "source": [
    "params = guess.copy()\n",
    "print('Before:', params)\n",
    "update_z_cl(z_gal, params, hyperparams)\n",
    "update_sigma2(z_gal, params, hyperparams)\n",
    "print('After:', params)\n",
    "print('Difference:', {k:params[k]-guess[k] for k in params.keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f461c0-ba34-4ad3-86d8-dc6a06c95b37",
   "metadata": {
    "id": "c8f461c0-ba34-4ad3-86d8-dc6a06c95b37"
   },
   "source": [
    "### Results\n",
    "\n",
    "We now have the necessary machinery to build a Markov chain, starting from your guessed parameter values above, and updating each parameter in turn. The cell below will do so, storing the chain in an $N_\\mathrm{samples}\\times2$ array. The order of the individual parameter updates is arbitrary, and could even be randomized if you particularly wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c9584-d434-4ff0-b465-14ee511efdae",
   "metadata": {
    "id": "ce1c9584-d434-4ff0-b465-14ee511efdae"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params = guess.copy()\n",
    "nsamples = 10000\n",
    "gchain = np.zeros((nsamples, len(params)))\n",
    "for i in range(nsamples):\n",
    "    update_z_cl(z_gal, params, hyperparams)\n",
    "    update_sigma2(z_gal, params, hyperparams)\n",
    "    gchain[i,:] = [params[k] for k in param_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d386b7-0998-402e-b160-55e7912f9611",
   "metadata": {
    "id": "12d386b7-0998-402e-b160-55e7912f9611"
   },
   "source": [
    "Let's do the most basic (yet still extremely important) visual check to see how our sampler performed, looking at traces of the Markov chain for each parameter. (It's ok if you haven't read the notes on MCMC Diagnostics yet; we will go more in-depth later.) These trace plots show the value of each parameter as a function of iteration, and we'll add a line showing your initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dda53-db7c-49d5-916e-7a8a8d088194",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "637dda53-db7c-49d5-916e-7a8a8d088194",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29c3fb7aca38a6c708289291b1e92040",
     "grade": true,
     "grade_id": "copy_gchain",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(gchain.shape[1], 1, figsize=(20, gchain.shape[1]*3));\n",
    "cr.plot_traces(gchain, ax, labels=param_labels, truths=[guess[k] for k in param_names]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd1d57-4a20-44ef-9752-89671fdc0705",
   "metadata": {
    "id": "2fdd1d57-4a20-44ef-9752-89671fdc0705"
   },
   "source": [
    "Let's go ahead and transform the samples from $\\sigma^2$ to $\\sigma$, since that's what we're more interested in. (This is not a trick question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d43ca3-79f1-452f-a88e-95a72dde7f7b",
   "metadata": {
    "deletable": false,
    "id": "b7d43ca3-79f1-452f-a88e-95a72dde7f7b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8aca0da84829ec173d079fa9e855deb",
     "grade": false,
     "grade_id": "sigma2_to_sigma",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform gchain[:,1] from sigma^2 to sigma\n",
    "# gchain[:,1] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3195a7-9a64-4568-baf2-8e14f5103083",
   "metadata": {
    "id": "bc3195a7-9a64-4568-baf2-8e14f5103083"
   },
   "source": [
    "The following cells will find the 1D and 2D credible regions, as you've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca7773-1909-4178-8e23-2c8e70972105",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "95ca7773-1909-4178-8e23-2c8e70972105",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f826ef7a372a3accdba6af84ff8de258",
     "grade": true,
     "grade_id": "test_sigma",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(gchain.shape[1], 2, figsize=(9, gchain.shape[1]*3));\n",
    "gCIs = {}\n",
    "for i,a in enumerate(ax):\n",
    "    h = cr.whist(gchain[:,i], plot=a[0]); a[0].set_xlabel(final_labels[i]);\n",
    "    gCIs[final_names[i]] = cr.whist_ci(h, plot=a[1]);\n",
    "    a[1].set_xlabel(final_labels[i]);\n",
    "gCIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86772249-7760-4580-928b-bf9bbe82f395",
   "metadata": {
    "id": "86772249-7760-4580-928b-bf9bbe82f395"
   },
   "outputs": [],
   "source": [
    "gtri = cr.whist_triangle(gchain, bins=50, smooth2D=1);\n",
    "cr.whist_triangle_plot(gtri, paramNames=final_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663733a-a2f2-4041-9e62-f43a2c9ab358",
   "metadata": {
    "id": "6663733a-a2f2-4041-9e62-f43a2c9ab358"
   },
   "source": [
    "### Check goodness of fit\n",
    "\n",
    "Just this once, since the whole point of the tutorial is to get familiar with sampling techniques, we will skip testing the goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854bc1b-85f0-42d7-88d5-b647b9017f7c",
   "metadata": {
    "id": "e854bc1b-85f0-42d7-88d5-b647b9017f7c"
   },
   "source": [
    "### Run multiple chains\n",
    "\n",
    "We weren't overly concerned with the starting point for the test chain above. But, for later notebooks, we'll want to see how multiple, independent chains with different starting points behave when using this method. The cell below will take care of running 4 chains, started at random yet broadly reasonable positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d29403-a2b9-4576-83ba-0abf4d05f40f",
   "metadata": {
    "id": "47d29403-a2b9-4576-83ba-0abf4d05f40f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nchains = 4\n",
    "gchains = [np.zeros((nsamples,len(param_names))) for j in range(nchains)]\n",
    "\n",
    "for samples in gchains:\n",
    "    # randomly initializing from within the prior is reasonable, unless it's improper\n",
    "    if np.isfinite(hyperparams['tau0']):\n",
    "        params = {'z_cl':st.norm.rvs(hyperparams['mu0'], hyperparams['tau0'])}\n",
    "    else:\n",
    "        params = {'z_cl':st.uniform.rvs(1.85, 0.3)} # just something to fall back on\n",
    "    if hyperparams['alpha0'] > 0 and hyperparams['beta0'] > 0:\n",
    "        params['sigma2'] = st.invgamma.rvs(hyperparams['alpha0'], scale=hyperparams['beta0'])\n",
    "    else:\n",
    "        params['sigma2'] = st.chi2.rvs(22) * 5e-6 # just something to fall back on\n",
    "    for i in range(nsamples):\n",
    "        update_z_cl(z_gal, params, hyperparams)\n",
    "        update_sigma2(z_gal, params, hyperparams)\n",
    "        samples[i,:] = [params[k] for k in param_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f8efb-e36f-4e61-961f-06dc4efa4f60",
   "metadata": {
    "id": "5d8f8efb-e36f-4e61-961f-06dc4efa4f60"
   },
   "source": [
    "Now we can look at a more colorful version of the trace plots, showing all of the chains simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef4c78-a687-4c9b-9c8c-f83c27d8ccb2",
   "metadata": {
    "id": "18ef4c78-a687-4c9b-9c8c-f83c27d8ccb2"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(gchain.shape[1], 1, figsize=(20, gchain.shape[1]*3));\n",
    "cr.plot_traces(gchains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c06ac9-fb52-4d46-a4f2-4d96b49c736c",
   "metadata": {
    "id": "42c06ac9-fb52-4d46-a4f2-4d96b49c736c"
   },
   "source": [
    "Here you might be able to see some extreme values as the chains burn in, before they settle down to sampling the posterior distribution, depending on where the chains were initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51d008-7cb7-45b9-a5f7-11e77498f35a",
   "metadata": {
    "id": "cc51d008-7cb7-45b9-a5f7-11e77498f35a"
   },
   "source": [
    "**Uncomment and run** the following cell to save the chains to your data folder. We will use them in a later tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded4e46-2ea8-4e82-8d21-b5929a97da14",
   "metadata": {
    "id": "0ded4e46-2ea8-4e82-8d21-b5929a97da14"
   },
   "outputs": [],
   "source": [
    "#for i,samples in enumerate(gchains):\n",
    "#    np.savetxt(datapath+'clredshift_gibbs_'+str(i)+'.txt.gz', samples, header=' '.join(param_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159785a9-59ee-4eed-933b-94950c24c661",
   "metadata": {
    "deletable": false,
    "id": "159785a9-59ee-4eed-933b-94950c24c661",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6958cad5f7e4d024d561f9e70fc66f4e",
     "grade": false,
     "grade_id": "savechains1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_have_saved_the_Gibbs_chains = False # you should not, in fact, need to change this to True\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bfdcd-2eb3-4891-bbea-1491c8d89487",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aa1bfdcd-2eb3-4891-bbea-1491c8d89487",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f04f6ddbef87c504f3694b91fd544f6",
     "grade": true,
     "grade_id": "define_pr_ll",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert all([file_exists(datapath+'clredshift_gibbs_'+str(i)+'.txt.gz') for i in range(nchains)]) or I_have_saved_the_Gibbs_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c7f81-b5fa-4399-8975-5a3fb0cd6dca",
   "metadata": {
    "id": "261c7f81-b5fa-4399-8975-5a3fb0cd6dca"
   },
   "source": [
    "## Solution using Metropolis sampling\n",
    "\n",
    "In the conjugate Gibbs solution, the sampling functions were explicitly tailored to our model definition. That won't be the case here, so the first thing to do is to implement log-prior and log-likelihood functions for this problem, just as you've done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2457258-fd79-4ac9-9d45-7f106e4ebba5",
   "metadata": {
    "deletable": false,
    "id": "e2457258-fd79-4ac9-9d45-7f106e4ebba5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd849ee378c097428910f7fa3bcc361e",
     "grade": false,
     "grade_id": "priorfun",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_prior(z_cl, sigma2, mu0, tau0, alpha0, beta0):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512679e0-1b35-42da-9fac-6ff0114d460e",
   "metadata": {
    "deletable": false,
    "id": "512679e0-1b35-42da-9fac-6ff0114d460e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb2ef6b3a451266317d78b828922953e",
     "grade": false,
     "grade_id": "likefun",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_likelihood(data, z_cl, sigma2, **unused_kwargs):\n",
    "    # unused_kwargs is there so we can pass hyperparameters without crashing,\n",
    "    # not that we would/could use them in the likelihood\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6a040-c333-4b13-940b-931128c79ab2",
   "metadata": {
    "id": "ccb6a040-c333-4b13-940b-931128c79ab2"
   },
   "source": [
    "The log-posterior function is given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1345c25-07be-4760-8535-73c2cc649746",
   "metadata": {
    "id": "f1345c25-07be-4760-8535-73c2cc649746"
   },
   "outputs": [],
   "source": [
    "def log_posterior(data, **allparams):\n",
    "    lnp = log_prior(**allparams)\n",
    "    if np.isfinite(lnp):\n",
    "        lnp += log_likelihood(data, **allparams)\n",
    "    return lnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534d180-aa09-4a4a-8fdd-39d1d6f9ae1a",
   "metadata": {
    "id": "1534d180-aa09-4a4a-8fdd-39d1d6f9ae1a"
   },
   "source": [
    "As always, let's check that they return finite numbers for reasonable parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb5da8-30c8-41f2-be97-3ea6ef36f433",
   "metadata": {
    "id": "3ddb5da8-30c8-41f2-be97-3ea6ef36f433"
   },
   "outputs": [],
   "source": [
    "print(log_prior(**guess, **hyperparams))\n",
    "print(log_likelihood(z_gal, **guess, **hyperparams))\n",
    "print(log_posterior(z_gal, **guess, **hyperparams))\n",
    "assert np.isfinite(log_posterior(z_gal, **guess, **hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68b30d-9d90-45c6-9d64-a8327811b717",
   "metadata": {
    "id": "9f68b30d-9d90-45c6-9d64-a8327811b717"
   },
   "source": [
    "Moving on to the sampler, we next need a proposal distribution. While in principle you can do something fancier here, I'll suggest using a multivariate Gaussian centered on the current position in parameter space. This is translationally invariant, so later on you can use the simple Metropolis acceptance rule instead of the slightly more complex Metropolis-Hastings rule. A Gaussian isn't necessarily the best choice in general, since the most likely proposals are very small steps, but it will do for this demonstration.\n",
    "\n",
    "Also in the name of keeping it simple, let's make the proposal independent in each parameter (a diagonal covariance matrix for the 2-dimensional Gaussian). Similarly to the grid method, you'll want to guess the appropriate order of magnitude for steps in each parameter, which is the same order as the width of the posterior, and you may need to return to this point to adjust them after seeing the performance.\n",
    "\n",
    "Since we're assuming a diagonal covariance, let's go ahead and just represent the proposal distribution as 2 univariate Gaussians, as in the dictionary below.\n",
    "\n",
    "**Aside:** You may not have seen it before in this class, but calling `scipy.norm()` as below produces what they call a \"frozen\" probability distribution object, with fixed parameters. The standard deviation is whatever you specify via the `scale` argument, and the (unspecified) mean would remain at the default value of 0.0. This means that each entry should be interpreted as a distribution for the **displacement** of a proposal from the current position of a chain. In other words, the proposal distributon density for a change to $x_0$ could be computed with `proposal_distribution['x0'].pdf(x0_proposed - x0_current)`. Other methods of `scipy` probability distributions, in particular random number generation, are also available; the main difference from how we've used them before is that we can't/don't need to specify parameter values in the call because they've already been frozen in.\n",
    "\n",
    "You are not required to follow the advice above, so long as your definitions of `proposal_distribution`, `propose` and `step` are all mutually consistent and functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352089f-5490-48e6-89e2-a130fbf9683a",
   "metadata": {
    "deletable": false,
    "id": "8352089f-5490-48e6-89e2-a130fbf9683a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43f567b62e24a7269682a7ec8ed270ba",
     "grade": false,
     "grade_id": "proposaldist",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proposal_distribution = {'z_cl':st.norm(scale=...) ,\n",
    "#                          'sigma2':st.norm(scale=...)}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b971d3f-83f5-4b95-ae20-09de03e30c3a",
   "metadata": {
    "id": "4b971d3f-83f5-4b95-ae20-09de03e30c3a"
   },
   "source": [
    "Next, define a function that returns a proposed point in parameter space, given the current location and the above dictionary of proposal distributions.\n",
    "\n",
    "**Technical note:** You might be tempted to begin this function with a command like `proposal = current_params`. If so, remember that, in Python, `b = a` does not make a copy of `a` if `a` is a dictionary (or a `numpy` array for that matter). Both `b` and `a` would point to the same data in memory. The safest/quickest way to get a new dictionary with the same structure as `a` whose values can then be safely overwritten is with `b = a.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922fb4e-b1a2-40ef-9e0a-b3228ff908d3",
   "metadata": {
    "deletable": false,
    "id": "9922fb4e-b1a2-40ef-9e0a-b3228ff908d3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2374d6e5b06bfcdbe563c6b4f5d83b14",
     "grade": false,
     "grade_id": "proposefun",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def propose(current_params, dists):\n",
    "    \"\"\"\n",
    "    current_params: dictionary holding current position in parameter space\n",
    "    dists: dictionary of proposal distributions\n",
    "\n",
    "    Return value: a new dictionary holding the proposed destination in parameter space\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7904425-7416-43f2-a408-274df50cd315",
   "metadata": {
    "id": "d7904425-7416-43f2-a408-274df50cd315"
   },
   "source": [
    "Let's make sure it does, indeed, propose new parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e87d66-65c5-405c-b1a6-989b0b447704",
   "metadata": {
    "id": "42e87d66-65c5-405c-b1a6-989b0b447704"
   },
   "outputs": [],
   "source": [
    "print('Test starting position:', guess)\n",
    "params = propose(guess, proposal_distribution)\n",
    "print('Test proposal:', params)\n",
    "print('Difference:', {k:params[k]-guess[k] for k in params.keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1aa2d2-39f3-4821-a206-79eb65a94da9",
   "metadata": {
    "id": "ac1aa2d2-39f3-4821-a206-79eb65a94da9"
   },
   "source": [
    "Finally, the sampler itself. Write a function that takes the current parameter values and log-posterior value as input (along with the data and proposal distributions), and returns the next set of parameters values and corresponding log-posterior as a tuple. These should be identical to the inputs if the proposal is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3cc2-52ea-46cf-8dc9-a7d1ccba3329",
   "metadata": {
    "deletable": false,
    "id": "a42b3cc2-52ea-46cf-8dc9-a7d1ccba3329",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45ef1ebe4aa502141edcce9a108dc96f",
     "grade": false,
     "grade_id": "stepfun",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def step(data, current_params, current_lnP, proposal_dists, hyperparameters):\n",
    "    \"\"\"\n",
    "    data: the data\n",
    "    current_params: dictionary of parameter values\n",
    "    current_lnP: log-posterior density corresponding to current_params\n",
    "    proposal_dists: dictionary of proposal distributions\n",
    "    hyperparameters: dictionary of prior hyperparameter values\n",
    "\n",
    "    Return value: a tuple holding the next parameter dictionary and corresponding log-posterior density\n",
    "    \"\"\"\n",
    "    # trial_params = ...\n",
    "    # trial_lnP = ...\n",
    "    # if [accept/reject condition]:\n",
    "    #    return (trial_params, trial_lnP)\n",
    "    # else:\n",
    "    #    return (current_params, current_lnP)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0db0e1-52a0-404d-bb9c-9ea5eaaad7cd",
   "metadata": {
    "id": "0c0db0e1-52a0-404d-bb9c-9ea5eaaad7cd"
   },
   "source": [
    "And, again, make sure it works without crashing. We'll essentially just run a short chain to verify that the final position is not the same as the initial one. If it is the same, either some of the functions above are buggy, or the proposal distribution is so poor for this problem that it should be tweaked before moving on. The cell will run 100 iterations, though ideally you would see _some_ movement within the first 10 proposals (printed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55e33c-5315-41e5-b26e-c8063198ed06",
   "metadata": {
    "id": "bb55e33c-5315-41e5-b26e-c8063198ed06"
   },
   "outputs": [],
   "source": [
    "guess_lnp = log_posterior(z_gal, **guess, **hyperparams)\n",
    "state = (guess, guess_lnp)\n",
    "for i in range(100):\n",
    "    state = step(z_gal, state[0], state[1], proposal_distribution, hyperparams)\n",
    "    if i < 10: print(state)\n",
    "assert guess_lnp != state[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65852cd-a3ca-43df-94ae-e6df9b6dcf98",
   "metadata": {
    "id": "e65852cd-a3ca-43df-94ae-e6df9b6dcf98"
   },
   "source": [
    "Assuming all this is working, let's run a chain and store the samples in an array, similar to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf6886-d65b-434d-9081-0ea7316e5563",
   "metadata": {
    "id": "b9cf6886-d65b-434d-9081-0ea7316e5563"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mchain = np.zeros((nsamples, len(param_names)))\n",
    "\n",
    "current_lnP = guess_lnp\n",
    "params = guess.copy()\n",
    "for i in range(mchain.shape[0]):\n",
    "    params,current_lnP = step(z_gal, params, current_lnP, proposal_distribution, hyperparams)\n",
    "    mchain[i,:] = [params[k] for k in param_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fad7-1e8f-433e-b19e-927043146331",
   "metadata": {
    "id": "c5c9fad7-1e8f-433e-b19e-927043146331"
   },
   "source": [
    "Once again, we'll look at the traces of each parameter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7fafe-643f-49b5-8331-74c4d72a1ddc",
   "metadata": {
    "id": "cfc7fafe-643f-49b5-8331-74c4d72a1ddc"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(mchain.shape[1], 1, figsize=(20, mchain.shape[1]*3));\n",
    "cr.plot_traces(mchain, ax, labels=param_labels, truths=[guess[k] for k in param_names]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b68a7-17b7-4842-9083-14ff6fd6ca29",
   "metadata": {
    "deletable": false,
    "id": "8e3b68a7-17b7-4842-9083-14ff6fd6ca29",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eef68664abda59e46754a2d7936eda30",
     "grade": false,
     "grade_id": "trans_sigma2_again",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform sigma^2 to sigma again\n",
    "# mchain[:,1] ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cc44f-dd10-431b-a425-b1542323bde4",
   "metadata": {
    "id": "f75cc44f-dd10-431b-a425-b1542323bde4"
   },
   "source": [
    "... and the various credible intervals/regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c12719-3205-4b42-9e3c-1b2fbb3ab342",
   "metadata": {
    "id": "02c12719-3205-4b42-9e3c-1b2fbb3ab342"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(mchain.shape[1], 2, figsize=(9, mchain.shape[1]*3));\n",
    "mCIs = {}\n",
    "for i,a in enumerate(ax):\n",
    "    h = cr.whist(mchain[:,i], plot=a[0]); a[0].set_xlabel(final_labels[i]);\n",
    "    mCIs[final_names[i]] = cr.whist_ci(h, plot=a[1]);\n",
    "    a[1].set_xlabel(final_labels[i]);\n",
    "mCIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907e465-ef65-4b37-9d7f-7294621c25b4",
   "metadata": {
    "id": "2907e465-ef65-4b37-9d7f-7294621c25b4"
   },
   "outputs": [],
   "source": [
    "mtri = cr.whist_triangle(mchain, bins=50, smooth2D=1);\n",
    "cr.whist_triangle_plot(mtri, paramNames=final_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8602d8-263c-462c-bfe5-d59817392a45",
   "metadata": {
    "id": "3a8602d8-263c-462c-bfe5-d59817392a45"
   },
   "source": [
    "### Compare with Gibbs\n",
    "\n",
    "You might have noticed that we didn't provide a check of the core functionality of your Metropolis sampler as we did with Gibbs. Don't worry! For this problem, the Gibbs sampler works extremely well, so we can just compare the results from both samplers to verify that the Metropolis code works.\n",
    "\n",
    "One caveat is that the comparisons below assume that both chains are predominantly sampling from the posterior correctly instead of struggling to even find their way to a place where the posterior is large from the starting position. If you see large, coherent movements taking up most of the traces in either case, it would be a good idea to adjust `guess`. Similarly, we need the proposal distribution for Metropolis to be good enough that the chain jumps around at least a bit instead of rejecting a large fraction of proposals. We'll discuss these issues more in the MCMC Diagnostics notes; for now, we just need things to be good enough that the two chains can be reasonably compared.\n",
    "\n",
    "The bottom line is that the Metropolis chain is likely to be less efficient than the Gibbs one for this problem, so (for the number of samples above) we can't insist that they resemble one another _too_ closely. The cell below will check whether some basic statistics of the chains, namely the \"centers\" and \"widths\" of each 1D marginalized posterior as defined by `incredible`, agree within 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efb79c-2a4b-4020-b838-be71401f6f65",
   "metadata": {
    "id": "06efb79c-2a4b-4020-b838-be71401f6f65"
   },
   "outputs": [],
   "source": [
    "for p in final_names:\n",
    "    print(p)\n",
    "    for n,CIs in zip(['Gibbs:', 'Metro:'],[gCIs, mCIs]):\n",
    "        print(\"  \", n, CIs[p]['center'][0], \"+/-\", CIs[p]['width'][0])\n",
    "assert np.isclose(mCIs['z_cl']['center'][0], gCIs['z_cl']['center'][0], rtol=1e-1)\n",
    "assert np.isclose(mCIs['z_cl']['width'][0], gCIs['z_cl']['width'][0], rtol=1e-1)\n",
    "assert np.isclose(mCIs['sigma']['center'][0], gCIs['sigma']['center'][0], rtol=1e-1)\n",
    "assert np.isclose(mCIs['sigma']['width'][0], gCIs['sigma']['width'][0], rtol=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9d23b-3624-4a32-9f96-d59f703270c1",
   "metadata": {
    "id": "86b9d23b-3624-4a32-9f96-d59f703270c1"
   },
   "source": [
    "That 10% does not require impressive agreement by any means, it's just something we chose to avoid this automatic check failing when the Metropolis chain is less than great (compared to what we could do using less basic proposal distributions, etc.). Therefore, also compare the credible regions from the two methods below - ideally, the Metropolis results (red) should just look like a jankier version of the Gibbs results (blue) rather than fundamentally different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2636c35-11c4-4cd2-8957-21843e7f631a",
   "metadata": {
    "id": "f2636c35-11c4-4cd2-8957-21843e7f631a"
   },
   "outputs": [],
   "source": [
    "fig,ax = cr.whist_triangle_plot(gtri, paramNames=final_labels, fill2D=False, linecolor1D='b', linecolor2D='b');\n",
    "cr.whist_triangle_plot(mtri, paramNames=final_labels, axes=ax, fill2D=False, linecolor1D='r', linecolor2D='r', linestyle1D='--', linestyle2D='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededb6ab-6e1f-46ce-9a40-11dc05ab62dc",
   "metadata": {
    "deletable": false,
    "id": "ededb6ab-6e1f-46ce-9a40-11dc05ab62dc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5c4090cb4f1534dab03dde3b8fa447b",
     "grade": false,
     "grade_id": "checkpoint1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "They_appear_to_agree_well = False # change to True when true\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517783ae-96dc-4b82-999a-632ff07852c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "517783ae-96dc-4b82-999a-632ff07852c5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12db87345775e074c5234813360726cd",
     "grade": true,
     "grade_id": "check1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert They_appear_to_agree_well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae892ad-b27f-47e8-b897-52969d0a1b6c",
   "metadata": {
    "id": "1ae892ad-b27f-47e8-b897-52969d0a1b6c"
   },
   "source": [
    "### Run multiple chains\n",
    "\n",
    "As before, we'll run 4 independent chains for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a184a2-375a-4bad-b388-daf9ba974751",
   "metadata": {
    "id": "04a184a2-375a-4bad-b388-daf9ba974751"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nchains = 4\n",
    "mchains = [np.zeros((nsamples,len(param_names))) for j in range(nchains)]\n",
    "\n",
    "for samples in mchains:\n",
    "    # randomly initializing from within the prior is reasonable, unless it's improper\n",
    "    if np.isfinite(hyperparams['tau0']):\n",
    "        params = {'z_cl':st.norm.rvs(hyperparams['mu0'], hyperparams['tau0'])}\n",
    "    else:\n",
    "        params = {'z_cl':st.uniform.rvs(1.85, 0.3)} # just something to fall back on\n",
    "    if hyperparams['alpha0'] > 0 and hyperparams['beta0'] > 0:\n",
    "        params['sigma2'] = st.invgamma.rvs(hyperparams['alpha0'], scale=hyperparams['beta0'])\n",
    "    else:\n",
    "        params['sigma2'] = st.chi2.rvs(22) * 5e-6 # just something to fall back on\n",
    "    current_lnP = log_posterior(z_gal, **params, **hyperparams)\n",
    "    for i in range(samples.shape[0]):\n",
    "        params,current_lnP = step(z_gal, params, current_lnP, proposal_distribution, hyperparams)\n",
    "        samples[i,:] = [params[k] for k in param_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6008e-4e2b-4784-9d9a-a7c656a8e884",
   "metadata": {
    "id": "3be6008e-4e2b-4784-9d9a-a7c656a8e884"
   },
   "source": [
    "Let's see what this version looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301addb5-169f-4411-8f85-93ed64825ce6",
   "metadata": {
    "id": "301addb5-169f-4411-8f85-93ed64825ce6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(mchain.shape[1], 1, figsize=(20, mchain.shape[1]*3));\n",
    "cr.plot_traces(mchains, ax, labels=param_labels, Line2D_kwargs={'markersize':1.0});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace3839-ab93-496d-89f6-a37d930725f7",
   "metadata": {
    "id": "bace3839-ab93-496d-89f6-a37d930725f7"
   },
   "source": [
    "As before, depending on where the chains were initialized, you might be able to see some extreme values as the chains burn in, before they settle down to sampling the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c160f-c724-477f-b52a-63b7c735a236",
   "metadata": {
    "id": "ea7c160f-c724-477f-b52a-63b7c735a236"
   },
   "source": [
    "**Uncomment and run** the following cell to save the chains to your data folder. We will use them in a later tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c22967-7acd-481d-a4aa-f094018e90de",
   "metadata": {
    "id": "70c22967-7acd-481d-a4aa-f094018e90de"
   },
   "outputs": [],
   "source": [
    "#for i,samples in enumerate(mchains):\n",
    "#    np.savetxt(datapath+'clredshift_metro_'+str(i)+'.txt.gz', samples, header=' '.join(param_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb218c6-e7cb-4315-846c-8dd078b72459",
   "metadata": {
    "deletable": false,
    "id": "3eb218c6-e7cb-4315-846c-8dd078b72459",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8107b474dd25b39a1146707770a660c",
     "grade": false,
     "grade_id": "savechains2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_have_saved_the_Metro_chains = False # again, you shouldn't actually need to change this\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30c39c-a49f-4a3b-ae9b-c1258f34ee9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7e30c39c-a49f-4a3b-ae9b-c1258f34ee9a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "995147136cd3d6f61f458683c1253de7",
     "grade": true,
     "grade_id": "svchains2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert all([file_exists(datapath+'clredshift_metro_'+str(i)+'.txt.gz') for i in range(nchains)]) or I_have_saved_the_Metro_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b271b3-84cb-424d-bb02-c168c04da403",
   "metadata": {
    "id": "81b271b3-84cb-424d-bb02-c168c04da403",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Parting thoughts\n",
    "\n",
    "In this notebook, you've hacked together two different MCMC algorithms:\n",
    "* Conjugate Gibbs sampling is often, though not always, highly efficient when it's applicable. Problems that are fully conjugate are not all that common in astrophysics, although, as you'll see later on, it's sometimes possible to perform conjugate updates of a subset of parameters in a complex model, using other strategies for the remaining parameters.\n",
    "* In contrast, Metropolis and Metropolis-Hastings are extremely widely applicable and therefore powerful, though one needs to think about how to provide an efficient proposal distribution. We'll see the consequences of not doing so in the MCMC Diagnostics notes, and some intelligent proposal strategies in the More Sampling Methods notes.\n",
    "\n",
    "In future notebooks, you won't need to write your own sampling code, and may not even use either of these specific algorithms. Still, this look under the hood of two of two sampling methods will hopefully provide some good intuition going forward."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
