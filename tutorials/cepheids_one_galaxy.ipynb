{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: The Cepheid Period-Luminosity Relation for a Single Galaxy\n",
    "\n",
    "In the [Cepheids](cepheids.ipynb) notebook, we loaded in data that should constrain the period-luminosity relation, and sketched out a hierarchical model that will let us determine whether the relation is universal (the same in all galaxies).\n",
    "\n",
    "Strictly speaking, even the model for a single galaxy is hierarchical, so we will start with that. Later on we will tackle the more complex challenge of fitting all the galaxies together.\n",
    "\n",
    "Start by restoring the previous notebook. This includes all of the `import`s from that notebook, so we don't need to repeat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import dill\n",
    "\n",
    "# may need to change the load path\n",
    "TBC() # dill.load_session('../ignore/cepheids.db')\n",
    "\n",
    "exec(open('tbc.py').read()) # (re-)define TBC and TBC_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are additional imports we imagine you might like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import emcee\n",
    "import incredible as cr\n",
    "from pygtc import plotGTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "Let's arbitarily use the first galaxy for this exercise - it's somewhere in the middle of the pack in terms of how many measured cepheids it contains.\n",
    "\n",
    "Even though we're only looking at one galaxy so far, let's try to write code that can later be re-used to handle any galaxy (so that we can fit all galaxies simultaneously). To that end, most functions will have an argument, `g`, which is a key into the `data` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ngc_numbers[0]\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of cepheids in this galaxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[g]['Ngal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model specification\n",
    "\n",
    "Note: it isn't especially onerous to keep all the galaxies around for the \"Model\" and \"Strategy\" sections, but feel free to specialize to the single galaxy case if it helps.\n",
    "\n",
    "Before charging forward, let's finish specifying the model. We previously said we would allow an intrinsic scatter about the overall period-luminosity relation - let's take that to be Gaussian such that the linear relation sets the mean of the scatter distribution, and there is an additional parameter for the width (in magnitudes), $\\sigma_i$, for the $i$th galaxy. (Note that normal scatter in magnitudes, which are log-luminosity, could also be called log-normal scatter in luminosity; these are completely equivalent.) We'll hold off on specifying hyperpriors for the distributions of intercepts, slopes and intrinsic scatters among galaxies.\n",
    "\n",
    "Your previous PGM should need minimal if any modification, but make sure that all of the model parameters are represented:\n",
    "\n",
    "* The observed apparent magnitude of the $j^{th}$ cepheid in the $i^{th}$ galaxy, $m^{\\rm obs}_{ij}$\n",
    "* The \"true\" apparent magnitude of the $j^{th}$ cepheid in the $i^{th}$ galaxy, $m_{ij}$\n",
    "* The known observational uncertainty on the apparent magnitude of the $j^{th}$ cepheid in the $i^{th}$ galaxy, $\\varepsilon_{ij}$\n",
    "* The true absolute magnitude of the $j^{th}$ cepheid in the $i^{th}$ galaxy, $M_{ij}$\n",
    "* The log period for the $j^{th}$ cepheid in the $i^{th}$ galaxy, $\\log_{10}P_{ij}$\n",
    "\n",
    "* The luminosity distance to the $i^{th}$ galaxy, $d_{L,i}$\n",
    "* The intercept parameter of the period-luminosity relation in the $i^{th}$ galaxy, $a_{i}$\n",
    "* The slope parameter of the period-luminosity relation in the $i^{th}$ galaxy, $b_{i}$\n",
    "* The intrinsic scatter parameter about the period-luminosity relation in the $i^{th}$ galaxy, $\\sigma_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: new PGM_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also write down the probabilistic expressions represented in the PGM, with the exception of those for $a_i$, $b_i$ and $\\sigma_i$, which we still haven't chosen priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC: probabilistic relationships_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this notebook, we will assume wide, uniform priors for $a_i$, $b_i$ and $\\sigma_i$, but it's useful (for later) to have the model sketched out above withut those assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strategy\n",
    "\n",
    "The hierarchical nature of this problem has left us with a large number of nuisance parameters, namely a true absolute magnitude for every one of the cepheids. The question now is: how are we going to deal with it?\n",
    "\n",
    "There are a few possibilities:\n",
    "\n",
    "### Sampling:\n",
    "\n",
    "We could take a brute force approach - just apply one of the general-purpose algorithms we've looked at and hope it works.\n",
    "\n",
    "Alternatively, while it might not be obvious, this problem (a linear model with normal distributions everywhere) is fully conjugate, given the right choice of prior. We could therefore use a conjugate Gibbs sampling code specific to the linear/Gaussian case (it's common enough thay they exist) or a more general code that works out and takes advantage of any conjugate relations, given a model. (You could also work out and code up the conjugacies yourself, if you're into that kind of thing.) These are all still \"brute-force\" in the sense that they are sampling all the nuisance parameters, but we might hope for faster convergence than a more generic algorithm.\n",
    "\n",
    "### Direct integration:\n",
    "\n",
    "If some parameters truly are nuisance parameters, in the sense that we don't care what their posteriors are, then we'll ultimately marginalize over them anyway. Rather than sampling the full-dimensional parameter space and then looking only at the marginal distributions we care about, we always have the option of sampling only parameters we care about, and, while evaluating _their_ posterior, doing integrals over the nuisance parameters in some other way. In other words, we should remember that obtaining samples of a parameter is only one method of integrating over it.\n",
    "\n",
    "Whether it makes sense to go this route depends on the structure of the model (and how sophisticated you care to make your sampler). Somtimes, sampling the nuisance parameters just like the parameters of interest turns out to be the best option. Other times, direct integration is much more efficient. And, of course, \"direct integration\" could take many forms, depending on the integrand: an integral might be analytic, or it might be best accomplished by quadrature or by monte carlo integration. The dimensionality of the integration (in particular, whether it factors into one- or at least low-dimensional integrals) is something to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for this model, try to write down the posterior for $a_i$, $b_i$ and $\\sigma_i$, marginalized over the $M_{ij}$ parameters. If you're persistent, you should find that the integral is analytic, meaning that we can reduce the sampling problem to a computationally efficient posterior distribution over just $a_i$, $b_i$ and $\\sigma_i$, at the expense of having to use our brains.\n",
    "\n",
    "If you get super stuck, note that working this out is not a requirement for the notebook (see below), but I suspect it provides the most efficient solution overall.\n",
    "\n",
    "Hint: the [`gaussians`](gaussians.ipynb) notebook is helpful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC math_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtain the posterior\n",
    "\n",
    "Sample the posterior of $a_i$, $b_i$ and $\\sigma_i$ for the one galaxy chosen above (i.e. a single $i$), and do the usual sanity checks and visualizations. Use \"wide uniform\" priors on $a$, $b$ and $\\sigma$.\n",
    "\n",
    "In the subsections below, you'll get to do this 3 different ways! First you'll apply a generic sampler to the brute-force and analytic integration methods. Then we'll walk through using a Gibbs sampling package.\n",
    "\n",
    "Hint: a common trick to reduce the posterior correlation between the intercept and slope parameters of a line is to reparametrize the model as $a+bx \\rightarrow a' + b(x-x_0)$, where the \"pivot\" $x_0$ is roughly the mean of $x$ in the data. You don't _have_ to do this, but smaller correlations usually mean faster convergence. If you do, don't forget about the redefinition when visualizing/interpretting the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pivots (nb different for every galaxy, which is not what we'd want in a simultaneous analysis)\n",
    "for i in ngc_numbers:\n",
    "    data[i]['pivot'] = data[i]['logP'].mean()\n",
    "# to avoid confusion later, reset all pivots to the same value\n",
    "global_pivot = np.mean([data[i]['logP'].mean() for i in ngc_numbers])\n",
    "for i in ngc_numbers:\n",
    "    data[i]['pivot'] = global_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function to evaluate the mean relation, with an extra argument for the pivot point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanfunc(x, xpivot, a, b):\n",
    "    '''\n",
    "    x is log10(period/days)\n",
    "    returns an absolute magnitude\n",
    "    '''\n",
    "    return a + b*(x - xpivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Brute force sampling of all parameters\n",
    "\n",
    "Attempt to simply sample all the parameters of the model. Let's... not include all the individual magnitudes in these lists of named parameters, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['a', 'b', 'sigma']\n",
    "param_labels = [r'$a$', r'$b$', r'$\\sigma$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest starting by finding decent guesses of $a$, $b$, $\\sigma$ by trial and error/inspection. For extra fun, chose values such that the model goes through the points, but isn't a _great_ fit. This will let us see how well the sampler used below performs when it needs to find its own way to the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC(1) # guess = {'a': ...\n",
    "\n",
    "guessvec = [guess[p] for p in param_names] # it will be useful to have `guess` as a vector also\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(data[g]['logP'], data[g]['M'], yerr=data[g]['merr'], fmt='none');\n",
    "plt.xlabel('log10 period/days', fontsize=14);\n",
    "plt.ylabel('absolute magnitude', fontsize=14);\n",
    "xx = np.linspace(0.5, 2.25, 100)\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], guess['a'], guess['b']))\n",
    "plt.gca().invert_yaxis();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll provide the familiar skeleton of function prototypes below, with a couple of small changes. One is that we added an option argument `Mtrue` to the log-prior - this allows the same prior function to be used in all parts of this exercise, even when the true magnitudes are not being explicitly sampled (the function calls in later sections would simply not pass anything for `Mtrue`). The log-posterior function is also generic, in the sense that it takes as an argument the log-likelihood function it should use. Another difference is that we provide a function called `logpost_vecarg_A` (\"A\" referring to this part of the notebook) that takes a vector of parameters as input, ordered $a$, $b$, $\\sigma$, $M_1$, $M_2$, ..., instead of a dictionary. This is for compatibility with the `emcee` sampler which is used below. (If you would like to use a different but still generic method instead, like HMC, go for it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior, likelihood, posterior functions for a SINGLE galaxy\n",
    "\n",
    "# generic prior for use in all parts of the notebook\n",
    "def log_prior(a, b, sigma, Mtrue=None):\n",
    "    TBC()\n",
    "    \n",
    "# likelihood specifically for part A\n",
    "def log_likelihood_A(gal, a, b, sigma, Mtrue):\n",
    "    '''\n",
    "    `gal` is an entry in the `data` dictionary; `a`, `b`, and `sigma` are scalars; `Mtrue` is an array\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "# generic posterior, again for all parts of the problem\n",
    "def log_posterior(gal, loglike, **params):\n",
    "    lnp = log_prior(**params)\n",
    "    if lnp != -np.inf:\n",
    "        lnp += loglike(gal, **params)\n",
    "    return lnp\n",
    "\n",
    "# posterior for part A, taking a parameter array argument for compatibility with emcee\n",
    "def logpost_vecarg_A(pvec):\n",
    "    params = {name:pvec[i] for i,name in enumerate(param_names)}\n",
    "    params['Mtrue'] = pvec[len(param_names):]\n",
    "    return log_posterior(data[g], log_likelihood_A, **params)\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick sanity check, which you can refine if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_A = np.concatenate((guessvec, data[g]['M']))\n",
    "logpost_vecarg_A(guess_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will set up and run `emcee` using the functions defined above. We've made some generic choices, such as using twice as many \"walkers\" as free parameters, and starting them distributed according to a Gaussian around `guess_A` with a width of 1%.\n",
    "\n",
    "#### IMPORTANT\n",
    "\n",
    "You do **not** need to run this version long enough to get what we would normally consider acceptable results, in terms of convergence and number of independent samples. Just convince yourself that it's functioning, and get a sense of how it performs. **Please do not turn in a notebook where the sampling cell below takes longer than $\\sim30$ seconds to evaluate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nsteps = 1000 # or whatever\n",
    "\n",
    "npars = len(guess_A)\n",
    "nwalkers = 2*npars\n",
    "sampler = emcee.EnsembleSampler(nwalkers, npars, logpost_vecarg_A)\n",
    "start = np.array([np.array(guess_A)*(1.0 + 0.01*np.random.randn(npars)) for j in range(nwalkers)])\n",
    "sampler.run_mcmc(start, nsteps)\n",
    "print('Yay!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the usual trace plots, including only one of the magnitudes since there are so many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npars = len(guess)+1\n",
    "plt.rcParams['figure.figsize'] = (16.0, 3.0*npars)\n",
    "fig, ax = plt.subplots(npars, 1);\n",
    "cr.plot_traces(sampler.chain[:min(8,nwalkers),:,:npars], ax, labels=param_labels+[r'$M_1$']);\n",
    "npars = len(guess_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are this is not very impressive. But we carry on, to have it as a point of comparison. The cell below will print out the usual quantitiative diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# burn = ...\n",
    "# maxlag = ...\n",
    "\n",
    "tmp_samples = [sampler.chain[i,burn:,:4] for i in range(nwalkers)]\n",
    "print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))\n",
    "print('NB: Since walkers are not independent, these will be optimistic!')\n",
    "print(\"Plus, there's a good chance that the results in this section are garbage...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at a triangle plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_A = sampler.chain[:,burn:,:].reshape(nwalkers*(nsteps-burn), npars)\n",
    "\n",
    "plotGTC([samples_A[:,:4]], paramNames=param_labels+[r'$M_1$'], chainLabels=['emcee/brute'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also probably look at how well the fitted model matches the data, qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(data[g]['logP'], data[g]['M'], yerr=data[g]['merr'], fmt='none');\n",
    "plt.xlabel('log10 period/days', fontsize=14);\n",
    "plt.ylabel('absolute magnitude', fontsize=14);\n",
    "xx = np.linspace(0.5, 2.25, 100)\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_A[:,0].mean(), samples_A[:,1].mean()), label='emcee/brute')\n",
    "plt.gca().invert_yaxis();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Sampling with analytic marginalization\n",
    "\n",
    "Next, implement sampling of $a$, $b$, $\\sigma$ using your analytic marginalization over the true magnitudes. Again, the machinery to do the sampling is below; you only need to provide the log-posterior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_B(gal, a, b, sigma):\n",
    "    '''\n",
    "    `gal` is an entry in the `data` dictionary; `a`, `b`, and `sigma` are scalars\n",
    "    '''\n",
    "    TBC()\n",
    "\n",
    "def logpost_vecarg_B(pvec):\n",
    "    params = {name:pvec[i] for i,name in enumerate(param_names)}\n",
    "    return log_posterior(data[g], log_likelihood_B, **params)\n",
    "\n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpost_vecarg_B(guessvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we run `emcee` below. Anticipating an improvment in efficiency, we've increased the default number of steps below. Unlike the last time, you should run long enough to have useful samples in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nsteps = 10000\n",
    "\n",
    "npars = len(param_names)\n",
    "nwalkers = 2*npars\n",
    "sampler = emcee.EnsembleSampler(nwalkers, npars, logpost_vecarg_B)\n",
    "start = np.array([np.array(guessvec)*(1.0 + 0.01*np.random.randn(npars)) for j in range(nwalkers)])\n",
    "sampler.run_mcmc(start, nsteps)\n",
    "print('Yay!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, trace plots. Note that we no longer get a trace of the magnitude parameters. If we really wanted a posterior for them, we would now need to do extra calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 3.0*npars)\n",
    "fig, ax = plt.subplots(npars, 1);\n",
    "cr.plot_traces(sampler.chain[:min(8,nwalkers),:,:], ax, labels=param_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, $R$ and $n_\\mathrm{eff}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# burn = ...\n",
    "# maxlab = ...\n",
    "\n",
    "tmp_samples = [sampler.chain[i,burn:,:] for i in range(nwalkers)]\n",
    "print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))\n",
    "print('NB: Since walkers are not independent, these will be optimistic!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the posterior from this analysis to the one we got before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_B = sampler.chain[:,burn:,:].reshape(nwalkers*(nsteps-burn), npars)\n",
    "\n",
    "plotGTC([samples_A[:,:3], samples_B], paramNames=param_labels, chainLabels=['emcee/brute', 'emcee/analytic'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your posterior is compared with our solution by the cell below. Note that we used the `global_pivot` defined above. If you did not, your constraints on $a$ will differ due to this difference in definition, even if everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = np.loadtxt('solutions/ceph1.dat.gz')\n",
    "plotGTC([sol, samples_B], paramNames=param_labels, chainLabels=['solution', 'my emcee/analytic'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, look at how the two fits you've done compare visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(data[g]['logP'], data[g]['M'], yerr=data[g]['merr'], fmt='none');\n",
    "plt.xlabel('log10 period/days', fontsize=14);\n",
    "plt.ylabel('absolute magnitude', fontsize=14);\n",
    "xx = np.linspace(0.5, 2.25, 100)\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_A[:,0].mean(), samples_A[:,1].mean()), label='emcee/brute')\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_B[:,0].mean(), samples_B[:,1].mean()), label='emcee/analytic')\n",
    "plt.gca().invert_yaxis();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on things like the efficiency, accuracy, and/or utility of the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TBC commentary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Conjugate Gibbs sampling\n",
    "\n",
    "Finally, we'll step through using a specialized Gibbs sampler to solve this problem. We'll use the `LRGS` package, not because it's the best option (it isn't), but because it's written in pure Python. The industry-standard (and far less specialized) alternative goes by the name JAGS, and requires a separate installation (though one can add a Python interface on top of that).\n",
    "\n",
    "Let me stress that LRGS is in no fashion optimized for speed; JAGS is presumably faster, not to mention applicable to more than just fitting lines. Even so, LRGS seems to be comparable in speed with our analytically supercharged `emcee` in this case, when one considers that the samples it returns are less correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lrgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRGS is a \"general\" linear model fitter, meaning that $x$ and $y$ can be multidimensional. So the input data are formatted as matrices with one row for each data point. In this case, they're column vectors ($n\\times1$ matrices).\n",
    "\n",
    "Measurement uncertainties are given as a list of covariance matrices. The code handles errors on both $x$ and $y$, so these are $2\\times2$ for us. Since our $x$'s are given precisely, we just put in a dummy value here and use a different option to fix the values of $x$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asmatrix(data[g]['logP'] - data[g]['pivot']).T\n",
    "y = np.asmatrix(data[g]['M']).T\n",
    "M = [np.matrix([[1e-6, 0], [0, err**2]]) for err in data[g]['merr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjugate Gibbs sampling can be parallelized in the simplest possible way - you just run multiple chains from different starting points or even just with different random seeds in parallel. (`emcee` is parallelized internally, since walkers need to talk to each other.) Therefore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets things up and does the actual sampling, returning a Numpy array in the usual format. The default priors are equivalent to the ones we chose above, helpfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 2000 # some arbitrary number of steps to run\n",
    "\n",
    "def do_gibbs(i):\n",
    "    # every parallel process will have the same random seed if we don't reset them here\n",
    "    if i > 0:\n",
    "        np.random.seed(i)\n",
    "    # lrgs.Parameters set up a sampler that assumes the x's are known precisely.\n",
    "    # Other classes would correspond to different possible priors on x.\n",
    "    par = lrgs.Parameters(x, y, M)\n",
    "    chain = lrgs.Chain(par, nsteps)\n",
    "    chain.run(fix='x') # fix='x' isn't necessary here, but it shows how one would fix other parameters if we wanted to\n",
    "    # Extracts the chain as a dictionary. Note that we have the option of hanging onto the samples of the magnitude\n",
    "    #  parameters in addition to the intercept, slope and scatter, though this is not the default.\n",
    "    dchain = chain.to_dict([\"B\", \"Sigma\"])\n",
    "    # since $sigma^2$ is sampled rather than $\\sigma$, take the square root here\n",
    "    return np.array([dchain['B_0_0'], dchain['B_1_0'], np.sqrt(dchain['Sigma_0_0'])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool() as pool:\n",
    "    gibbs_samples = pool.map(do_gibbs, range(2)) # 2 parallel processes - change if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16.0, 3.0*npars)\n",
    "fig, ax = plt.subplots(npars, 1);\n",
    "cr.plot_traces(gibbs_samples, ax, labels=param_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn = 50\n",
    "maxlag = 1000\n",
    "\n",
    "tmp_samples = [x[burn:,:] for x in gibbs_samples]\n",
    "print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_C = np.concatenate(tmp_samples, axis=0)\n",
    "\n",
    "plotGTC([samples_A[:,:3], samples_B, samples_C], paramNames=param_labels,\n",
    "        chainLabels=['emcee/brute', 'emcee/analytic', 'LRGS/Gibbs'],\n",
    "        figureSize=8, customLabelFont={'size':12}, customTickFont={'size':12}, customLegendFont={'size':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, look at the fit compared with the other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.errorbar(data[g]['logP'], data[g]['M'], yerr=data[g]['merr'], fmt='none');\n",
    "plt.xlabel('log10 period/days', fontsize=14);\n",
    "plt.ylabel('absolute magnitude', fontsize=14);\n",
    "xx = np.linspace(0.5, 2.25, 100)\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_A[:,0].mean(), samples_A[:,1].mean()), label='emcee/brute')\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_B[:,0].mean(), samples_B[:,1].mean()), label='emcee/analytic')\n",
    "plt.plot(xx, meanfunc(xx, data[g]['pivot'], samples_C[:,0].mean(), samples_C[:,1].mean()), '--', label='LRGS/Gibbs')\n",
    "plt.gca().invert_yaxis();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up\n",
    "\n",
    "There's yet more fun to be had in the next tutorial, so let's again save the definitions and data from this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pool # cannot be pickled\n",
    "\n",
    "TBC() # change path below if desired\n",
    "# dill.dump_session('../ignore/cepheids_one.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
