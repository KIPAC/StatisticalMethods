{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tutorial: AGN Photometry on a Grid\n",
    "\n",
    "Note: this tutorial follows [X-ray Image Data](xmm_image.ipynb) and precedes [AGN Photometry with Gibbs Sampling](agn_photometry_gibbs.ipynb) and [AGN Photometry with Metropolis Sampling](agn_photometry_metro.ipynb).\n",
    "\n",
    "In this notebook we will fit a model to X-ray imaging data, bringing together what you've learned about generative models, Bayes law, posterior evaluations on a grid, and credible intervals. After stepping through the inference of model parameters in this way, we'll motivate the use of sampling methods that you'll explore in upcoming tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import astropy.io.fits as pyfits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from astropy.visualization import LogStretch\n",
    "logstretch = LogStretch()\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import minimize\n",
    "import incredible as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "Now that you know your way around the image data itself, we're going to fit a model to a cut-out of the data. To keep things simple, we will ignore the galaxy cluster and just fit the brightness of one of the AGN in the field, after asserting that we know both its position and the telescope PSF (this is called \"forced photometry\").\n",
    "\n",
    "In generative terms, we will define a 2-dimensional model for source brightness on the sky, apply the PSF to smear it out as the telescope optics would, and apply the exposure map to compute an expected, average number of counts in each pixel given our observation's length. This can then be compared with the measured counts using the sampling distribution which, for counts of rare events, is Poisson. More concretely,\n",
    "\n",
    "* The physical model we're interested in is the source brightness, $S(x,y)$.\n",
    "* Assuming the PSF doesn't vary with position within our cut-out, it's action is a simple convolution.\n",
    "* Multiplication by the exposure map, $E(x,y)$, then takes care of the overall observation length and the position-dependent vignetting.\n",
    "\n",
    "Even more compactly, we could write the model expectation number of counts per pixel as\n",
    "\n",
    "$\\mu(x,y) = E(x,y) \\times \\left[\\mathrm{PSF}\\otimes S(x,y)\\right]$,\n",
    "\n",
    "and the data as\n",
    "\n",
    "$N(x,y) \\sim \\mathrm{Poisson}\\left[\\mu(x,y)\\right]$.\n",
    "\n",
    "Remembering that the exposure map had units of seconds, $S$ as used above must have units of counts per second per pixel (the units of \"pixel on the sky\" are solid angle). In real life we might want to eventually measure the AGN flux in real units (e.g. ergs/s/cm$^2$), which would require an assumption about the spectrum of the source (or an analysis that didn't throw out the spectral information!). We'll stick to inferring things in terms of counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more things are necessary to fully specify our model. We'll take the exposure map to be known and fixed, which leaves $S(x,y)$ and the PSF.\n",
    "\n",
    "An AGN is point-like as far as this telescope is concerned, so we can write\n",
    "\n",
    "$S_\\mathrm{agn}(x,y) = F_0\\,\\delta(x-x_0)\\,\\delta(y-y_0)$.\n",
    "\n",
    "For simplicity, let's assume there is a constant background in whatever region in the image we decide to analyze:\n",
    "\n",
    "$S_\\mathrm{bg}(x,y) = b$.\n",
    "\n",
    "In the context of measuring the AGN flux, \"background\" includes the cluster emmission, so we'll want to select an AGN in a part of the field of view where the cluster emission is subdominant to the more uniform background.\n",
    "\n",
    "Finally, we'll assume a symmetric, Gaussian PSF, with a standard deviation of $\\sigma=5''=1.25$ pixels. This is not entirely accurate, but it would take an extremely bright source, or a statistical analysis of many fainter sources, for us to see the actual PSF pattern, which is quite complicated in detail.\n",
    "\n",
    "We thus have only 2 model parameters to fit, $F_0$ and $b$. Since AGN fluxes span many orders of magnitude, but the quiescent background count rate does not, let's use wide uniform priors on $\\ln(F_0)$ and $b$ and see what we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with an implementation, go through our usual generative model sanity check: enumerate the model parameters and data, write down the probabilistic relationships among them, and visualize them with a PGM. Keep in mind which parameters will be free vs fixed in our initial analysis, as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For convenience, this defines the `Image` class as in the [`xmm_image`](xmm_image.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xray_image import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in the data and displays it, just as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # datadir = '../ignore/' # or whatever - path to where you put the downloaded files\n",
    "\n",
    "imagefile = datadir + 'P0098010101M2U009IMAGE_3000.FTZ'\n",
    "expmapfile = datadir + 'P0098010101M2U009EXPMAP3000.FTZ'\n",
    "\n",
    "imfits = pyfits.open(imagefile)\n",
    "exfits = pyfits.open(expmapfile)\n",
    "\n",
    "im = imfits[0].data\n",
    "ex = exfits[0].data\n",
    "\n",
    "orig = Image(im, ex)\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "orig.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to decide on a specific AGN to measure, and decide what size cut-out to make the measurement in, keeping in mind the model assumptions above (especially the uniform background and lack of other AGN within the cut-out). We'll make a standard choice below so that you'll have a known solution to compare to. But, for completeness, here is a list of rough AGN positions in IMAGE coordinates determined some time ago (by eye).\n",
    "\n",
    "```\n",
    "232 399\n",
    "188 418\n",
    "362 474\n",
    "336 417\n",
    "381 359\n",
    "391 418\n",
    "398 294\n",
    "417 209\n",
    "271 216\n",
    "300 212\n",
    "286 162\n",
    "345 153\n",
    "168 361\n",
    "197 248\n",
    "277 234\n",
    "241 212\n",
    "251 379\n",
    "310 413\n",
    "460 287\n",
    "442 353\n",
    "288 268\n",
    "148 317\n",
    "151 286\n",
    "223 239\n",
    "490 406\n",
    "481 318\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8th entry looks pretty good, but feel free to mess around with these choices (just keep in mind that you won't be able to compare to the solutions below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = 417\n",
    "y0 = 209\n",
    "stampwid = 25\n",
    "stamp = orig.cutout(x0-stampwid, x0+stampwid, y0-stampwid, y0+stampwid)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "stamp.display(log_image=False) # not a huge dynamic range in this cutout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the subset of the data we'll use to fit the AGN flux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "It's time to write functions to evaluate the prior, sampling and posterior distributions. Even though we're only fitting for $\\ln(F_0)$ and $b$ for the moment, let's write those functions more generally, so that they also depend explicitly on the AGN position, $(x_0,y_0)$, and the PSF width, $\\sigma$.\n",
    "\n",
    "Here's a dictionary of parameter values, with arbitrary values for `lnF0` and `b`, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'x0':x0, 'y0':y0, 'lnF0':-5.0, 'b':1e-6, 'sigma':1.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we try to evaluate the _log_ of these distributions. This is because floating-point underflows can be an issue, especially when the sampling distribution is a product with many terms. Another benefit is that we don't have to worry about normalizing coefficients that don't depend on model parameters - if we need some distribution to be normalized later, we can always normalize it explicitly by dividing it by its numerical integral. If any of these functions should return zero probability, you can and should have them return a log-probability of -infinity.\n",
    "\n",
    "Complete the function evaluating the log-prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(x0, y0, lnF0, b, sigma):\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we should make sure it returns some kind of value instead of crashing when fed an example parameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "log_prior(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the log-likelihood/sampling distribution. This is, of course, where all the fun of evaluating the model happens, although I'd suggest outsourcing the evaluation of $\\mu(x,y)$ to a separate function. Keep in mind that our model consists of a delta function and a uniform background, so it isn't necessary to carry out the convolution in the model numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, x0, y0, lnF0, b, sigma):\n",
    "    \"\"\"\n",
    "    `data` will be an Image object\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "log_likelihood(stamp, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the log-posterior. As will normally be the case, we will just return the sum of the log-prior and log-likelihood, i.e. we will neglect the normalizing constant (evidence), which is constant with repect to the model parameters.\n",
    "\n",
    "The construction below is a good one to be in the habit of using. Usually, the likelihood is much more expensive to compute than the prior, and might even crash if passed prior-incompatible parameter values. So it's worth the extra check of a non-zero prior probability before attempting to evaluate the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(data, **params):\n",
    "    \"\"\"\n",
    "    `data` will be an Image object\n",
    "    \"\"\"\n",
    "    lnp = log_prior(**params)\n",
    "    if np.isfinite(lnp):\n",
    "        lnp += log_likelihood(data, **params)\n",
    "    return lnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check\n",
    "log_posterior(stamp, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our posterior function is written to work with scalar values of its arguments, but sometimes (especially when working over a grid) it's convenient to have a version that takes vector arguments. According to the `numpy` documentation, `vectorize` is not actually more efficient than evaluating your function within nested `for` loops, but it's an option anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_lnpost = np.vectorize(log_posterior, excluded=['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the posterior\n",
    "\n",
    "We should now be able to define a grid and evaluate the posterior over it, as we've seen before. In practical terms, the extent of the grid defines the bounds of the \"wide, uniform\" priors we decided to use. It would, therefore, be helpful to know roughly what part of `lnF0` - `b` space we need to cover. A resonable approach is to use a numerical optimizer to find the maximum of the log-posterior (or the minimum of its additive inverse). That won't instantly tell us how wide to make the grid in order to contain nearly all the posterior probability, but it's a good start and we can then iterate if need be.\n",
    "\n",
    "We'll use `scipy.optimize.minimize` (imported above as `minimize`). Refer to its documentation if needed. Define the function that `minimize` will work on here. (Note that it will want a vector argument rather than a dictionary of parameter values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlnpost(p):\n",
    "    \"\"\"\n",
    "    p: a numpy array of parameter values in the order lnF0, b\n",
    "    Return value: minus the log-posterior\n",
    "    \"\"\"\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the optimization and print out the best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit = minimize(mlnpost, [params['lnF0'], params['b']])\n",
    "bestfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** If you're fitting to the postage stamp defined above with the suggested priors, the best fit should be approximately `lnF0 = -6.6` and `b = 5.3e-6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the bounds and spacing of the grid to evaluate on. You may need to refine these values after seeing the results. You can get an initial guess at the appropriate size in each direction by treating `bestfit['hess_inv']` as the covariance of the log-posterior and looking at the square root of its diagonal - this won't be wonderfully accurate, but will get you an order of magnitude.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# lnF0_min = \n",
    "# lnF0_max = \n",
    "# dlnF0 = \n",
    "# b_min = \n",
    "# b_max = \n",
    "# db = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will define 2D grids holding the values of `lnF0` and `b` for each entry, as illustrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnF0_values = np.arange(lnF0_min, lnF0_max+dlnF0, dlnF0)\n",
    "b_values = np.arange(b_min, b_max+db, db)\n",
    "grid_lnF0, grid_b = np.meshgrid(lnF0_values, b_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14.0, 5.0)\n",
    "fig, ax = plt.subplots(1,2);\n",
    "ax[0].imshow(grid_lnF0, cmap='gray', origin='lower', extent=[lnF0_min, lnF0_max, b_min, b_max], aspect='auto');\n",
    "ax[0].set_xlabel('ln F0');\n",
    "ax[0].set_ylabel('b');\n",
    "ax[1].imshow(grid_b, cmap='gray', origin='lower', extent=[lnF0_min, lnF0_max, b_min, b_max], aspect='auto');\n",
    "ax[1].set_xlabel('ln F0');\n",
    "ax[1].set_ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will evaluate the log-posterior over the grid and display the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_params = params.copy()\n",
    "grid_params['lnF0'] = grid_lnF0\n",
    "grid_params['b'] = grid_b\n",
    "grid_lnpost = vectorized_lnpost(stamp, **grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.imshow(grid_lnpost, cmap='gray', origin='lower', aspect='auto', extent=[lnF0_min, lnF0_max, b_min, b_max]);\n",
    "plt.xlabel('ln F0');\n",
    "plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding credible regions\n",
    "\n",
    "Next, we'll go through the steps to find standard credible regions and intervals for the two parameters. We can use the machinery you saw in the [Credible Regions](credible_intervals.ipynb) tutorial by reformating our grid evaluation in a form those functions will like.\n",
    "\n",
    "Below, create a dictionary that looks like the output of `incredible.whist2d` by filling in the entry for the posterior density estimated over an array. Recall that we haven't properly normalized the log-posterior, so it's a good idea to subtract off the maximum log-posterior before exponentiating to avoid numerical under/overflows (that way the maximum posterior value is automatically 1.0 and not something like 1e-300 or 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC() # grid_post = ...\n",
    "\n",
    "h2d = {'x':lnF0_values, 'y':b_values, 'z':grid_post}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what that gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "contours = cr.whist2d_ci(h2d)\n",
    "plt.xlabel('ln F0');\n",
    "plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar to get the 1D credible intervals. Use your amazing skills to compute the marginalized posteriors for `lnF0` and `b` at  the grid points defined by `lnF0_values` and `b_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBC()\n",
    "# lnF0_post = \n",
    "# b_post ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll pass those to our CI finder. This should reveal if your grid is too course to resolve the CI's well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1d_lnF0 = {'x':lnF0_values, 'density':lnF0_post}\n",
    "h1d_b = {'x':b_values, 'density':b_post}\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 5.0)\n",
    "fig, ax = plt.subplots(1,2);\n",
    "ci_lnF0 = cr.whist_ci(h1d_lnF0, plot=ax[0])\n",
    "ax[0].set_xlabel('ln F0');\n",
    "ax[0].set_ylabel('marginalized post');\n",
    "ci_b = cr.whist_ci(h1d_b, plot=ax[1])\n",
    "ax[1].set_xlabel('b');\n",
    "ax[1].set_ylabel('marginalized post');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the CI's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_lnF0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** with the given setup, you should end up with something like $\\ln(F_0)=-6.6\\pm0.2$ and $b=(5.3\\pm0.4)\\times10^{-6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the best fit\n",
    "\n",
    "It's always a good idea to visualize how well your fit predicts the data. This is a little bit of an aside for this notebook (we will do more later on), so the code below is mostly given. You will need to fill in code to evaluate the model expectated number of counts over the postage stamp, just as in the likelihood. The plot shows the data as a surface brightness profile (average counts/second/pixel as a function of radius) compared with the mean prediction +/- 1 standard deviation of the best fitting model you found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_profile(r_bins, data, x0, y0, lnF0, b, sigma):\n",
    "    r2 = (data.imx - x0)**2 + (data.imy - y0)**2\n",
    "    TBC() # mu = mean counts in each pixel in `data`, as an array\n",
    "    rmin2 = r_bins[range(len(r_bins)-1)]**2\n",
    "    rmax2 = r_bins[range(1, len(r_bins))]**2\n",
    "    data_counts = np.zeros(len(rmin2))\n",
    "    model_counts = np.zeros(len(rmin2))\n",
    "    model_sd = np.zeros(len(rmin2))\n",
    "    for i in range(len(rmin2)):\n",
    "        j = np.where( np.all([r2>=rmin2[i], r2<rmax2[i]], axis=0) )\n",
    "        npix = len(j[0])\n",
    "        data_counts[i] = np.sum( data.im[j] ) / npix\n",
    "        model_counts[i] = np.sum( mu[j] ) / npix\n",
    "        model_sd[i] = np.sqrt(model_counts[i] / npix)\n",
    "    r = 0.5*(np.sqrt(rmin2) + np.sqrt(rmax2))\n",
    "    plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "    plt.loglog(r, data_counts, 'o');\n",
    "    plt.errorbar(r, model_counts, yerr=model_sd);\n",
    "    plt.xlabel('r');\n",
    "    plt.ylabel('counts/pix');\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = params.copy()\n",
    "best_params['lnF0'] = bestfit['x'][0]\n",
    "best_params['b'] = bestfit['x'][1]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_profile(np.arange(0.0, 30.0, 1.0), stamp, **best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality\n",
    "\n",
    "If we were very mean, we would now ask you to repeat everything above with one more free parameter, say $\\sigma$. And feel free, if you'd like. We'll wait. And we will be waiting... a pretty long time.\n",
    "\n",
    "Alternatively, you could observe how long it took to evaluate the log-posterior over a 2D grid with sufficient extent and resolution to contain nearly all the posterior probability and resolve the credible intervals well. (This should have been printed out above). Now estimate the time needed to do the same over a 3D grid, assuming the 3rd parameter needs about as many grid points as the first 2. How about with 10 parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now keep in mind that this is a pretty simple, toy problem. It's not unusual for the likelihood to take ~1 second for a single evaluation in real life - or even longer. Hence, this exponential scaling, $(\\mathrm{grid~points})^{(\\mathrm{free~parameters})}$, is in general prohibitive for us. In the following tutorials you'll solve the same (or at least similar) problems using Monte Carlo methods instead. We will see that Monte Carlo methods also take longer when there are more parameters to juggle, but the scaling is typically much nicer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
