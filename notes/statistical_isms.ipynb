{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: Bayesianism, Frequentism, and other isms\n",
    "\n",
    "Containing:\n",
    "* A brief and not at all unbiased look at Bayesian inference, other flavors of statistical analysis, and why one might use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen how Bayes' Law follows simply from the insight that probability distributions are the natural mathematical tool for describing quantities that are uncertain. Yet it's extremely likely that you've done some kind of data analysis before, and even taken multiple statistics courses, without ever having heard of it. Why is that?\n",
    "\n",
    "I am not a historian of science or mathematics, nor, frankly, am I particularly invested in philosophical arguments about the nature of probability. So please do not take any of the arguments in this set of notes as carefully researched fact. Let me also acknowledge that there is a great deal of opinion in what follows.\n",
    "\n",
    "That said, I suspect the answer to \"Why?\" is (a) Bayesian analysis was, for most purposes, infeasible before the advent of modern computing; (b) in the meantime, a lot of effort was invested in alternatives, because people had to do _something_; and (c) human beings have a lot of mental inertia.\n",
    "\n",
    "Regardless, the fact is that the majority of physical scientists do not use Bayesian analysis, either because they were never taught it, have never heard of it, lack the intellectual curiosity to learn, or object to it on grounds that generally don't pass the laugh test. (However, in some subfields, for example cosmology, it has been widely adopted.)\n",
    "\n",
    "It is, therefore, worth taking a break to learn a bit about what is _not_ contained in this course. Partly, this is so you can understand what others have done in the literature. Sometimes, the alternative frameworks that have been developed can provide useful numerical methods that can be put to work in Bayesian inference. And, sometimes, we're faced with a task that isn't (or doesn't need to be) a carefully worked inference problem, and it's good to know what other tools there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian statistics\n",
    "\n",
    "This is probably not what it's normally called, but is meant to encompass what statistitians refer to as the Generalized Linear Model (GLM), Analysis of Variance (ANOVA), and related techniques.\n",
    "\n",
    "Essentially, these all revolve around fitting models whose predictions are linear in the model parameters. That includes the slope and intercept of a line, but also the coefficients of an arbitrarily high-order polynomial, for example.\n",
    "\n",
    "If we think about the basic task of \"fitting the data\", meaning getting a model curve to pass near/through some points in the $x$-$y$ plane, then minimizing the distance in $y$ between those points and the model predictions at the same $x$ coordinates is a reasonable goal (though not the only one that could be chosen). Finding the model that minimizes the sum of absolute distances is not such an easy thing to do, it turns out. But, if we instead minimize the sum of squares of those distances, then finding the minimum suddenly corresponds to solving a quadratic equation, which can be done algebraically. Thus were interesting things discovered about celestial dynamics in the pre-digital age.\n",
    "\n",
    "From a modeling standpoint, the assumptions above are quite restrictive: we need to believe that the $x$ values of our data are known precisely (and than an $x$-$y$ representation of the data makes sense to begin with), that the average model prediction as a function of $x$ is linear in the parameters, and that departures from that average are Gaussian with known standard deviations (this corresponds to the key \"sum of squares\" part). It's tough to think of circumstances where all of this is actually true, but on the other hand these are assumptions that we might be able to justify, or be forced to make, on occasion. And, when we do, GLM/ANOVA tools provide an extremely computationally efficient alternative to the more general methods we'll cover later in this class. [Endnote 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentism and Maximum Likelihood\n",
    "\n",
    "Frequentism seems to have grown out of a conceptualization of mathematical probability that rejects the idea of assigning probabilities to things that are uncertain but which have a true value. That is, a frequentist would say that it isn't valid to describe the value of a model parameter with a probability distribution, but they would be fine assigning a probability distribution to the outcome of a repeatable measurement. Consequently, frequentism is built around manipulations of the sampling distribution/likelihood function alone.\n",
    "\n",
    "Instead of deriving the relative probability of different values of model parameters conditioned on the data, frequentists compute _estimators_ of the parameters that are functions of the data, and then try to prove things about the distribution of those estimators over possible data sets (because the estimators are functions of data, it's permitted to speak of them probabilistically).\n",
    "\n",
    "Maximum Likelihood methods are a subset of frequentism methods, in which the estimators are taken to be the parameter values that maximize the likelihood function. Among other nice properties, maximum likelihood estimators are asymptotically normally distributed about the truth, and the standard methods for estimating confidence intervals lean heavily on the assumption of normality. (Reviewed [here](https://github.com/KIPAC/StatisticsCookbook/blob/master/basic_model_fitting), if you'd like.)\n",
    "\n",
    "Since it's forbidden to discuss the probability of parameter values, frequentism cannot characterize model uncertainties as such. Instead, _confidence intervals_ (not credible intervals) are defined such that\n",
    "\n",
    "> If it were somehow possible to independently repeat the experiment arbitrarily many times, and we computed an $X\\%$ confidence interval according to the frequentist recipe for each experiment, then $X\\%$ of those intervals would contain the true value of the parameter.\n",
    "\n",
    "This is, of course and by construction, not the same as saying that the true value of the parameter is within the single confidence interval that we have in practice with $X\\%$ probability.\n",
    "\n",
    "(Please note that there is no shame in having to re-read the above passages several times.)\n",
    "\n",
    "In practical terms, if we are in the situation of using wide, uniform priors on all parameters, and we have enough data to get into the \"asymptotically normal\" regime, then there isn't much difference between a Bayesian analysis and Maximum Likelihood. As with Gaussian statistics, the computational tools of frequentist analysis can provide helpful approximations and shortcuts in such circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentism vs Bayes\n",
    "\n",
    "It probably wouldn't be accurate to say that there's a healthy debate between frequentist and Bayesian adherents in physics or astrophysics. There may well be a healthy debate among mathematical statistitians who deeply understand the possible axiomatic foundations of probability. Among scientists, the \"debate\" you're most likely to encounter is between Bayesian analysts who are trying to get some science done, and others who casually dismiss them for a variety of poorly thought out reasons.\n",
    "\n",
    "At the risk of providing oxygen to an argument that you may not have any interest in, here is my personal take on why we do Bayes.\n",
    "* The use of probability to describe uncertainty, central to Bayesian analysis, is how humans intuitively make sense of uncertainty. We discuss odds. We talk about the chance of rain tomorrow as if it were random rather than deterministic, because it _is_ random _as far as we are concerned_. There's nothing wrong with applying this reasoning to science, given a robust mathematical framework for doing so.\n",
    "* The point of science is to answer questions. Notably, the question answered by a confidence interval (relying on the counterfactual infinite repetition of independent experiments) is not one that anyone has ever wanted to know the answer to. We care about questions like \"What is $x$?\", or the more reasonable \"What possibilities for $x$ are most likely?\" The only way to answer questions like this is to go through Bayes' Law.\n",
    "* The need to chose a prior is sometimes (ok, often) brought up as a disadvantage of Bayesian analysis. It isn't. It's just a requirement for answering the questions that are worth answering. Either pick a prior you can defend and use it, or go home. Yes, your results will depend on that choice - just like anyone's results depend on the assumptions they make in analyzing data.\n",
    "* The need to make assumptions such as priors is not somehow unique to Bayes. Any method that tries to learn something about a model must build on assumptions; just because they're built into the \"standard\" way of doing a given analysis doesn't mean the assumptions aren't there. The flexibility to make those assumptions _appropriate_ and the expectation that they will be _explicit_ are features, not bugs.\n",
    "* On a more immediately practical level, the ability to include nuisance parameters in a model, with appropriate PDFs, provides a perfectly straightforward and interpretable way to propagate systematic uncertainties to our results.\n",
    "\n",
    "If you'd like to read something on this subject by much more knowledgeable people, see these [pro- and anti-Bayes arguments](https://statmodeling.stat.columbia.edu/2008/07/31/responses_to_my/) by Gelman and a [response](https://statmodeling.stat.columbia.edu/2008/09/02/more_bayes_rant/) by Skilling. (Warning: Gelman is a social scientist, so his writing contains some jargon and some particular concerns that are not so relevant to us.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and the \"engineering\" approach\n",
    "\n",
    "In principle, any conclusion we might hope to draw from data can be cast as an inference problem. But, to be honest, we don't always care enough to do that. Sometimes we just need a curve that more or less goes through some data points so that we can move on with our lives; we don't always need to carefully determine what type of curve best fits the data nor what the posterior distributions of its parameters are. Sometimes there's no hope of building a generative model for some data, or it would just be more trouble than it's worth. And so we do something else. We just do _something_ to get a particular result (or summary, or prediction, ...) and accept that we won't be able to make principled statements about the probability of those results being true, as we could if we had solved the inference problem.\n",
    "\n",
    "And that's ok.\n",
    "\n",
    "That's what we mean by the \"engineering\" approach. Just do something that lets us make a decision about where to point the telescope next (for example) without worrying about whether the probability of that decision being right is perfectly predicted. Granted, we would probably do our best to empirically determine accuracy after the fact, but not as a matter of first principles.\n",
    "\n",
    "The exact boundaries of what is known these days as \"Machine Learning\" are hard to define, but methods involving artificial neural networks, decision trees and the like generally fall into this category. Yes, they do have models of a sort under the hood, and could therefore fit into our inference framework in that limiting sense, but they generally aren't anything that would naturally show up in a generative model. From our point of view in this course, machine learning methods would be best understood as interpolation algorithms, which makes them potentially useful, e.g., for approximate Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endnotes\n",
    "\n",
    "1) I can't resist sharing some stories from my grad school (i.e., once upon a time), when I took a few statistics courses (in the actual Statistics department). In the first or second lecture of a Ph.D level course on the GLM, having just finished the theoretical overview of the generalized linear model, the professor made remarks along the lines of the following (paraphrased from memory):\n",
    "\n",
    "> So we have this very elegent framework that works for a particular model - we need a model which is linear in the parameters, where the covariates are fixed, and where the responses are normally distributed about the curve. But that's a model that comes up a lot and can be applied in many situations. That's not to say that there aren't other approaches; some people would say that we should put all this aside and do Bayesian analysis all the time. But then we'd have to make lots of assumptions.\n",
    "\n",
    "He was not intentionally being ironic.\n",
    "\n",
    "Not for nothing did a T-shirt worn by a different statistics professor (teaching a course on ANOVA, no less) read as follows:\n",
    "* Lies\n",
    "* Damned lies\n",
    "* $y = a + bx + \\varepsilon; \\, \\varepsilon \\sim \\mathrm{N}(0, \\sigma^2)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
