{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notes: Credible Intervals and Regions\n",
    "\n",
    "Containing:\n",
    "* An explanation of and opinions about various conventions for summarizing posterior PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing 1D posterior distributions\n",
    "\n",
    "The posterior distribution is The Answer to an inference problem, but a PDF over potentially many dimensions is a cumbersome beast. Often, what we want to do is summarize the constraints on one or more parameters with a statement like $\\theta = \\theta_0 \\pm \\Delta\\theta$ or $\\theta_0{}_{-\\Delta\\theta_m}^{+\\Delta\\theta_p}$. These should be interpreted as statements about the *marginal* posterior of $\\theta$, namely that $\\theta$ is in the given **credible interval** (CI) with some specified probability. To be explicit, if $\\theta$ is the parameter of immediate interest, and $\\phi$ are all the other parameters, the marginal posterior distribution of $\\theta$ is\n",
    "\n",
    "$p(\\theta|\\mathrm{data}) = \\int d\\phi \\, p(\\theta,\\phi|\\mathrm{data})$,\n",
    "\n",
    "and a credible interval for $\\theta$ containing probability $q$ satisfies\n",
    "\n",
    "$\\int_\\mathrm{CI} d\\theta \\, p(\\theta|\\mathrm{data}) = q$.\n",
    "\n",
    "Note that credible intervals/regions are technically different from the confidence intervals/regions that appear in non-Bayesian statistics, which we'll touch on in later notes. Unfortunately, in practice, the terms are often used interchangeably. Appologies in advance if that happens in this course.\n",
    "\n",
    "Saying that a 68% CI contains a probability of 0.68 doesn't uniquely tell us _how_ to define the interval, nor what \"best\" value we should report as $\\theta_0$, and in practice there are multiple conventions. Let's look at a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most probable value and interval\n",
    "\n",
    "This is also sometimes known as \"highest posterior density\" (HPD).\n",
    "\n",
    "Quite simply, this convention would report as $\\theta_0$ the mode of the marginal posterior. The credible interval is the (possibly multiply connected, i.e. consisting of multiple, disjoint intervals) interval _of largest posterior density_ that contains the specified probability. Consequently, it is also the smallest possible interval containing the target probability.\n",
    "\n",
    "The mode and 68.3% and 95.4% CI's are shown below for a somewhat funky PDF (left) and an \"upper-limit\" type measurement (right). Dark and light shading show the extent of the two intervals, while the horizontal, dashed lines show the corresponding thresholds in probability density. That is, the 68.3% CI consists of all those values of $\\theta$ for which $p(\\theta|\\mathrm{data})$ exceeds the higher dashed line.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/bayes_ci_maxp.png\" width=70%></td>\n",
    "        <td><img src=\"graphics/bayes_ci_limit_maxp.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Note that, in Bayesian terms, there is no fundamental difference between a CI and an upper or lower limit, as there is in classical statistics. We may say \"limit\" in the case where one endpoint of the CI is entirely dictated by the extent of the prior, as in the case above-right where the prior is zero for $\\theta<0$.\n",
    "\n",
    "**Advantages:** I would argue that this definition is by far the most intuitive. The nominal value we report is actually the \"best\" in the sense of most probable, and similarly the interval contains the most probable values. Faced with the statement $\\theta = \\theta_0 \\pm \\Delta\\theta$, what else would you possibly think?\n",
    "\n",
    "**Disadvantages:** When the CI is multiply connected (highly unusual in my limited experience), it's more annoying to describe. When we use Monte Carlo methods to characterize the posterior, we will need to use kernel density estimation (a fancy way to say \"smoothing a histogram\") to get a smooth function from a finite number of samples; this means that a human needs to at least look at the distribution to verify that the results are ok (horrors!). I know of exactly one Python package that computes this style of CI correctly, including the multiply connected case, and it's the one I wrote (so, you know, tech support is questionable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Median value and symmetric interval in probability (percentile) space\n",
    "\n",
    "In this convention, $\\theta_0$ is the median of the marginal posterior, and the endpoints of the CI containing probability $q$ are the quantiles corresponding to probabilities $(1-q)/2$ and $(1+q)/2$. So, for example, the 68.3% CI spans the quantiles of 0.1585 and 0.8415. Note that the CI is not necessarily symmetric about $\\theta_0$. Here is how it looks for the same two PDFs.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/bayes_ci_perc.png\" width=70%></td>\n",
    "        <td><img src=\"graphics/bayes_ci_limit_perc.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Advantages:** Straightforward to automatically compute from monte carlo samples or evaluations on a grid without human intervention or thought. This is the method I would recommend when the result being reported is not _the_ result of the work, and the more eyes-on method above isn't feasible (e.g. when you need to plot many measurements in a figure).\n",
    "\n",
    "**Disadvantages:** The nominal value and CI reported are not the most probable! Especially when the PDF is asymmetric and/or multimodal, as in this example, the CI can be... non-intuitive. Since there is no unique definition of the CDF or quantile in multiple dimensions, this method has no multivariate analog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean value and symmetric interval\n",
    "\n",
    "Here we would (if we were silly) report the mean of the PDF as $\\theta_0$, and build a CI by extending symmetrically (in $\\theta$) in both directions until we contain the right probability.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/bayes_ci_mean.png\" width=70%></td>\n",
    "        <td><img src=\"graphics/bayes_ci_limit_mean.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Advantages:** None that I know of. It's not any easier than the second method.\n",
    "\n",
    "**Disadvantages:** Even odder behavior for asymmetric/multimodal distributions. If the PDF has a sharp cutoff (e.g. due to the prior going to zero), the CI can easily extend into this forbidden region.\n",
    "\n",
    "Needless to say, this option is not recommended, but you might see it occasionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other conventions\n",
    "\n",
    "For completeness, two more methods are worth mentioning. Each of these is only valid (in the sense of even delivering the advertised contained probability) in certain limits, namely as (and if) the central limit theorem makes the posterior approximately Gaussian.\n",
    "* CI's defined from differences in the log-likelihood or log-posterior, following the Maximum Likelihood tradition. This can be quite accurate in central limit theorem satisfying situations, even if the posterior isn't quite Guassian, but doesn't present any particular advantages.\n",
    "* Reporting the mean and standard deviation of the PDF. Please don't do this. It's hardly any computationally easier than finding the percentiles of a distribution, but only valid for a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing multi-dimensional posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also common to visualize 2D marginalized posteriors (over 2 parameters),\n",
    "\n",
    "$p(\\theta_1,\\theta_2|\\mathrm{data}) = \\int d\\phi \\, p(\\theta_1,\\theta_2,\\phi|\\mathrm{data})$.\n",
    "\n",
    "Most often, we don't try to show the PDF itself, but just the contours delineating the 2-dimensional credible regions. These are defined analogously to the 1D case as regions (CR's) that contain a specified amount of probability of the 2D marginalized posterior. As far as I know, the only convention in wide use for how to uniquely define the regions is the \"most probable\" one. (You might occasionally see the analog of the \"mean and standard deviation\" method, in which the covariance matrix translates to a 2D ellipse, but this is not suddenly a better idea in multiple dimensions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One reason to look at 2D posteriors is that they reveal _degeneracies_ between parameters, i.e. particular directions in parameter space in which the posterior is constant or slowly changing. In the example below, you can see that the posterior for each analysis shown has a degeneracy between the parameters $\\sigma_8$ and $\\gamma$, but the direction and the degree to which the degeneracy is broken vary. Here we can see that jointly analyzing all 3 data sets could result in much tighter constraints, something that might not be obvious from the 1D posterior distributions alone.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/degeneracies.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Generally, a 1D CI will have a smaller extent in a given parameter than the 2D CR for the same contained probaility. This is not a bug, but follows from the fact that the 1D marginalized posterior is integrated over an additional degree of freedom compared with the 2D one, which can result in the 1D PDF being more sharply peaked. In the case above, if we looked at only the 1D posteriors, we might be led to believe that the \"Galaxies\" and \"CMB\" posteriors were inconsistent with each other, while in 2D we can see that they overlap nicely.\n",
    "\n",
    "Naturally, features like degeneracies can exist in more than 2 dimensions, but there isn't a widely used method for visualizing CR's in more than 2D. You may encounter color being used to show the most probable value of a third parameter at different points in the $\\theta_1$-$\\theta_2$ plane, but that's about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where did 68.3% and 95.4% come from?\n",
    "\n",
    "These numbers are a legacy of the centrality of the Gaussian distribution in statistics. You might recognize them as approximately the integral of a Gaussian from $-1\\sigma$ to $+1\\sigma$ (68.3%) and $-2\\sigma$ to $+2\\sigma$ (95.4%), where $\\sigma$ is the standard deviation. Consequently, they are often referred to as \"$1\\sigma$ confidence\" and \"$2\\sigma$ confidence\". (This is another bit of terminology that we will almost certainly slip into, despite it not being technically meaningful.) If you ever need to, the probability associated with \"$N\\sigma$\" can be computed in 1 line as the CDF of the $\\chi^2_1$ distribution evaluated at $N^2$ (this is equivalent to the more obvious difference of two Gaussian CDFs, based on the definition above):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scipy.stats as st\n",
    "def probLevel(Nsigma):\n",
    "    return st.chi2.cdf(Nsigma**2, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the origin, these are conventional probability levels in our field. It's not so bad. Roughly speaking, you can just think of 68.3% as $2/3$ and 95.4% as $19/20$; conversely, there's a 1 in 3 chance that the 68.3% CR doesn't include the truth, but only a 1 in 20 chance that the 95.4% CR does not.\n",
    "\n",
    "Most often, you will see 68.3% and 95.4% CR's shown in 2D contour plots, and 68.3% CI's reported in text (though often 95.4% is used when quoting a limit). Note that the difference between 95.0% and 95.4% is not necessarily trivial; we aren't just being pedantic by including the third significant digit here. In practice, we don't often see higher probability values because (a) we don't normally care, and (b) robustly determining the CI's/CR's becomes increasingly computationally expensive for higher probabilities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
