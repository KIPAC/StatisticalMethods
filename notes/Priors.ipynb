{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"There are only two problems in inference: how to assign probability distributions, and how to do integrals.\" - John Skilling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All our inferences necessarily depend on our model parameters' prior PDFs, because you can't do inference without making assumptions.\n",
    "\n",
    "\n",
    "* The personal and subjective nature of probability in Bayesian statistics (remember, you are quantifying *your* degree of belief in the parameter values) is in slight tension with the collective aspiration of the scientific community to learn the \"objective truth\" about the world.\n",
    "\n",
    "\n",
    "* There are several things we can do when *publishing* inferences:\n",
    "\n",
    "  * State clearly what priors we assumed, so that anyone reproducing our analysis can compare their results given their assumptions with ours.\n",
    "  \n",
    "  * Try to make it easy for others to carry out the same inference but with different assumptions. Making our posterior samples is a good example of this: those samples can often be re-weighted in an importance sampling analysis that involves different assumptions.\n",
    "\n",
    "  * Carry out \"sensitivity analyses\" so that our readers don't have to do the above: if a conclusion is sensitive to the prior PDF we assumed, that means there was relatively little information in our data about that parameter.\n",
    "  \n",
    "\n",
    "* Still, to be able to derive the posterior PDF for the parameters, we must assign a prior PDF. Let's look at some guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformative Priors\n",
    "\n",
    "* One way to assign priors is to *plead ignorance.*  \n",
    "\n",
    "\n",
    "* Not knowing the value of a parameter up to an additive constant indicates that a *uniform* (i.e. top hat) prior might be appropriate. \n",
    "\n",
    "\n",
    "* Not knowing the value of a parameter up to an multiplicative constant indicates that a prior *uniform in the log of the parameter* might be appropriate. Equating ${\\rm Pr}(x)\\,dx = {\\rm Pr}(\\log{x})\\,d \\log{x}$ and assigning the above uniform PDF leads to ${\\rm Pr}(x) \\propto 1/x$, which is sometimes known loosely as the \"Jeffreys Prior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, ${\\rm Pr}(x) \\propto 1/x$ seems like a bad idea, because it rises steeply with decreasing $x$, \"biasing\" the result. But suppose $x$ is galaxy mass, and you want to plead ignorance: you assign a a uniform prior between 0 and $10^{14} M_{\\odot}$. With this assignment you are saying that *a priori* ${\\rm Pr}(x > 10^{12}) = 0.99$ - a highly informative statement!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational problem with uninformative priors is that they can lead to parameter space volumes that are unmanageably large.\n",
    "\n",
    "### Exercise: \n",
    "\n",
    "Consider a uniform joint prior PDF in N parameter dimensions. What fraction of the *a priori* allowed volume is in a hypercubic shell that has thickness *f* of the side length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = 0.01\n",
    "N = 2\n",
    "# Compute difference between two hypervolumes:\n",
    "dV = 'not yet coded'\n",
    "\n",
    "print(\"Volume fraction for f =\",f,\" is\",dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect can cause real computational problems when attempting to characterize posterior PDFs - you've seen it already, just in our attempts with two-dimensional grids!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy Priors\n",
    "\n",
    "* Attempts have been made to put uninformative priors on a sound theoretical footing, by deriving them from various symmetries and invariants. (Jeffreys' principle is one such attempt that is worth reading about.)\n",
    "\n",
    "\n",
    "* The uniform and logarithmic priors are examples of distributions derivable from the Principle of Maximum Entropy, which is a formalization of the request for an uninformative prior. \n",
    "\n",
    "\n",
    "* The entropy of a PDF $p(x)$ is the functional $H(p) = - \\int p \\log p dx$, and measures something like \"randomness.\" The distribution $p$ that maximizes the entropy $H$ given some constraints is the least informative (or most non-committal) one. \n",
    "\n",
    "\n",
    "* Maximizing the entropy of $p(x)$ is an exercise in the calculus of variations. The constraints (which are often integral in nature) are added in with Lagrange multipliers, and then the functional derivative of $H$ is taken and set to zero, and the optimal $p(x)$ solved for. The values of the multipliers come from normalizing the resulting PDF $p(x)$. See the [Maximum Entropy wikipedia page](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) and related [page of Maximum Entropy probability distributions](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution) for more details.\n",
    "\n",
    "\n",
    "* Examples of constraints, and the maximum entropy distributions that result from them, include:\n",
    "\n",
    "  * No constraint, except that $\\int p(x) dx = 1$, gives $p(x) \\propto {\\rm constant}$, the *uniform distribution.*\n",
    "\n",
    "  * Known mean, $\\int x p(x) dx = \\mu$, gives $p(x) \\propto \\exp(-x/\\mu)$, *the exponential distribution.*\n",
    "\n",
    "  * Known mean $\\mu$ and variance $\\sigma^2$ gives $p(x) \\propto \\exp(-(x - \\mu)^2/2\\sigma^2)/\\sigma$, the *Gaussian distribution.*\n",
    "  \n",
    "  \n",
    "* This last one provides some justification for our repeated interpretation of points with error bars as Gaussian sampling distributions: when we are just told a mean and a width, the Gaussian assumption is the least committal one we can make - and over a large number of situations making this assumption, we are not persuaded that it's a bad idea to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informative Priors\n",
    "\n",
    "* Another way to think of the prior PDF is as an *opportunity* - to include *more information* into our analysis.\n",
    "\n",
    "\n",
    "* One good prior PDF is the *posterior PDF* from a previous analysis.\n",
    "\n",
    "${\\rm Pr}(x|d,H) \\propto {\\rm Pr}(d|x,H)\\;{\\rm Pr}(x|c,H)$\n",
    "\n",
    "\n",
    "* Note that ${\\rm Pr}(x|c,H)\\propto {\\rm Pr}(c|x,H)\\;{\\rm Pr}(x|H)$ by Bayes Theorem, so that *using posteriors as priors is equivalent to joint inference from multiple datasets*:\n",
    "\n",
    "${\\rm Pr}(x|d,H) \\propto {\\rm Pr}(d|x,H)\\;{\\rm Pr}(c|x,H)\\;{\\rm Pr}(x|H)$\n",
    "\n",
    "\n",
    "More generally, a good prior PDF is *one that accurately represents your beliefs.* \n",
    "\n",
    "\n",
    "  * Such \"subjective priors\" are typically treated with justified caution, because scientists aspire to perform *objective* analyses. However, a prior PDF assignment is just one type of assumption in the many that are involved in the model, and they can and should be tested in the same way. \n",
    "  \n",
    "  \n",
    "  * Subjective priors can be derived by \"elicitation\" - asking experts for opinions. This is not done often in astronomy, but if you look carefully you might find examples.\n",
    "  \n",
    "  \n",
    "* In a domain like astrophysics some very good prior PDFs are generated as the outputs of *simulations.*  Approximate realizations of complex systems are generated by computer models implementing the known laws of physics plus a range of plausible assumptions, and the result can often be interpreted as a prior distribution for some simpler phenomenological model parameters. \n",
    "\n",
    "\n",
    "* An example from KIPAC's research would be our treatment of large catalogs of simulated dark matter halos as ensembles of samples drawn from the prior PDF for model dark matter halo parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
