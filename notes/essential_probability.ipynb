{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notes: Review of Probability\n",
    "\n",
    "Containing:\n",
    "* a review of the most revelant tools of mathemetical probability\n",
    "* some common probability distributions to be familiar with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology and axioms\n",
    "\n",
    "We'll keep things general at the start. Consider the set of all possible answers/outcomes for a given question/experiment. This is called the **sample space**, $\\Omega$. A subset of $\\Omega$ is called an **event**, $E$.\n",
    "\n",
    "The probability of an event, $P(E)$,  is a real function satisfying certain requirements known as the axioms of probability:\n",
    "\n",
    "**Axioms of probability**\n",
    "* $\\forall E: 0 \\leq P(E) \\leq 1$\n",
    "* $P(\\Omega) = P\\left(\\bigcup_{\\mathrm{all~}i} E_i\\right) = 1$\n",
    "* If $E_i$ are mutually exclusive, $P\\left(\\bigcup_i E_i\\right) = \\sum_i P(E_i)$\n",
    "\n",
    "This dry definition provides a function with the right properties to describe our intuitive understanding of probability. In particular, if some experiment can in principle be repeated many times, independently and under identical conditions, then the fraction of outcomes $E$ will tend to $P(E)$. Probabilities can also be applied in circumstances that do not lend themselves to numerous, independent and identical repetition (that is, to any circumstances whatsoever). In this case, the probability can be interpreted as a weight associated with evidence, information, knowledge, or belief. Although the word \"subjective\" is often used in this context, this doesn't imply a lack or rigor. The theory is still fully mathematical, and as such people should agree on the outcome of calculations based on the same starting assumptions, whatever their opinions might be. (They can disagree about the starting assumptions, but that's a different story.)\n",
    "\n",
    "For those who have studied statistical mechanics, even if you haven't formally studied probability, the math above hopefully seems familiar. In particular, you might have seen this when studying the microcanonical ensemble - if $\\Omega$ is the set of states available to a system of fixed energy, the fundamental assumption of statistical mechanics is that all of those states are equally probable to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random variables\n",
    "\n",
    "Any quantity whose value can only be described probabilistically (i.e. that is not known with certainty) is called **random**. To be clear, \"random\" does not mean that _nothing_ is known about the value of the variable; the form of its probability distribution may, in fact, be known precisely. This is analogous to the idea that it's possible to know the precise state of a quantum system, and make precise predictions about the frequency of different measurement outcomes, without knowing with certainty what the outcome of a specific measurement will be.\n",
    "\n",
    "Very often, the events we're concerned with correspond to the different values that a random variable might have, with the sample space corresponding to the real numbers, non-negative real numbers, non-negative integers, etc., depending on the variable in question.\n",
    "\n",
    "The axioms translate to this case straightforwardly, e.g. for an integer-valued variable $n$, the second axiom, $P(\\Omega)=1$, becomes the normalization condition\n",
    "\n",
    "$\\sum_{n'=-\\infty}^{\\infty} P(n=n') = 1$.\n",
    "\n",
    "For discrete random variables, like $n$ in this example, $P(n=n')$ is a function of $n'$ and is a probability. This is sometimes called the **probability mass function** of $n$.\n",
    "\n",
    "The corresponding function for a continuous random variable (e.g. a real number) is the **probability density function** (PDF), e.g. $p(x=x')$ for a real-valued $x$. However, it's important to realize that a PDF is **not** a probability! $p(x=x')$ is also not, in general, dimensionless (its dimensions are the inverse of $x$'s). However, any integrals of a PDF _are_ probabilities, for example\n",
    "* $p(x=x') \\, dx'$\n",
    "* $P(x_0 < x < x_1) = \\int_{x_0}^{x_1} dx' \\, p(x=x')$\n",
    "\n",
    "As the notation $p(x=x')$ rapidly becomes cumbersome, we will be lazy and simply write $p(x)$ from now on. This should be interpreted as a function of $x$, the value of a random variable whose identity should normally be clear from the choice of symbol.\n",
    "\n",
    "The distinction between probability mass and density is highly relevant if we ever want to change variables, e.g. $x\\rightarrow y(x)$. In particular, $p(y) \\neq p[x(y)]$. Instead, it turns out,\n",
    "\n",
    "$p(y) = p(x) \\left|\\frac{dx}{dy}\\right|$.\n",
    "\n",
    "To keep this distinction straight, we will use capital $P$ for probability, and lower-case $p$ for probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More probabilistic functions\n",
    "\n",
    "The **cumulative distribution function** (CDF) is the probability that $X\\leq x$. This is sometimes referred to simply as the \"distribution function\". Yes, that can be confusing, so we won't.\n",
    "\n",
    "The CDF is usually written\n",
    "\n",
    "$F(x') = P(x \\leq x') = \\int_{-\\infty}^{x'} p(x=s)ds$.\n",
    "\n",
    "It is, necessarily, non-decreasing and bounded by zero and one. For distributions over a continuous variable, the derivative of the CDF is the PDF.\n",
    "\n",
    "\n",
    "The **quantile function** is the inverse of the CDF, $F^{-1}(\\mathcal{P})$. It's argument is a probability, and it returns a _quantile_ which lives in the same space as $x$. For example, the median of a distribution is $F^{-1}(0.5)$. A quantile is conceptually the the same as a _percentile_, except that one is a function of probability (between 0 and 1), while the other is a function of percentage probability (between 0 and 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple example: an unfair coin toss\n",
    "\n",
    "We flip a coin which is weighted to land on heads a fraction $q$ of the time. To make things numeric, let $X=0$ for stand for an outcome of tails and $X=1$ for heads. We have the discrete PMF\n",
    "* $P(0) = 1-q$,\n",
    "* $P(1) = q$;\n",
    "\n",
    "and the corresponding CDF\n",
    "* $F(0) = 1-q$,\n",
    "* $F(1) = 1$.\n",
    "\n",
    "The quantile function would be\n",
    "* $F^{-1}(\\mathcal{P}) = 0 \\quad$ for $1-q \\le \\mathcal{P} < 1$,\n",
    "* $F^{-1}(1) = 1$,\n",
    "\n",
    "and would be undefined for $\\mathcal{P}<1-q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate probability distributions\n",
    "\n",
    "Things get more interesting when we deal with joint distributions of multiple events, $p(X=x$ and $Y=y)$, or just $p(x,y)$. We might visualize such a function using contours of equal probability:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_correlated.png\" width=50% alt=\"Contour plot of a correlated multivariate PDF\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are various meaningful manipulations and reductions of multivariate distributions to know.\n",
    "\n",
    "The **marginal probability** of $y$, $p(y)$, means the probability of $y$ *irrespective* of what $x$ is. Mathematically, this corresponds to summing $p(x,y)$ over all possible values of $x$, for a given $y$:\n",
    "\n",
    "$p(y) = \\int dx \\, p(x,y)$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_marginal.png\" width=50% alt=\"Marginal distribution of y\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "You can see that the result is a function of $y$ only, and that information about the structure of the 2-dimensional function $p(x,y)$ is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **conditional probability** of $y$ *given* a value of $x$, $p(y|x)$, is most easily understood this way:\n",
    "\n",
    "$p(x,y) = p(y|x)\\,p(x)$.\n",
    "\n",
    "That is, the probability of getting $x$ AND $y$ can be *factorized* into the product of probabilities of\n",
    "* getting $x$ regardless of $y$, *and at the same time*\n",
    "* getting $y$ given $x$.\n",
    "\n",
    "$p(y|x)$ is a normalized slice through $p(x,y)$ rather than an integral. Specifically, $1/p(x)$ is exactly the coefficient needed to normalize things such that $\\int dy \\, p(y|x) = \\int dy \\, p(x,y)/p(x) = 1$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_conditional.png\" width=50%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In this example, comparing to the 2-dimensional plot above, you can see that the peak of $p(y|x)$ will move depending on what value of $x$ is conditioned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the special case where $p(y|x) = p(y)$, $x$ and $y$ are **independent**. Equivalently, $p(x,y) = p(x)\\,p(y)$. It should be clear that this requires the isoprobability contours to look qualitatively different than in the example above.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_independent.png\" width=50%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability calculus\n",
    "\n",
    "The section above outlines the basic operations we can perform on probability densitites, namely marginalization and conditioning. Rather than diving into the formalities of probability as a branch of mathematics, we hope that learning by example in the later notes will be enough. Still, here are a couple of basic rules that will hopefully avoid confusion:\n",
    "1. One cannot marginalize over a variable if it appears only to the _right_ of a \"|\".\n",
    "2. Applying the definition of conditional probability and substituting into an equation is safer than trying to intuit when and where \"|\" can just be dropped in.\n",
    "\n",
    "Those who are dying to read a more comprehensive yet not terribly accessible reference (which quickly gets far ahead of what we need for the moment), are invited to start [here](https://arxiv.org/abs/1205.4446)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Here is a quick test of your understanding of some of the concepts so far. (Solutions are at the end of the notes.)\n",
    "\n",
    "Take the coin tossing example from earlier, where $P(\\mathrm{heads})=q$ and $P(\\mathrm{tails})=1-q$ for a given toss. Assume that this holds independently for each toss, and that the coin is tossed twice.\n",
    "\n",
    "Find:\n",
    "\n",
    "1. The conditional probability that both tosses are heads, given that the first toss is heads.\n",
    "2. The conditional probability that both tosses are heads, given that at least one of the tosses is heads.\n",
    "3. The marginal probability that the second toss is heads.\n",
    "\n",
    "Writing out the probability of all 4 possible outcomes of the 2 coin tosses and then reasoning about them is a completely legitimate approach here, and even preferable to relying on formulae. But it's still a good idea to see those formulae in action, producing the same result.\n",
    "\n",
    "For more practice manipulating probabilities see [probability essentials tutorial](../tutorials/probability_essentials.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some standard probability distributions\n",
    "\n",
    "For reference, here is a non-exhaustive list of distributions that commonly arise. They are \"standard\" in the sense that lots of properties are known, and various functions are usually implemented for you in software (e.g. `scipy.stats`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli distribution\n",
    "\n",
    "This is the example used above. It's support (the space over which the PDF or PMF is defined) is a discrete set of cardinality 2 (i.e. heads or tails, true or false, 1 or 0), and the distribution's only parameter is the probability of \"success\" (usually defined as heads/true/1), $q$.\n",
    "\n",
    "Not especially useful on its own, apart from as an introductory example. But, if we have $N$ independent Bernoulli trials, the total number of successes follows the..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution\n",
    "\n",
    "... whose PMF is $P(k|q,n) = {n \\choose k} q^k (1-q)^{n-k}$ for $k$ successes. ${n \\choose k}$ is the combinatoric \"choose\" function, $\\frac{n!}{k!(n-k)!}$.\n",
    "\n",
    "The Binomial distribution is additive, in that the sum of two Binomial random variables with the same $q$, respectively with $n_1$ and $n_2$ trials, also follows the Binomial distribution, with parameters $q$ and $n_1+n_2$.\n",
    "\n",
    "This distribution might be useful for inferring the fraction of objects belonging to one of two classes ($q$), based on typing of an unbiased but finite number of them ($n$). (The multinomial generalization would work for more than two classes.)\n",
    "\n",
    "Or, consider the case that each Bernoulli trial is whether a photon hits our detector in a given, tiny time interval. For an integration whose total length is much longer than any such time interval we could plausibly distinguish, we might take the limit where $n$ becomes huge, thus reaching the..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "... which describes the number of successes when the number of trials is in principle infinite, but $q$ is correspondingly vanishingly small. It has a single parameter, $\\mu$, which corresponds to the product $qn$ when interpretted as a limit of the Binomial distribution. The PMF is\n",
    "\n",
    "$P(k|\\mu) = \\frac{\\mu^k e^{-\\mu}}{k!}$.\n",
    "\n",
    "Like the Binomial distribution, the Poisson distribution is additive. It also has the following (probably familiar) properties:\n",
    "* Expectation value (mean) $\\langle k\\rangle = \\mu$,\n",
    "* Variance $\\left\\langle \\left(k-\\langle k \\rangle\\right)^2 \\right\\rangle = \\mu$.\n",
    "\n",
    "For Sufficiently Large values of $\\mu$, the Poisson distribution is well approximated by the..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal or Gaussian distribution\n",
    "\n",
    "... which is, more generally, defined over the real line as\n",
    "\n",
    "$p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right]$,\n",
    "\n",
    "and has mean $\\mu$ and variance $\\sigma^2$. With $\\mu=0$ and $\\sigma=1$, this is known as the standard normal distribution.\n",
    "\n",
    "Because physicists love Gauss more than normality, you can expect to see \"Gaussian\" more often in the (astro)physics literature. Statisticians normally say \"normal\".\n",
    "\n",
    "The limiting relationship between the Poisson and Normal distributions is one example of the **central limit theorem**, which says that, under quite general conditions, the average of a large number of random variables tends towards normal, even if the individual variables are not normally distributed. Whether and how well it applies, and just what \"large number\" means, in any given situation is a practical question more than anything else, at least in this course. But the bottom line is that this distribution shows up quite often, and is normally a reasonable guess in those situations where we have to make a choice and have no better ideas.\n",
    "\n",
    "Note that intentionally binning up data, when doing so throws out potentially useful information, in the hopes of satisfying the central limit theorem, is generally frowned upon in this class.\n",
    "\n",
    "Finally, the sum of squares of $\\nu$ standard normal variables follows the..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square (or chi-squared, or $\\chi^2$) distribution\n",
    "\n",
    "... whose PDF is\n",
    "\n",
    "$p(x|\\nu) = \\frac{x^{\\nu/2-1} e^{-x/2}}{2^{\\nu/2}\\Gamma(\\nu/2)}$,\n",
    "\n",
    "where $\\Gamma$ is the gamma function and $\\nu$ is a positive integer. This distribution is also sometimes abbreviated as $\\chi^2_\\nu$, explicitly showing the parameter, $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "When a probability distribution is known to be one of these standard distributions, we will often use its name explicitly. So,\n",
    "\n",
    "$\\mathrm{Normal}(x|\\mu,\\sigma)$\n",
    "\n",
    "should be read as a Gaussian PDF, as a function of $x$, with parameters $\\mu$ and $\\sigma$. The notation\n",
    "\n",
    "$\\mathrm{Normal}(\\mu,\\sigma)$\n",
    "\n",
    "would refer to the distribution generally (i.e. not specifically to its density function), as in \"$x$ is distributed as $\\mathrm{Normal}(\\mu,\\sigma)$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise solutions\n",
    "1) $P(X=1,Y=1|X=1) = P(X=1,Y=1)/P(X=1) = q^2/q = q$\n",
    "\n",
    "2) $P(X=1,Y=1|X=1\\cup Y=1) = P(X=1,Y=1)/P(X=1\\cup Y=1)$\n",
    "\n",
    "$\\quad = P(X=1,Y=1)/[P(X=1,Y=1)+P(X=1,Y=0)+P(X=0,Y=1)]$\n",
    "\n",
    "$\\quad = q^2/[q^2 + 2q(1-q)]$\n",
    "\n",
    "3) $P(Y=1) = P(X=0,Y=1) + P(X=1,Y=1) = q(1-q) + q^2 = q$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
