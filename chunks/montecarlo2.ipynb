{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Efficient Monte Carlo Sampling\n",
    "\n",
    "Goals:\n",
    "* Introduce some of the approaches used to make sampling more efficient\n",
    "* Experiment with using some of these techniques on difficult PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Ivezic 5.8\n",
    "* MacKay chapters 29-30\n",
    "* Gelman chapters 11 and 13\n",
    "* Handbook of MCMC (Brooks, Gelman, Jones and Meng, eds.), parts of which are [on the web](http://www.mcmchandbook.net/HandbookTableofContents.html)\n",
    "* [Recent](http://arxiv.org/abs/1304.4473) and [less recent](http://cosmologist.info/notes/MCMC.ppt) notes on `CosmoMC` by Antony Lewis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Quite often, repeated evaluation of the posterior function becomes a bottleneck. Any tweaks that speed up convergence can be worthwhile.\n",
    "\n",
    "The simple methods introduced so far especially struggle with\n",
    "* strongly degenerate parameter combinations\n",
    "* PDFs with multiple, separated modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: degeneracies\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Cosmology is bananas<br>\n",
    "            <img src=\"../graphics/mc2_banana_eg.png\" width=100%>\n",
    "        </td>\n",
    "        <td></td>\n",
    "        <td>\n",
    "            The Rosenbrock function<br>\n",
    "            <img src=\"../graphics/mc2_rosenbrock.png\" width=100%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: multiple modes\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Marginalized bananas<br>\n",
    "            <img src=\"../graphics/mc2_multimode_eg.png\" width=100%>\n",
    "        </td>\n",
    "        <td></td>\n",
    "        <td>\n",
    "            The eggbox function<br>\n",
    "            <img src=\"../graphics/mc2_eggbox.png\" width=100%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Above 2 slides' image credits (left panels): A. Mantz ([MNRAS, 446:2205, 2015](http://adsabs.harvard.edu/abs/2015MNRAS.446.2205M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding things up\n",
    "\n",
    "Broadly speaking, we can try to\n",
    "1. tailor faster algorithms to specific classes of PDF\n",
    "2. look for ways to make the general samplers more intelligent\n",
    "\n",
    "We can also use different samplers for different subsets of parameters - the only rule is that every parameter must get updated somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampling\n",
    "is a specialization of Metropolis-Hastings\n",
    "* Instead of making a general proposal, we cycle through the parameters proposing changes to one at a time\n",
    "* A proposal for $\\theta_i$ is into the **fully conditional posterior** $P(\\theta_i|\\theta_{-i},x)$, where $-i$ means all subscripts other than $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gibbs Sampling\n",
    "\n",
    "```\n",
    "while we want more samples\n",
    "    propose theta1 | theta2, theta3, ..., data\n",
    "    accept/reject theta1\n",
    "    propose theta2 | theta1, theta3, ..., data\n",
    "    accept/reject theta2\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate Gibbs Sampling\n",
    "\n",
    "In general, this is not obviously an improvement to proposing changes to all $\\theta$ simultaneously.\n",
    "\n",
    "**But**, something interesting happens if the fully conditional likelihood and prior are conjugate - then we know the conditional posterior exactly. If we use independent samples of the conditional posterior as proposals, then the Metropolis-Hastings acceptance ratio becomes\n",
    "\n",
    "$\\frac{P(y|\\theta_{-i})Q(x|y,\\theta_{-i})}{P(x|\\theta_{-i})Q(y|x,\\theta_{-i})} = \\frac{P(y|\\theta_{-i})P(x|\\theta_{-i})}{P(x|\\theta_{-i})P(y|\\theta_{-i})} = 1$\n",
    "\n",
    "and every proposal is automatically accepted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate Gibbs Sampling\n",
    "\n",
    "```\n",
    "while we want more samples\n",
    "    draw th1 from P(th1|th2,th3,...,data)\n",
    "    draw th2 from P(th2|th1,th3,...,data)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: normal PDF\n",
    "\n",
    "25 Metropolis iterations (left) vs. 25 Gibbs transitions (right)\n",
    "\n",
    "Color goes blue$\\rightarrow$red with time (step number)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../graphics/mc2_metro.png\" width=100%>\n",
    "        </td>\n",
    "        <td></td>\n",
    "        <td>\n",
    "            <img src=\"../graphics/mc2_gibbs.png\" width=100%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate Gibbs Sampling\n",
    "Pros\n",
    "* No cycles \"wasted\" on rejected proposals\n",
    "* No pesky tuning of the proposal scale\n",
    "\n",
    "Cons\n",
    "* Only works for conjugate or partially conjugate models\n",
    "* Occasionally still slower than proposing multi-parameter Metropolis updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate Gibbs Sampling\n",
    "Several packages exist which can figure out what conjugate relationships exist based on a model definition, and do Gibbs and/or Metropolis sampling.\n",
    "* [BUGS](http://openbugs.net/w/FrontPage)\n",
    "* [JAGS](http://mcmc-jags.sourceforge.net/)\n",
    "* [STAN](http://mc-stan.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: normal distribution\n",
    "\n",
    "Suppose we have some data, $x_i$, that we're modelling as being generated by a common normal (aka Gaussian) distribution.\n",
    "\n",
    "$P(x_i|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\exp-\\frac{(x_i-\\mu)^2}{2\\sigma^2}$\n",
    "\n",
    "The model parameters are the mean, $\\mu$, and variance, $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: normal distribution\n",
    "\n",
    "Consulting [the Wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior), we see that we can Gibbs sample this posterior using two steps:\n",
    "* the conjugate distribution for $\\mu|\\sigma^2,x$ is normal\n",
    "* the conjugate distribution for $\\sigma^2|\\mu,x$ is scaled inverse $\\chi^2$\n",
    "\n",
    "Remeber that using conjugacy limits our options for choosing priors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slice sampling\n",
    "\n",
    "A less specialized modification of Metropolis-Hastings is **slice sampling**. This method also avoids having to manually tune a proposal scale, although it usually involves a rejection step.\n",
    "\n",
    "Given a starting point $P(\\theta_0)$, we uniformly choose a probability $q\\leq P(\\theta_0)$. This defines a *slice* where $P(\\theta)\\geq q$, and we sample the next $\\theta$ uniformly within the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slice sampling\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/mc2_slice.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slice sampling with rejection\n",
    "See also [Neal 2003](https://dx.doi.org/10.1214%2Faos%2F1056562461)\n",
    "\n",
    "```\n",
    "start with position x\n",
    "evaluate p = P(x)\n",
    "guess a width W\n",
    "while we want samples\n",
    "    draw q from Uniform(0,p)\n",
    "    choose L,R: R-L=W and L<=x<=R\n",
    "    while P(L) > q, set L = L - W\n",
    "    while P(R) > q, set R = R + W\n",
    "    loop forever\n",
    "        draw y from Uniform(L,R)\n",
    "        if P(y) < q,\n",
    "            if y < x, set L = x1\n",
    "            otherwise, set R = x1\n",
    "        otherwise, break\n",
    "    set x = x1 and record x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accelerating Metropolis\n",
    "\n",
    "The simple Metropolis implementation we tinkered with in the [Basic Monte Carlo](montecarlo1.ipynb) chunk can be improved on in a number of ways.\n",
    "\n",
    "```\n",
    "choose a starting point and a proposal scale\n",
    "while we want more samples\n",
    "    propose a new point from a scaled Gaussian\n",
    "     centered on our current position\n",
    "    accept/reject the new point\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis\n",
    "\n",
    "These strategies boil down to cleverly, and usually automatically, tuning our proposal distribution.\n",
    "\n",
    "Strictly speaking, tuning the proposal distribution on the fly breaks the rules of MCMC (specifically, detailed balance).\n",
    "* We *should* stop tuning after some time and throw away all steps before this point\n",
    "* In practice, if we've actually managed to converge to the target distribution, subsequent updates to the proposal distribution will be negligible anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal lengths\n",
    "\n",
    "A Gaussian proposal distribution makes the most likely proposals very near the current point. This is not necessarily efficient. We could instead\n",
    "* choose a proposal direction (see later)\n",
    "* propose a step length from a heavier-tailed distribution\n",
    "* tune the scale of the proposed move based on the acceptances so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal lengths\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/mc2_steplength.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal basis\n",
    "\n",
    "Parameter degeneracies are inconvenient.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/mc1_sandbox_ab.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal basis\n",
    "\n",
    "Degeneracies can usually be reduced or eliminated by reparametrizing the model.\n",
    "* But beware - a nonlinear reparametrization does not leave your prior invariant - recall the transformation of PDFs.\n",
    "* Linear reparametrizations are equivalent to a change of basis for the parameter space. To gain the benefits of this, we merely need to change the way we propose steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal basis\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Original<br><img src=\"../graphics/mc2_reparam0.png\" width=100%></td>\n",
    "        <td>Parameter basis change<br><img src=\"../graphics/mc2_reparam1.png\" width=100%></td>\n",
    "        <td>Proposal basis change<br><img src=\"../graphics/mc2_reparam2.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: proposal basis\n",
    "For a nice, elliptical PDF, the optimal basis diagonalizes the covariance of the parameters.\n",
    "* This doesn't mean we can only propose along the preferred directions\n",
    "* For a given direction, we know the optimal length scale for the proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: fast-slow decompositions\n",
    "If changes to some parameters require much more costly calculations than others, straightforward diagonalization may not be optimal in practice.\n",
    "* We can choose to propose changes to fast parameters more often than slow parameters\n",
    "* Given a hierarchy of speeds, we can also *triangularize* the covariance matrix - so the fastest parameter appears in every basis vector and the slowest only in one\n",
    "* ... ad nauseam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating Metropolis: multiple chains\n",
    "Unless we know what to do a priori, the best information for how to tune proposal lengths and directions comes from running test chains started from different positions and using the information they build up about the posterior.\n",
    "\n",
    "Parallel, communicating chains (e.g. using MPI) that tune themselves this way can **vastly** decrease the time to convergence compared with single chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tempering\n",
    "Consider the function $[P(x)]^{1/T}$, where $P(x)$ is the target PDF.\n",
    "* We say a chain sampling this function has temperature $T$. For $T>1$, $P^{1/T}$ is smoothed out compared with $P$ (cf simulated annealing).\n",
    "* This allows chains to move among multiple peaks more easily.\n",
    "* Of course, we're only actually interested in $T=1$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parallel tempering\n",
    "\n",
    "With parallel tempering, we run one chain with $T=1$ and several more chains with $T>1$. A modified Metropolis-Hastings update occasionally allows the chains to exchange positions, giving the $T=1$ chain a mechanism for sampling regions of parameter space it might otherwise have low probability of proposing. Samples from the $T=1$ chain can be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: think\n",
    "\n",
    "Recall some of the ugly PDF features we've considered before, namely strong/nonlinear degeneracies and multiple modes.\n",
    "For each of the methods above, do you expect an improvement compared with standard Metropolis in these situations, and why?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../graphics/mc2_rosenbrock.png\" width=80%>\n",
    "        </td>\n",
    "        <td></td>\n",
    "        <td>\n",
    "            <img src=\"../graphics/mc2_eggbox.png\" width=80%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus numerical exercise: play\n",
    "\n",
    "[This sandbox notebook](../code/mc2_sandbox.ipynb) contains code for a simple Metropolis sampler and 4 PDFs:\n",
    "1. The Rosenbrock function (actually minus this; it's normally used for testing minimizers), illustrated above\n",
    "2. The eggbox function, illustrated above\n",
    "3. A spherical shell function\n",
    "4. A Gaussian PDF (suitable for Gibbs sampling)\n",
    "\n",
    "Choose one of the speed-ups above (something that sounds implementable in a reasonable time) and modify/replace the sampler to use it on one or more of these PDFs. Compare performance (burn-in length, acceptance rate, etc.) with simple Metropolis. (Apart from PDF 4, chances are that nothing you do will work brilliantly, but you should see a difference!)\n",
    "\n",
    "For Gibbs sampling, interpret PDF 4 as a likelihood function and put some thought into how you define your priors. Otherwise, just take the given PDF to be the posterior distribution."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
